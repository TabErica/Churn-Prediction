{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T23:58:16.721124Z",
     "start_time": "2025-12-18T23:58:13.432098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ],
   "id": "2c25b5c33d119e0e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 0. Common preprocessing",
   "id": "30f46387c161fe97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T23:58:16.724373Z",
     "start_time": "2025-12-18T23:58:16.722032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def basic_preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df = df[df['userId'] != 0]\n",
    "    df['ts'] = pd.to_datetime(df['ts'], unit='ms')\n",
    "    df['registration'] = pd.to_datetime(df['registration'])\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['page'] = df['page'].astype(str).str.strip()\n",
    "    df = df.sort_values(['userId', 'ts'])\n",
    "    return df"
   ],
   "id": "ce39628b6ab5acd5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Build labels + cutoff (no fixed window, churn if CC exists)",
   "id": "dcb5ecdddcfc4265"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T23:58:16.727439Z",
     "start_time": "2025-12-18T23:58:16.724851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_labels_and_cutoff(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        y: Series(index=userId, values in {0,1})\n",
    "        cutoff_ts: Series(index=userId), used to truncate behavior logs\n",
    "\n",
    "    Rules:\n",
    "      - If CC exists: y=1, cutoff = first CC timestamp\n",
    "      - Otherwise:    y=0, cutoff = last observed timestamp\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    cc_mask = (df['page'] == \"Cancellation Confirmation\")\n",
    "    first_cc = (\n",
    "        df[cc_mask]\n",
    "        .groupby('userId')['ts']\n",
    "        .min()\n",
    "    )\n",
    "    \n",
    "    last_ts = df.groupby('userId')['ts'].max()\n",
    "    all_users = last_ts.index\n",
    "    \n",
    "    y = pd.Series(0, index=all_users, dtype=int)\n",
    "    y.loc[first_cc.index] = 1\n",
    "    y.name = \"target\"\n",
    "    \n",
    "    cutoff_ts = last_ts.copy()\n",
    "    cutoff_ts.loc[first_cc.index] = first_cc\n",
    "    cutoff_ts.name = \"cutoff_ts\"\n",
    "    \n",
    "    return y.sort_index(), cutoff_ts.sort_index()"
   ],
   "id": "d497f4c8e6eb428b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Feature engineering",
   "id": "94c0b18195f610b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T23:58:16.742037Z",
     "start_time": "2025-12-18T23:58:16.728628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_features(df: pd.DataFrame,\n",
    "                   cutoff_ts: pd.Series,\n",
    "                   pages_ref=None):\n",
    "    \"\"\"\n",
    "    df: preprocessed raw logs (with ts, page, etc.)\n",
    "    cutoff_ts: index=userId, value=cutoff time\n",
    "    pages_ref:\n",
    "        - training: None\n",
    "        - test: pass page list from training to align columns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Map cutoff timestamp to each row\n",
    "    df['cutoff_ts'] = df['userId'].map(cutoff_ts)\n",
    "    \n",
    "    # Remove events after cutoff\n",
    "    df_obs = df[df['ts'] <= df['cutoff_ts']].copy()\n",
    "    \n",
    "    # Remove leakage pages (CC and Cancel)\n",
    "    LEAK_PAGES = [\"Cancel\", \"Cancellation Confirmation\"]\n",
    "    df_obs = df_obs[~df_obs['page'].isin(LEAK_PAGES)].copy()\n",
    "    \n",
    "    # All userIds\n",
    "    all_users = cutoff_ts.index\n",
    "    \n",
    "    # -------- A. Basic attributes --------\n",
    "    agg_basic = df_obs.groupby('userId').agg(\n",
    "        gender=('gender', 'first'),\n",
    "        level=('level', 'last'),\n",
    "    )\n",
    "    \n",
    "    # -------- B. Activity features --------\n",
    "    df_obs['date'] = df_obs['ts'].dt.date\n",
    "    \n",
    "    n_events = df_obs.groupby('userId').size().rename('n_events')\n",
    "    n_active_days = df_obs.groupby('userId')['date'].nunique().rename('n_active_days')\n",
    "    \n",
    "    nextsong = df_obs[df_obs['page'] == 'NextSong']\n",
    "    total_listen_time = nextsong.groupby('userId')['length'].sum().rename('total_listen_time')\n",
    "    song_count = nextsong.groupby('userId').size().rename('song_count')\n",
    "    \n",
    "    activity_feat = pd.concat(\n",
    "        [n_events, n_active_days, total_listen_time, song_count],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    activity_feat['avg_song_length'] = (\n",
    "        activity_feat['total_listen_time'] / (activity_feat['song_count'] + 1e-6)\n",
    "    )\n",
    "    activity_feat['events_per_day'] = (\n",
    "        activity_feat['n_events'] / (activity_feat['n_active_days'] + 1e-6)\n",
    "    )\n",
    "    activity_feat['listen_time_per_day'] = (\n",
    "        activity_feat['total_listen_time'] / (activity_feat['n_active_days'] + 1e-6)\n",
    "    )\n",
    "    \n",
    "    # ===== Recent 7-day behavior (relative to cutoff) =====\n",
    "    df_obs['days_to_cutoff'] = (\n",
    "        (df_obs['cutoff_ts'] - df_obs['ts']).dt.total_seconds() / 86400.0\n",
    "    )\n",
    "    recent_mask = df_obs['days_to_cutoff'] <= 7\n",
    "    \n",
    "    recent_events = (\n",
    "        df_obs[recent_mask]\n",
    "        .groupby('userId')\n",
    "        .size()\n",
    "        .rename('events_last_7d')\n",
    "    )\n",
    "    \n",
    "    recent_songs = (\n",
    "        df_obs[recent_mask & (df_obs['page'] == 'NextSong')]\n",
    "        .groupby('userId')\n",
    "        .size()\n",
    "        .rename('songs_last_7d')\n",
    "    )\n",
    "    \n",
    "    activity_feat = pd.concat(\n",
    "        [activity_feat, recent_events, recent_songs],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # -------- C. Page counts + ratios --------\n",
    "    if pages_ref is None:\n",
    "        pages_ref = sorted(df_obs['page'].unique())\n",
    "    \n",
    "    df_obs['page'] = pd.Categorical(df_obs['page'], categories=pages_ref)\n",
    "    \n",
    "    page_counts = (\n",
    "        df_obs\n",
    "        .pivot_table(index='userId',\n",
    "                     columns='page',\n",
    "                     values='ts',\n",
    "                     aggfunc='count')\n",
    "        .fillna(0)\n",
    "    )\n",
    "    \n",
    "    page_counts['total_events_from_pages'] = page_counts.sum(axis=1)\n",
    "    \n",
    "    ratio_feat = page_counts.div(\n",
    "        page_counts['total_events_from_pages'] + 1e-6,\n",
    "        axis=0\n",
    "    )\n",
    "    \n",
    "    page_counts = page_counts.add_prefix('cnt_page_')\n",
    "    ratio_feat = ratio_feat.add_prefix('ratio_page_')\n",
    "    \n",
    "    # -------- D. Time-related features --------\n",
    "    first_obs_ts = df_obs.groupby('userId')['ts'].min().rename('first_obs_ts')\n",
    "    last_obs_ts = df_obs.groupby('userId')['ts'].max().rename('last_obs_ts')\n",
    "    registration_ts = df.groupby('userId')['registration'].first().rename('registration_ts')\n",
    "    \n",
    "    time_feat = pd.concat([first_obs_ts, last_obs_ts, registration_ts], axis=1)\n",
    "    \n",
    "    time_feat['days_since_registration'] = (\n",
    "        (time_feat['last_obs_ts'] - time_feat['registration_ts']).dt.days\n",
    "    )\n",
    "    time_feat['obs_window_days'] = (\n",
    "        (time_feat['last_obs_ts'] - time_feat['first_obs_ts']).dt.days\n",
    "    )\n",
    "    \n",
    "    time_feat['days_since_registration'] = time_feat['days_since_registration'].clip(lower=0)\n",
    "    time_feat['obs_window_days'] = time_feat['obs_window_days'].clip(lower=0)\n",
    "    \n",
    "    # Midpoint split: first vs second half\n",
    "    mid_ts = first_obs_ts + (last_obs_ts - first_obs_ts) / 2\n",
    "    mid_ts.name = 'mid_ts'\n",
    "    df_obs['mid_ts'] = df_obs['userId'].map(mid_ts)\n",
    "    \n",
    "    is_second_half = df_obs['ts'] > df_obs['mid_ts']\n",
    "    \n",
    "    events_first_half = (\n",
    "        df_obs[~is_second_half]\n",
    "        .groupby('userId')\n",
    "        .size()\n",
    "        .rename('events_first_half')\n",
    "    )\n",
    "    \n",
    "    events_second_half = (\n",
    "        df_obs[is_second_half]\n",
    "        .groupby('userId')\n",
    "        .size()\n",
    "        .rename('events_second_half')\n",
    "    )\n",
    "    \n",
    "    time_feat = pd.concat(\n",
    "        [time_feat, events_first_half, events_second_half],\n",
    "        axis=1\n",
    "    )\n",
    "    time_feat['ratio_second_to_first'] = (\n",
    "        time_feat['events_second_half'] / (time_feat['events_first_half'] + 1e-6)\n",
    "    )\n",
    "    \n",
    "    # -------- E. Payment / subscription features --------\n",
    "    is_paid_last = (agg_basic['level'] == 'paid').astype(int)\n",
    "    is_paid_last.name = 'is_paid_last'\n",
    "    \n",
    "    ever_paid = (\n",
    "        df_obs.groupby('userId')['level']\n",
    "        .apply(lambda s: int((s == 'paid').any()))\n",
    "        .rename('ever_paid')\n",
    "    )\n",
    "    \n",
    "    def count_level_change(s):\n",
    "        s = s.dropna()\n",
    "        if s.empty:\n",
    "            return 0\n",
    "        return (s != s.shift(1)).sum() - 1\n",
    "    \n",
    "    n_level_change = (\n",
    "        df_obs.groupby('userId')['level']\n",
    "        .apply(count_level_change)\n",
    "        .rename('n_level_change')\n",
    "    )\n",
    "    \n",
    "    # -------- F. Status code features --------\n",
    "    status_counts = (\n",
    "        df_obs\n",
    "        .groupby(['userId', 'status'])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "    \n",
    "    status_counts = status_counts.add_prefix('status_')\n",
    "    \n",
    "    # Ensure 404 / 307 columns exist\n",
    "    for code in [404, 307]:\n",
    "        col = f'status_{code}'\n",
    "        if col not in status_counts.columns:\n",
    "            status_counts[col] = 0\n",
    "    \n",
    "    n_404 = status_counts['status_404']\n",
    "    n_307 = status_counts['status_307']\n",
    "    \n",
    "    status_counts['frac_404'] = n_404 / (n_events + 1e-6)\n",
    "    status_counts['frac_307'] = n_307 / (n_events + 1e-6)\n",
    "    \n",
    "    # -------- G. Session-related features --------\n",
    "    session_grp = df_obs.groupby(['userId', 'sessionId'])\n",
    "    \n",
    "    session_stats = session_grp.agg(\n",
    "        session_start=('ts', 'min'),\n",
    "        session_end=('ts', 'max'),\n",
    "        session_event_count=('ts', 'count'),\n",
    "    )\n",
    "    \n",
    "    session_stats['session_duration'] = (\n",
    "        session_stats['session_end'] - session_stats['session_start']\n",
    "    ).dt.total_seconds()\n",
    "    \n",
    "    sess_user_grp = session_stats.groupby('userId')\n",
    "    \n",
    "    session_count = sess_user_grp.size().rename('session_count')\n",
    "    \n",
    "    session_duration_stats = sess_user_grp['session_duration'].agg(\n",
    "        mean_session_duration='mean',\n",
    "        max_session_duration='max',\n",
    "        min_session_duration='min',\n",
    "        std_session_duration='std',\n",
    "    )\n",
    "    \n",
    "    session_event_stats = sess_user_grp['session_event_count'].agg(\n",
    "        mean_event_count_per_session='mean',\n",
    "        max_event_count_per_session='max',\n",
    "        min_event_count_per_session='min',\n",
    "        std_event_count_per_session='std',\n",
    "    )\n",
    "    \n",
    "    # Idle time: first session idle = 0 (safe handling)\n",
    "    session_stats_sorted = (\n",
    "        session_stats\n",
    "        .reset_index()\n",
    "        .sort_values(['userId', 'session_start'])\n",
    "    )\n",
    "    \n",
    "    session_stats_sorted['prev_end'] = (\n",
    "        session_stats_sorted\n",
    "        .groupby('userId')['session_end']\n",
    "        .shift(1)\n",
    "    )\n",
    "    \n",
    "    session_stats_sorted['idle_time'] = (\n",
    "        session_stats_sorted['session_start'] - session_stats_sorted['prev_end']\n",
    "    ).dt.total_seconds()\n",
    "    \n",
    "    session_stats_sorted['idle_time'] = session_stats_sorted['idle_time'].fillna(0)\n",
    "    \n",
    "    idle_stats = (\n",
    "        session_stats_sorted\n",
    "        .groupby('userId')['idle_time']\n",
    "        .agg(\n",
    "            mean_idle_time='mean',\n",
    "            max_idle_time='max',\n",
    "            min_idle_time='min',\n",
    "            latest_idle_time='last',\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    idle_stats['is_big_idle'] = (idle_stats['max_idle_time'] >= 86400).astype(int)\n",
    "    idle_stats['is_very_big_idle'] = (idle_stats['max_idle_time'] >= 864000).astype(int)\n",
    "    \n",
    "    song_count_series = activity_feat['song_count']\n",
    "    song_per_session = (song_count_series / (session_count + 1e-6)).rename('song_per_session')\n",
    "    \n",
    "    has_single_session = (session_count == 1).astype(int).rename(\"has_single_session\")\n",
    "    \n",
    "    # -------- Merge all user-level features --------\n",
    "    feats = pd.concat(\n",
    "        [\n",
    "            agg_basic,\n",
    "            activity_feat,\n",
    "            page_counts,\n",
    "            ratio_feat,\n",
    "            time_feat,\n",
    "            status_counts,\n",
    "            session_count.to_frame(),\n",
    "            session_duration_stats,\n",
    "            session_event_stats,\n",
    "            idle_stats,\n",
    "            is_paid_last.to_frame(),\n",
    "            ever_paid.to_frame(),\n",
    "            n_level_change.to_frame(),\n",
    "            song_per_session.to_frame(),\n",
    "            has_single_session.to_frame(),\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    feats = feats.reindex(all_users)\n",
    "    feats = feats.sort_index()\n",
    "    \n",
    "    # Drop datetime columns\n",
    "    dt_cols = feats.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns\n",
    "    feats = feats.drop(columns=dt_cols)\n",
    "    \n",
    "    return feats, pages_ref"
   ],
   "id": "2b671a04d50b78ed",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Load training data and build X, y",
   "id": "4fa4876bfb6e04a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:00:05.290195Z",
     "start_time": "2025-12-18T23:58:16.742846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_raw = pd.read_parquet(\"train.parquet\")\n",
    "train_df = basic_preprocess(train_raw)\n",
    "\n",
    "y, cutoff_ts_train = build_labels_and_cutoff(train_df)\n",
    "X_train_all, pages_ref = build_features(train_df, cutoff_ts_train, pages_ref=None)\n",
    "\n",
    "common_users = y.index.intersection(X_train_all.index)\n",
    "X = X_train_all.loc[common_users]\n",
    "y = y.loc[common_users]\n",
    "\n",
    "print(\"Training set shape:\", X.shape, \" Positive rate:\", y.mean())"
   ],
   "id": "84718cf61cd51075",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/xtlxv68s5sv1j3rj3ww3zh_h0000gn/T/ipykernel_96940/888373634.py:90: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  .pivot_table(index='userId',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (19140, 77)  Positive rate: 0.22314524555903867\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:00:05.325421Z",
     "start_time": "2025-12-19T00:00:05.299009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_base = X.copy()\n",
    "\n",
    "cat_cols = [c for c in ['gender', 'level'] if c in X_base.columns]\n",
    "num_cols = X_base.select_dtypes(include=['number']).columns.tolist()\n",
    "num_cols = [c for c in num_cols if c not in cat_cols]\n",
    "\n",
    "for c in cat_cols:\n",
    "    X_base[c] = X_base[c].astype(str).fillna('missing')\n",
    "\n",
    "X_base[num_cols] = X_base[num_cols].fillna(0)"
   ],
   "id": "84cd80d032c57aca",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:00:18.835313Z",
     "start_time": "2025-12-19T00:00:05.326532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build test features (shared pipeline)\n",
    "\n",
    "test_raw = pd.read_parquet(\"test.parquet\")\n",
    "test_df = basic_preprocess(test_raw)\n",
    "\n",
    "cutoff_ts_test = test_df.groupby('userId')['ts'].max().sort_index()\n",
    "X_test_all, _ = build_features(test_df, cutoff_ts_test, pages_ref=pages_ref)\n",
    "\n",
    "X_test = X_test_all.reindex(columns=X_base.columns)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "for c in cat_cols:\n",
    "    if c in X_test.columns:\n",
    "        X_test[c] = X_test[c].astype(str).fillna('missing')\n",
    "\n",
    "X_test[num_cols] = X_test[num_cols].fillna(0)\n",
    "X_test.index = X_test.index.astype(int)"
   ],
   "id": "a94c41065e628573",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/xtlxv68s5sv1j3rj3ww3zh_h0000gn/T/ipykernel_96940/888373634.py:90: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  .pivot_table(index='userId',\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Base preprocessing (for LR / ET)",
   "id": "14afb427ae777ec5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:00:18.841006Z",
     "start_time": "2025-12-19T00:00:18.836598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sparse preprocessing (LR / ET)\n",
    "preprocess_sparse = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Dense preprocessing (KNN)\n",
    "preprocess_dense = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_arr = y.values"
   ],
   "id": "e95eaf02161cb8b7",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Oversampling strategy",
   "id": "7adbdefdce25b120"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:00:18.847817Z",
     "start_time": "2025-12-19T00:00:18.841774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def oversample_dataframe(X_df: pd.DataFrame, y_sr: pd.Series, random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Simulate Spark-style oversampling:\n",
    "      - Identify majority class (0) and minority class (1)\n",
    "      - Replicate minority class to match majority size\n",
    "      - Shuffle and return\n",
    "    \"\"\"\n",
    "    df_xy = X_df.copy()\n",
    "    df_xy['label'] = y_sr.values\n",
    "    \n",
    "    major_df = df_xy[df_xy['label'] == 0]\n",
    "    minor_df = df_xy[df_xy['label'] == 1]\n",
    "    \n",
    "    if len(minor_df) == 0:\n",
    "        raise ValueError(\"No positive samples, cannot oversample.\")\n",
    "    \n",
    "    ratio = int(len(major_df) / len(minor_df))\n",
    "    ratio = max(1, ratio)\n",
    "    print(\"Oversample ratio:\", ratio)\n",
    "    \n",
    "    oversampled_minor = pd.concat([minor_df] * ratio, ignore_index=True)\n",
    "    \n",
    "    combined = pd.concat([major_df, oversampled_minor], axis=0)\n",
    "    combined = combined.sample(frac=1.0, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    X_over = combined.drop(columns=['label'])\n",
    "    y_over = combined['label'].values\n",
    "    \n",
    "    print(\"Oversampled class distribution:\", np.bincount(y_over) / len(y_over))\n",
    "    return X_over, y_over\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ],
   "id": "42eb92565280825",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. Single Models",
   "id": "5ad669f9a9f1cb2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:00:23.479302Z",
     "start_time": "2025-12-19T00:00:18.850764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train Logistic Regression (with oversampling)\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    solver='liblinear',\n",
    "    class_weight=None  # already oversampled\n",
    ")\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    ('preprocess', preprocess_sparse),\n",
    "    ('clf', log_reg),\n",
    "])\n",
    "\n",
    "oof_lr = np.zeros(len(X_base))\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(X_base, y_arr), 1):\n",
    "    X_trn, X_val = X_base.iloc[trn_idx], X_base.iloc[val_idx]\n",
    "    y_trn, y_val = y_arr[trn_idx], y_arr[val_idx]\n",
    "    \n",
    "    # Oversampling inside each CV fold\n",
    "    X_trn_over, y_trn_over = oversample_dataframe(\n",
    "        X_trn, pd.Series(y_trn, index=X_trn.index),\n",
    "        random_state=42 + fold\n",
    "    )\n",
    "    \n",
    "    pipe_lr.fit(X_trn_over, y_trn_over)\n",
    "    pred_val = pipe_lr.predict_proba(X_val)[:, 1]\n",
    "    oof_lr[val_idx] = pred_val\n",
    "    \n",
    "    auc = roc_auc_score(y_val, pred_val)\n",
    "    print(f\"[LR] Fold {fold} AUC: {auc:.4f}\")\n",
    "\n",
    "print(\"\\n[LR] OOF AUC:\", roc_auc_score(y_arr, oof_lr))\n",
    "\n",
    "# Train final LR on full oversampled data\n",
    "X_over_full, y_over_full = oversample_dataframe(X_base, y, random_state=2025)\n",
    "pipe_lr.fit(X_over_full, y_over_full)\n",
    "final_lr = pipe_lr"
   ],
   "id": "1538e2f5bf714e44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53721098 0.46278902]\n",
      "[LR] Fold 1 AUC: 0.8955\n",
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53711731 0.46288269]\n",
      "[LR] Fold 2 AUC: 0.8947\n",
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53711731 0.46288269]\n",
      "[LR] Fold 3 AUC: 0.8896\n",
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53711731 0.46288269]\n",
      "[LR] Fold 4 AUC: 0.8907\n",
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53711731 0.46288269]\n",
      "[LR] Fold 5 AUC: 0.9025\n",
      "\n",
      "[LR] OOF AUC: 0.894540723158478\n",
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53713605 0.46286395]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:00:33.606241Z",
     "start_time": "2025-12-19T00:00:23.479877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train ExtraTrees (with oversampling)\n",
    "\n",
    "et_clf = ExtraTreesClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=\"sqrt\",\n",
    "    n_jobs=-1,\n",
    "    class_weight=None,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "pipe_et = Pipeline([\n",
    "    ('preprocess', preprocess_sparse),\n",
    "    ('clf', et_clf),\n",
    "])\n",
    "\n",
    "oof_et = np.zeros(len(X_base))\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(X_base, y_arr), 1):\n",
    "    X_trn, X_val = X_base.iloc[trn_idx], X_base.iloc[val_idx]\n",
    "    y_trn, y_val = y_arr[trn_idx], y_arr[val_idx]\n",
    "    \n",
    "    X_trn_over, y_trn_over = oversample_dataframe(\n",
    "        X_trn, pd.Series(y_trn, index=X_trn.index),\n",
    "        random_state=100 + fold\n",
    "    )\n",
    "    \n",
    "    pipe_et.fit(X_trn_over, y_trn_over)\n",
    "    pred_val = pipe_et.predict_proba(X_val)[:, 1]\n",
    "    oof_et[val_idx] = pred_val\n",
    "    \n",
    "    auc = roc_auc_score(y_val, pred_val)\n",
    "    print(f\"[ET] Fold {fold} AUC: {auc:.4f}\")\n",
    "\n",
    "print(\"\\n[ET] OOF AUC:\", roc_auc_score(y_arr, oof_et))\n",
    "\n",
    "X_over_full_et, y_over_full_et = oversample_dataframe(X_base, y, random_state=303)\n",
    "pipe_et.fit(X_over_full_et, y_over_full_et)\n",
    "final_et = pipe_et"
   ],
   "id": "8504d6720f5f2426",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53721098 0.46278902]\n",
      "[ET] Fold 1 AUC: 0.9012\n",
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53711731 0.46288269]\n",
      "[ET] Fold 2 AUC: 0.8985\n",
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53711731 0.46288269]\n",
      "[ET] Fold 3 AUC: 0.8973\n",
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53711731 0.46288269]\n",
      "[ET] Fold 4 AUC: 0.8948\n",
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53711731 0.46288269]\n",
      "[ET] Fold 5 AUC: 0.9142\n",
      "\n",
      "[ET] OOF AUC: 0.9011470801922208\n",
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53713605 0.46286395]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:00:36.416113Z",
     "start_time": "2025-12-19T00:00:33.606993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train KNN (with oversampling, dense preprocessing)\n",
    "\n",
    "knn_clf = KNeighborsClassifier(\n",
    "    n_neighbors=50,\n",
    "    weights='distance',\n",
    "    p=2,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "pipe_knn = Pipeline([\n",
    "    ('preprocess', preprocess_dense),\n",
    "    ('clf', knn_clf),\n",
    "])\n",
    "\n",
    "oof_knn = np.zeros(len(X_base))\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(X_base, y_arr), 1):\n",
    "    X_trn, X_val = X_base.iloc[trn_idx], X_base.iloc[val_idx]\n",
    "    y_trn, y_val = y_arr[trn_idx], y_arr[val_idx]\n",
    "    \n",
    "    X_trn_over, y_trn_over = oversample_dataframe(\n",
    "        X_trn, pd.Series(y_trn, index=X_trn.index),\n",
    "        random_state=200 + fold\n",
    "    )\n",
    "    \n",
    "    pipe_knn.fit(X_trn_over, y_trn_over)\n",
    "    pred_val = pipe_knn.predict_proba(X_val)[:, 1]\n",
    "    oof_knn[val_idx] = pred_val\n",
    "    \n",
    "    auc = roc_auc_score(y_val, pred_val)\n",
    "    print(f\"[KNN] Fold {fold} AUC: {auc:.4f}\")\n",
    "\n",
    "print(\"\\n[KNN] OOF AUC:\", roc_auc_score(y_arr, oof_knn))\n",
    "\n",
    "X_over_full_knn, y_over_full_knn = oversample_dataframe(X_base, y, random_state=404)\n",
    "pipe_knn.fit(X_over_full_knn, y_over_full_knn)\n",
    "final_knn = pipe_knn"
   ],
   "id": "f764cbdece7c392c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53721098 0.46278902]\n",
      "[KNN] Fold 1 AUC: 0.8657\n",
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53711731 0.46288269]\n",
      "[KNN] Fold 2 AUC: 0.8556\n",
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53711731 0.46288269]\n",
      "[KNN] Fold 3 AUC: 0.8545\n",
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53711731 0.46288269]\n",
      "[KNN] Fold 4 AUC: 0.8483\n",
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53711731 0.46288269]\n",
      "[KNN] Fold 5 AUC: 0.8741\n",
      "\n",
      "[KNN] OOF AUC: 0.8595239130394047\n",
      "Oversample ratio: 3\n",
      "Oversampled class distribution: [0.53713605 0.46286395]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:03:23.635952Z",
     "start_time": "2025-12-19T00:03:22.942414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_submission(name, proba, X_index):\n",
    "    example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "    proba_by_uid = pd.Series(proba, index=X_index)\n",
    "    \n",
    "    # Fixed positive rate (top-k strategy)\n",
    "    pos_rate = 0.5\n",
    "    n_test = len(example_sub)\n",
    "    k = int(round(n_test * pos_rate))\n",
    "    k = max(1, min(k, n_test - 1))\n",
    "    \n",
    "    topk_users = proba_by_uid.sort_values(ascending=False).index[:k]\n",
    "    \n",
    "    pred = pd.Series(0, index=proba_by_uid.index, dtype=int)\n",
    "    pred.loc[topk_users] = 1\n",
    "    \n",
    "    example_sub['target'] = example_sub['id'].map(pred).fillna(0).astype(int)\n",
    "    \n",
    "    print(name, \"target distribution:\")\n",
    "    print(example_sub['target'].value_counts())\n",
    "    \n",
    "    example_sub.to_csv(name, index=False)\n",
    "    print(\"Saved\", name)\n",
    "\n",
    "proba_lr_test = final_lr.predict_proba(X_test)[:, 1]\n",
    "proba_et_test = final_et.predict_proba(X_test)[:, 1]\n",
    "proba_knn_test = final_knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "make_submission(\"submission_lr_oversample_v1.csv\", proba_lr_test, X_test.index)\n",
    "make_submission(\"submission_et_oversample_v1.csv\", proba_et_test, X_test.index)\n",
    "make_submission(\"submission_knn_oversample_v1.csv\", proba_knn_test, X_test.index)"
   ],
   "id": "5d42c2347a0afaf3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_lr_oversample_v1.csv target distribution:\n",
      "target\n",
      "0    1452\n",
      "1    1452\n",
      "Name: count, dtype: int64\n",
      "Saved submission_lr_oversample_v1.csv\n",
      "submission_et_oversample_v1.csv target distribution:\n",
      "target\n",
      "1    1452\n",
      "0    1452\n",
      "Name: count, dtype: int64\n",
      "Saved submission_et_oversample_v1.csv\n",
      "submission_knn_oversample_v1.csv target distribution:\n",
      "target\n",
      "0    1452\n",
      "1    1452\n",
      "Name: count, dtype: int64\n",
      "Saved submission_knn_oversample_v1.csv\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7. Voting (LR + KNN)",
   "id": "4eed1e6c2d25e3d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:03:23.824067Z",
     "start_time": "2025-12-19T00:03:23.750405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 12. Voting ensemble: LR + KNN only\n",
    "\n",
    "print(\"\\n===== Grid Search Voting Weights (LR + KNN) =====\")\n",
    "\n",
    "best_auc_vote_lk = -1\n",
    "best_weights_lk = None\n",
    "best_oof_vote_lk = None\n",
    "\n",
    "# oof_lr, oof_knn, y_arr already computed\n",
    "for w_lr in [1, 2, 3, 4]:\n",
    "    for w_knn in [1, 2, 3, 4]:\n",
    "        w_sum = w_lr + w_knn\n",
    "        oof_vote = (w_lr * oof_lr + w_knn * oof_knn) / w_sum\n",
    "        auc_vote = roc_auc_score(y_arr, oof_vote)\n",
    "        if auc_vote > best_auc_vote_lk:\n",
    "            best_auc_vote_lk = auc_vote\n",
    "            best_weights_lk = (w_lr, w_knn)\n",
    "            best_oof_vote_lk = oof_vote\n",
    "\n",
    "print(\n",
    "    f\"[LR+KNN VOTING] Best OOF AUC = {best_auc_vote_lk:.4f}, \"\n",
    "    f\"best weights (w_lr, w_knn) = {best_weights_lk}\"\n",
    ")\n",
    "\n",
    "# Apply best weights to test predictions\n",
    "w_lr_best, w_knn_best = best_weights_lk\n",
    "w_sum_best = w_lr_best + w_knn_best\n",
    "\n",
    "proba_voting_lr_knn = (\n",
    "    w_lr_best * proba_lr_test +\n",
    "    w_knn_best * proba_knn_test\n",
    ") / w_sum_best\n",
    "\n",
    "print(\"LR+KNN voting test_proba min/max:\",\n",
    "      proba_voting_lr_knn.min(), proba_voting_lr_knn.max())\n",
    "\n",
    "make_submission(\n",
    "    \"submission_voting_lr_knn_oversample_v1.csv\",\n",
    "    proba_voting_lr_knn,\n",
    "    X_test.index\n",
    ")"
   ],
   "id": "cc3fb48f33aa9cec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Grid Search Voting Weights (LR + KNN) =====\n",
      "[LR+KNN VOTING] Best OOF AUC = 0.8953, best weights (w_lr, w_knn) = (4, 1)\n",
      "LR+KNN voting test_proba min/max: 0.00040484525713156027 0.9776098675508538\n",
      "submission_voting_lr_knn_oversample_v1.csv target distribution:\n",
      "target\n",
      "0    1452\n",
      "1    1452\n",
      "Name: count, dtype: int64\n",
      "Saved submission_voting_lr_knn_oversample_v1.csv\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3f75b8b31bd8a8b6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
