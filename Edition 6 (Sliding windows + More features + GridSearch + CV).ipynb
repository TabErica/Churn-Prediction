{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:41:03.482782400Z",
     "start_time": "2025-12-19T00:41:03.088749600Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "id": "eaf8eac82a5b59b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Preprocess"
   ],
   "id": "67b63d8196e8b390"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:41:37.028237800Z",
     "start_time": "2025-12-19T00:41:03.475667700Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (17499636, 20)\n",
      "Test shape: (4393179, 20)\n"
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "# 1. Read raw parquet files\n",
    "train_df = pd.read_parquet(\"train.parquet\")\n",
    "test_df = pd.read_parquet(\"test.parquet\")\n",
    "\n",
    "# 3. Convert time fields\n",
    "train_df['ts'] = pd.to_datetime(train_df['ts'], unit='ms')\n",
    "test_df['ts'] = pd.to_datetime(test_df['ts'], unit='ms')\n",
    "\n",
    "train_df['registration'] = pd.to_datetime(train_df['registration'])\n",
    "test_df['registration'] = pd.to_datetime(test_df['registration'])\n",
    "\n",
    "train_df['time'] = pd.to_datetime(train_df['time'])\n",
    "test_df['time'] = pd.to_datetime(test_df['time'])\n",
    "\n",
    "# 4. Clean the 'page' field (strip spaces)\n",
    "train_df['page'] = train_df['page'].astype(str).str.strip()\n",
    "test_df['page'] = test_df['page'].astype(str).str.strip()\n",
    "\n",
    "# 5. Extract state from location (last 2 chars) as 'state'\n",
    "#    e.g. \"New York, NY\" -> \"NY\"\n",
    "train_df['location'] = train_df['location'].astype(str).str.strip()\n",
    "test_df['location'] = test_df['location'].astype(str).str.strip()\n",
    "\n",
    "train_df['state'] = train_df['location'].str[-2:]\n",
    "test_df['state'] = test_df['location'].str[-2:]\n",
    "\n",
    "# 6. Sort by userId + ts to ensure chronological order\n",
    "train_df = train_df.sort_values(['userId', 'ts'])\n",
    "test_df = test_df.sort_values(['userId', 'ts'])\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)"
   ],
   "id": "5c159ef378c8d521"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Window label"
   ],
   "id": "eddaee2febced3b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:41:37.097750500Z",
     "start_time": "2025-12-19T00:41:37.034361800Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutoff_start: 2018-10-15\n",
      "max_ts_date : 2018-11-20\n",
      "cutoff_end  : 2018-11-09\n"
     ]
    }
   ],
   "execution_count": 3,
   "source": [
    "# 1. Set sliding-window parameters\n",
    "horizon_days = 10  # prediction horizon length = 10 days (competition setting)\n",
    "\n",
    "# Manually set an earliest cutoff start date\n",
    "cutoff_start = pd.to_datetime(\"2018-10-15\")\n",
    "\n",
    "# The last cutoff must satisfy: cutoff + 10 days <= train_df['ts'].max().normalize()\n",
    "# We take the max date (normalized to day) and subtract horizon_days\n",
    "max_ts_date = train_df['ts'].max().normalize()          # typically 2018-11-20\n",
    "cutoff_end = max_ts_date - pd.Timedelta(days=horizon_days + 1)\n",
    "\n",
    "\n",
    "print(\"cutoff_start:\", cutoff_start.date())\n",
    "print(\"max_ts_date :\", max_ts_date.date())\n",
    "print(\"cutoff_end  :\", cutoff_end.date())"
   ],
   "id": "d312751b36c77bcf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:41:37.099818800Z",
     "start_time": "2025-12-19T00:41:37.080641100Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cutoff_dates: 26\n",
      "cutoff_dates preview: [Timestamp('2018-10-15 00:00:00'), Timestamp('2018-10-16 00:00:00'), Timestamp('2018-10-17 00:00:00'), Timestamp('2018-10-18 00:00:00'), Timestamp('2018-10-19 00:00:00')] ... [Timestamp('2018-11-05 00:00:00'), Timestamp('2018-11-06 00:00:00'), Timestamp('2018-11-07 00:00:00'), Timestamp('2018-11-08 00:00:00'), Timestamp('2018-11-09 00:00:00')]\n"
     ]
    }
   ],
   "execution_count": 4,
   "source": [
    "# Generate a list of cutoff dates (one per day)\n",
    "cutoff_dates = pd.date_range(start=cutoff_start, end=cutoff_end, freq=\"D\")\n",
    "print(\"Number of cutoff_dates:\", len(cutoff_dates))\n",
    "print(\"cutoff_dates preview:\", list(cutoff_dates[:5]), \"...\", list(cutoff_dates[-5:]))"
   ],
   "id": "692e06dc5e932b21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:41:37.677096600Z",
     "start_time": "2025-12-19T00:41:37.083387Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users who churned at least once: 4271\n"
     ]
    }
   ],
   "execution_count": 5,
   "source": [
    "# 2. For each cutoff_date, generate samples (userId, cutoff_date, target)\n",
    "\n",
    "# Compute each user's first churn timestamp (first time seeing Cancellation Confirmation)\n",
    "first_churn_ts = (\n",
    "    train_df[train_df['page'] == \"Cancellation Confirmation\"]\n",
    "    .groupby('userId')['ts']\n",
    "    .min()\n",
    ")\n",
    "\n",
    "print(\"Number of users who churned at least once:\", len(first_churn_ts))"
   ],
   "id": "414476464183483f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:42:11.933123500Z",
     "start_time": "2025-12-19T00:41:37.789132600Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-15\n",
      "Users in observation window (including already churned): 16271\n",
      "Alive users after filtering churned: 14832\n",
      "Alive users with churn(=1) in prediction window: 826\n",
      "Samples for this cutoff: 14832   Positives: 826   Positive rate: 0.05569039913700108\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-16\n",
      "Users in observation window (including already churned): 16537\n",
      "Alive users after filtering churned: 15003\n",
      "Alive users with churn(=1) in prediction window: 837\n",
      "Samples for this cutoff: 15003   Positives: 837   Positive rate: 0.05578884223155369\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-17\n",
      "Users in observation window (including already churned): 16754\n",
      "Alive users after filtering churned: 15135\n",
      "Alive users with churn(=1) in prediction window: 858\n",
      "Samples for this cutoff: 15135   Positives: 858   Positive rate: 0.05668979187314172\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-18\n",
      "Users in observation window (including already churned): 16972\n",
      "Alive users after filtering churned: 15251\n",
      "Alive users with churn(=1) in prediction window: 819\n",
      "Samples for this cutoff: 15251   Positives: 819   Positive rate: 0.0537013966297292\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-19\n",
      "Users in observation window (including already churned): 17196\n",
      "Alive users after filtering churned: 15377\n",
      "Alive users with churn(=1) in prediction window: 784\n",
      "Samples for this cutoff: 15377   Positives: 784   Positive rate: 0.050985237692657864\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-20\n",
      "Users in observation window (including already churned): 17347\n",
      "Alive users after filtering churned: 15434\n",
      "Alive users with churn(=1) in prediction window: 777\n",
      "Samples for this cutoff: 15434   Positives: 777   Positive rate: 0.05034339769340417\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-21\n",
      "Users in observation window (including already churned): 17416\n",
      "Alive users after filtering churned: 15452\n",
      "Alive users with churn(=1) in prediction window: 817\n",
      "Samples for this cutoff: 15452   Positives: 817   Positive rate: 0.05287341444473207\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-22\n",
      "Users in observation window (including already churned): 17500\n",
      "Alive users after filtering churned: 15471\n",
      "Alive users with churn(=1) in prediction window: 830\n",
      "Samples for this cutoff: 15471   Positives: 830   Positive rate: 0.05364876220024562\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-23\n",
      "Users in observation window (including already churned): 17630\n",
      "Alive users after filtering churned: 15503\n",
      "Alive users with churn(=1) in prediction window: 828\n",
      "Samples for this cutoff: 15503   Positives: 828   Positive rate: 0.05340901760949494\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-24\n",
      "Users in observation window (including already churned): 17771\n",
      "Alive users after filtering churned: 15552\n",
      "Alive users with churn(=1) in prediction window: 842\n",
      "Samples for this cutoff: 15552   Positives: 842   Positive rate: 0.05414094650205761\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-25\n",
      "Users in observation window (including already churned): 17888\n",
      "Alive users after filtering churned: 15571\n",
      "Alive users with churn(=1) in prediction window: 803\n",
      "Samples for this cutoff: 15571   Positives: 803   Positive rate: 0.05157022670348725\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-26\n",
      "Users in observation window (including already churned): 17993\n",
      "Alive users after filtering churned: 15575\n",
      "Alive users with churn(=1) in prediction window: 757\n",
      "Samples for this cutoff: 15575   Positives: 757   Positive rate: 0.04860353130016051\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-27\n",
      "Users in observation window (including already churned): 18096\n",
      "Alive users after filtering churned: 15574\n",
      "Alive users with churn(=1) in prediction window: 719\n",
      "Samples for this cutoff: 15574   Positives: 719   Positive rate: 0.046166688069860025\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-28\n",
      "Users in observation window (including already churned): 18136\n",
      "Alive users after filtering churned: 15561\n",
      "Alive users with churn(=1) in prediction window: 765\n",
      "Samples for this cutoff: 15561   Positives: 765   Positive rate: 0.049161364950838636\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-29\n",
      "Users in observation window (including already churned): 18191\n",
      "Alive users after filtering churned: 15563\n",
      "Alive users with churn(=1) in prediction window: 814\n",
      "Samples for this cutoff: 15563   Positives: 814   Positive rate: 0.05230354044849965\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-30\n",
      "Users in observation window (including already churned): 18271\n",
      "Alive users after filtering churned: 15558\n",
      "Alive users with churn(=1) in prediction window: 814\n",
      "Samples for this cutoff: 15558   Positives: 814   Positive rate: 0.052320349659339245\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-31\n",
      "Users in observation window (including already churned): 18333\n",
      "Alive users after filtering churned: 15530\n",
      "Alive users with churn(=1) in prediction window: 789\n",
      "Samples for this cutoff: 15530   Positives: 789   Positive rate: 0.05080489375402447\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-01\n",
      "Users in observation window (including already churned): 18419\n",
      "Alive users after filtering churned: 15541\n",
      "Alive users with churn(=1) in prediction window: 765\n",
      "Samples for this cutoff: 15541   Positives: 765   Positive rate: 0.0492246316195869\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-02\n",
      "Users in observation window (including already churned): 18493\n",
      "Alive users after filtering churned: 15519\n",
      "Alive users with churn(=1) in prediction window: 713\n",
      "Samples for this cutoff: 15519   Positives: 713   Positive rate: 0.04594368193826922\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-03\n",
      "Users in observation window (including already churned): 18562\n",
      "Alive users after filtering churned: 15486\n",
      "Alive users with churn(=1) in prediction window: 678\n",
      "Samples for this cutoff: 15486   Positives: 678   Positive rate: 0.043781480046493605\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-04\n",
      "Users in observation window (including already churned): 18592\n",
      "Alive users after filtering churned: 15460\n",
      "Alive users with churn(=1) in prediction window: 702\n",
      "Samples for this cutoff: 15460   Positives: 702   Positive rate: 0.04540750323415265\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-05\n",
      "Users in observation window (including already churned): 18620\n",
      "Alive users after filtering churned: 15433\n",
      "Alive users with churn(=1) in prediction window: 735\n",
      "Samples for this cutoff: 15433   Positives: 735   Positive rate: 0.04762521868722867\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-06\n",
      "Users in observation window (including already churned): 18670\n",
      "Alive users after filtering churned: 15417\n",
      "Alive users with churn(=1) in prediction window: 762\n",
      "Samples for this cutoff: 15417   Positives: 762   Positive rate: 0.04942595835765713\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-07\n",
      "Users in observation window (including already churned): 18716\n",
      "Alive users after filtering churned: 15363\n",
      "Alive users with churn(=1) in prediction window: 749\n",
      "Samples for this cutoff: 15363   Positives: 749   Positive rate: 0.0487534986656252\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-08\n",
      "Users in observation window (including already churned): 18775\n",
      "Alive users after filtering churned: 15324\n",
      "Alive users with churn(=1) in prediction window: 705\n",
      "Samples for this cutoff: 15324   Positives: 705   Positive rate: 0.046006264682850434\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-09\n",
      "Users in observation window (including already churned): 18827\n",
      "Alive users after filtering churned: 15289\n",
      "Alive users with churn(=1) in prediction window: 655\n",
      "Samples for this cutoff: 15289   Positives: 655   Positive rate: 0.042841258421087054\n"
     ]
    }
   ],
   "execution_count": 6,
   "source": [
    "all_samples_list = []\n",
    "\n",
    "for cutoff_date in cutoff_dates:\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"Current cutoff_date =\", cutoff_date.date())\n",
    "    \n",
    "    # ---- 2.1 Observation window & prediction window ----\n",
    "    # Observation window: ts <= cutoff_date\n",
    "    obs_mask = (train_df['ts'] <= cutoff_date)\n",
    "    \n",
    "    # Prediction window: cutoff_date < ts <= cutoff_date + horizon_days\n",
    "    future_end = cutoff_date + pd.Timedelta(days=horizon_days)\n",
    "    fut_mask = (\n",
    "        (train_df['ts'] > cutoff_date) &\n",
    "        (train_df['ts'] <= future_end)\n",
    "    )\n",
    "    \n",
    "    # ---- 2.2 Users present in the observation window ----\n",
    "    users_obs = train_df.loc[obs_mask, 'userId'].unique()\n",
    "    users_obs = np.sort(users_obs)\n",
    "    print(\"Users in observation window (including already churned):\", len(users_obs))\n",
    "    \n",
    "    if len(users_obs) == 0:\n",
    "        print(\"No observed users for this cutoff, skipping\")\n",
    "        continue\n",
    "\n",
    "    # ---- 2.2.1 Filter out users who already churned ----\n",
    "    # Reindex first_churn_ts on users_obs to get churn time or NaT\n",
    "    churn_ts_sub = first_churn_ts.reindex(users_obs)\n",
    "    \n",
    "    # Keep if:\n",
    "    #   - churn_ts is NaT  -> never churned  -> keep\n",
    "    #   - churn_ts > cutoff_date -> churn in the future -> keep\n",
    "    # Drop if:\n",
    "    #   - churn_ts <= cutoff_date -> already churned (or churned today) -> drop\n",
    "    alive_mask = (churn_ts_sub.isna()) | (churn_ts_sub > cutoff_date)\n",
    "    alive_users = users_obs[alive_mask.values]\n",
    "    \n",
    "    print(\"Alive users after filtering churned:\", len(alive_users))\n",
    "    \n",
    "    if len(alive_users) == 0:\n",
    "        print(\"No alive users for this cutoff, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # ---- 2.3 Users who click 'Cancellation Confirmation' in the prediction window ----\n",
    "    cc_future_users = (\n",
    "        train_df.loc[fut_mask & (train_df['page'] == \"Cancellation Confirmation\"), 'userId']\n",
    "        .unique()\n",
    "    )\n",
    "    \n",
    "    # Keep only users that are both alive and present in observation window\n",
    "    cc_future_users = np.intersect1d(cc_future_users, alive_users)\n",
    "    print(\"Alive users with churn(=1) in prediction window:\", len(cc_future_users))\n",
    "    \n",
    "    # ---- 2.4 Build label array for this cutoff ----\n",
    "    # alive_users is the true sample user list for this cutoff\n",
    "    # Default target=0; set target=1 if user is in cc_future_users\n",
    "    y_array = np.zeros(len(alive_users), dtype=int)\n",
    "    pos_mask = np.isin(alive_users, cc_future_users)\n",
    "    y_array[pos_mask] = 1\n",
    "    \n",
    "    # ---- 2.5 Assemble into a DataFrame ----\n",
    "    tmp = pd.DataFrame({\n",
    "        \"userId\": alive_users,\n",
    "        \"cutoff_date\": cutoff_date,   # same for the whole batch\n",
    "        \"target\": y_array,\n",
    "    })\n",
    "    \n",
    "    print(\"Samples for this cutoff:\", len(tmp),\n",
    "          \"  Positives:\", tmp['target'].sum(),\n",
    "          \"  Positive rate:\", tmp['target'].mean())\n",
    "    \n",
    "    all_samples_list.append(tmp)"
   ],
   "id": "d5c7850a59bf3b20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:42:11.954344200Z",
     "start_time": "2025-12-19T00:42:11.935139600Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Overall after merging all cutoffs:\n",
      "Total samples: 400774\n",
      "Total positives: 20143\n",
      "Overall positive rate: 0.050260246423171166\n",
      "\n",
      "Target distribution:\n",
      "target\n",
      "0    0.94974\n",
      "1    0.05026\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 7,
   "source": [
    "# 3. Concatenate all cutoffs to obtain the sliding-window label table\n",
    "\n",
    "sliding_labels = pd.concat(all_samples_list, ignore_index=True)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Overall after merging all cutoffs:\")\n",
    "print(\"Total samples:\", len(sliding_labels))\n",
    "print(\"Total positives:\", sliding_labels['target'].sum())\n",
    "print(\"Overall positive rate:\", sliding_labels['target'].mean())\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(sliding_labels['target'].value_counts(normalize=True))"
   ],
   "id": "553fb636251d2d02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Training set features"
   ],
   "id": "55083a7aa845f983"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1 Category features"
   ],
   "id": "41f126281b87af13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:42:18.474160300Z",
     "start_time": "2025-12-19T00:42:11.954344200Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sliding_labels with gender/state:\n",
      "    userId cutoff_date  target gender state\n",
      "0  1000025  2018-10-15       1      M    CT\n",
      "1  1000035  2018-10-15       0      F    SC\n",
      "2  1000103  2018-10-15       0      F    OH\n",
      "3  1000164  2018-10-15       0      F    AZ\n",
      "4  1000168  2018-10-15       0      M    FL\n"
     ]
    }
   ],
   "execution_count": 8,
   "source": [
    "# Static user info (one row per user; train_df is enough)\n",
    "user_static = (\n",
    "    train_df\n",
    "    .sort_values('ts')\n",
    "    .groupby('userId')\n",
    "    .agg({\n",
    "        'gender': 'first',   # or last; gender should not change\n",
    "        'state':  'first',   # last 2 chars extracted from location\n",
    "    })\n",
    ")\n",
    "\n",
    "# Map to sliding_labels\n",
    "sliding_labels['gender'] = sliding_labels['userId'].map(user_static['gender'])\n",
    "sliding_labels['state']  = sliding_labels['userId'].map(user_static['state'])\n",
    "\n",
    "print(\"sliding_labels with gender/state:\")\n",
    "print(sliding_labels[['userId', 'cutoff_date', 'target', 'gender', 'state']].head())"
   ],
   "id": "85c40477fa2d23e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2 Multi-Index"
   ],
   "id": "cdc0410b2d357690"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:42:18.797499700Z",
     "start_time": "2025-12-19T00:42:18.475216Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First rows of sliding_labels:\n",
      "                     userId cutoff_date  target gender state\n",
      "sample_id                                                   \n",
      "1000025_2018-10-15  1000025  2018-10-15       1      M    CT\n",
      "1000035_2018-10-15  1000035  2018-10-15       0      F    SC\n",
      "1000103_2018-10-15  1000103  2018-10-15       0      F    OH\n",
      "1000164_2018-10-15  1000164  2018-10-15       0      F    AZ\n",
      "1000168_2018-10-15  1000168  2018-10-15       0      M    FL\n",
      "First rows of y_all:\n",
      "sample_id\n",
      "1000025_2018-10-15    1\n",
      "1000035_2018-10-15    0\n",
      "1000103_2018-10-15    0\n",
      "1000164_2018-10-15    0\n",
      "1000168_2018-10-15    0\n",
      "Name: target, dtype: int32\n",
      "First rows of cutoff_ts_all:\n",
      "sample_id\n",
      "1000025_2018-10-15   2018-10-15\n",
      "1000035_2018-10-15   2018-10-15\n",
      "1000103_2018-10-15   2018-10-15\n",
      "1000164_2018-10-15   2018-10-15\n",
      "1000168_2018-10-15   2018-10-15\n",
      "Name: cutoff_date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "execution_count": 9,
   "source": [
    "# Use userId_cutoff_date as MultiIndex (required)\n",
    "sliding_labels['sample_id'] = (\n",
    "    sliding_labels['userId'].astype(str)\n",
    "    + \"_\" +\n",
    "    sliding_labels['cutoff_date'].astype(str)\n",
    ")\n",
    "\n",
    "# Set index\n",
    "sliding_labels = sliding_labels.set_index('sample_id')\n",
    "\n",
    "# Target variable y_all\n",
    "y_all = sliding_labels['target']\n",
    "\n",
    "# Cutoff timestamps (direct reference)\n",
    "cutoff_ts_all = sliding_labels['cutoff_date']\n",
    "\n",
    "\n",
    "print(\"First rows of sliding_labels:\")\n",
    "print(sliding_labels.head())\n",
    "print(\"First rows of y_all:\")\n",
    "print(y_all.head())\n",
    "print(\"First rows of cutoff_ts_all:\")\n",
    "print(cutoff_ts_all.head())"
   ],
   "id": "dccde7cc42206986"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.3 Lifetime"
   ],
   "id": "aea9361ef69f45a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:42:24.162556400Z",
     "start_time": "2025-12-19T00:42:18.799961200Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days_since_registration examples:\n",
      "sample_id\n",
      "1000025_2018-10-15    96.604073\n",
      "1000035_2018-10-15    32.188633\n",
      "1000103_2018-10-15    22.689295\n",
      "1000164_2018-10-15    63.602768\n",
      "1000168_2018-10-15    67.329018\n",
      "dtype: float32\n"
     ]
    }
   ],
   "execution_count": 10,
   "source": [
    "# Registration date per user\n",
    "uid_registration = (\n",
    "    train_df\n",
    "    .sort_values('ts')\n",
    "    .groupby('userId')['registration']\n",
    "    .first()\n",
    ")\n",
    "\n",
    "# Map to sliding_labels\n",
    "sliding_labels['registration_ts'] = sliding_labels['userId'].map(uid_registration)\n",
    "\n",
    "# Days from registration to cutoff\n",
    "days_since_registration = (\n",
    "    (sliding_labels['cutoff_date'] - sliding_labels['registration_ts'])\n",
    "    / np.timedelta64(1, 'D')\n",
    ").astype('float32')\n",
    "\n",
    "# Clip negative values (rare: abnormal registration timestamp)\n",
    "days_since_registration = days_since_registration.clip(lower=0)\n",
    "\n",
    "print(\"days_since_registration examples:\")\n",
    "print(days_since_registration.head())"
   ],
   "id": "6f2509742c54d5d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.4 Behaviors"
   ],
   "id": "103ada0d035d42bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:42:24.222638100Z",
     "start_time": "2025-12-19T00:42:24.206172200Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 11,
   "source": [
    "# Ensure cutoff_date is datetime\n",
    "sliding_labels['cutoff_date'] = pd.to_datetime(sliding_labels['cutoff_date'])"
   ],
   "id": "9262528ec14bbd4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:42:43.691214400Z",
     "start_time": "2025-12-19T00:42:24.217586400Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users (train): 19140\n",
      "Number of users (sliding_labels): 17351\n"
     ]
    }
   ],
   "execution_count": 12,
   "source": [
    "# 1) Sort train_df by userId + ts to ensure per-user chronological order\n",
    "train_df_sorted = train_df.sort_values(['userId', 'ts']).copy()\n",
    "\n",
    "# 2) Group behavior data by user for train\n",
    "train_groups = dict(tuple(train_df_sorted.groupby('userId')))\n",
    "\n",
    "# 3) Group samples by user on label side (sliding_labels index is sample_id)\n",
    "label_groups = dict(tuple(sliding_labels.groupby('userId')))  # user : label table\n",
    "\n",
    "print(\"Number of users (train):\", len(train_groups))\n",
    "print(\"Number of users (sliding_labels):\", len(label_groups))  # users appearing only after 2018-11-10 may be missing"
   ],
   "id": "d90fe346ec5dafa3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:42:47.248884200Z",
     "start_time": "2025-12-19T00:42:43.695273800Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_events computed\n",
      "sample_id\n",
      "1000025_2018-10-15    1259\n",
      "1000035_2018-10-15      88\n",
      "1000103_2018-10-15      51\n",
      "1000164_2018-10-15     166\n",
      "1000168_2018-10-15     306\n",
      "dtype: int32\n"
     ]
    }
   ],
   "execution_count": 13,
   "source": [
    "# Initialize series for event counts\n",
    "\n",
    "# Initialize: index = sample_id (sliding_labels.index), fill with 0\n",
    "n_events = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    # Skip if user has no behavior in train\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "\n",
    "    # All events for this user (already sorted by ts)\n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values  # datetime64[ns] array\n",
    "\n",
    "    # All samples (cutoffs) for this user\n",
    "    cutoffs = lbl_u['cutoff_date'].values  # cutoff times\n",
    "    sample_ids = lbl_u.index  # sample_id list\n",
    "\n",
    "    # For each cutoff, find number of events <= cutoff\n",
    "    pos = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "\n",
    "    # Write back to n_events aligned by sample_id\n",
    "    n_events.loc[sample_ids] = pos.astype('int32')\n",
    "\n",
    "print(\"n_events computed\")\n",
    "print(n_events.head())"
   ],
   "id": "731fee63105c3003"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.5 recency_hours"
   ],
   "id": "a3058ec4bfba401b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:42:58.656648Z",
     "start_time": "2025-12-19T00:42:47.253574100Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recency_hours computed\n",
      "sample_id\n",
      "1000025_2018-10-15     71.650000\n",
      "1000035_2018-10-15     77.664444\n",
      "1000103_2018-10-15    243.771944\n",
      "1000164_2018-10-15      6.444167\n",
      "1000168_2018-10-15     50.306667\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 14,
   "source": [
    "# Definition: cutoff_date - last event time\n",
    "\n",
    "# Initialize recency_hours (in hours)\n",
    "recency_hours = pd.Series(np.nan, index=sliding_labels.index, dtype='float32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values   # sorted datetime64 array\n",
    "    \n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "    \n",
    "    # hi[j] = number of events <= cutoff_j\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "    \n",
    "    # hi==0: no event before cutoff -> set large recency, e.g., 9999 hours\n",
    "    # hi>0: last event index is hi-1\n",
    "    for idx_in_uid, sample_id in enumerate(sample_ids):\n",
    "        h = hi[idx_in_uid]\n",
    "        \n",
    "        if h == 0:\n",
    "            # No events before cutoff -> very large recency (9999 hours ~ 416 days)\n",
    "            recency_hours.loc[sample_id] = 9999.0\n",
    "        else:\n",
    "            last_ts = ts_vals[h-1]\n",
    "            delta = (cutoffs[idx_in_uid] - last_ts)\n",
    "            recency_hours.loc[sample_id] = delta / np.timedelta64(1, 'h')\n",
    "\n",
    "print(\"recency_hours computed\")\n",
    "print(recency_hours.head())"
   ],
   "id": "47b2571740fd699"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.6 events_last_7d"
   ],
   "id": "4d1e16d8ce394995"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:43:00.780672100Z",
     "start_time": "2025-12-19T00:42:58.658732600Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events_last_7d computed\n",
      "sample_id\n",
      "1000025_2018-10-15    356\n",
      "1000035_2018-10-15     82\n",
      "1000103_2018-10-15      0\n",
      "1000164_2018-10-15    163\n",
      "1000168_2018-10-15    195\n",
      "dtype: int32\n"
     ]
    }
   ],
   "execution_count": 15,
   "source": [
    "events_last_7d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "seven_days = np.timedelta64(7, 'D')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values             # sorted datetime64 array\n",
    "    cutoffs = lbl_u['cutoff_date'].values   # cutoff array\n",
    "    sample_ids = lbl_u.index\n",
    "    \n",
    "    # hi = number of events <= cutoff\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "\n",
    "    # lo = number of events <= cutoff - 7 days\n",
    "    window_starts = cutoffs - seven_days\n",
    "    lo = np.searchsorted(ts_vals, window_starts, side='right')\n",
    "    \n",
    "    # last 7 days events = hi - lo\n",
    "    cnt_7d = hi - lo\n",
    "    events_last_7d.loc[sample_ids] = cnt_7d.astype('int32')\n",
    "\n",
    "print(\"events_last_7d computed\")\n",
    "print(events_last_7d.head())"
   ],
   "id": "8577293c6c8744ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.7 songs_last_7d"
   ],
   "id": "38138798bff65ea0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:43:10.454965500Z",
     "start_time": "2025-12-19T00:43:00.781712600Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "songs_last_7d computed\n",
      "sample_id\n",
      "1000025_2018-10-15    292\n",
      "1000035_2018-10-15     63\n",
      "1000103_2018-10-15      0\n",
      "1000164_2018-10-15    125\n",
      "1000168_2018-10-15    154\n",
      "dtype: int32\n"
     ]
    }
   ],
   "execution_count": 16,
   "source": [
    "songs_last_7d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "seven_days = np.timedelta64(7, 'D')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    # Filter song-play events (NextSong)\n",
    "    df_song = df_u[df_u['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        # User never listened -> all zeros\n",
    "        continue\n",
    "    \n",
    "    ts_song_vals = df_song['ts'].values  # sorted datetime array\n",
    "\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi = number of songs <= cutoff\n",
    "    hi = np.searchsorted(ts_song_vals, cutoffs, side='right')\n",
    "\n",
    "    # lo = number of songs <= cutoff - 7 days\n",
    "    window_starts = cutoffs - seven_days\n",
    "    lo = np.searchsorted(ts_song_vals, window_starts, side='right')\n",
    "\n",
    "    cnt_7d = hi - lo\n",
    "    songs_last_7d.loc[sample_ids] = cnt_7d.astype('int32')\n",
    "\n",
    "print(\"songs_last_7d computed\")\n",
    "print(songs_last_7d.head())"
   ],
   "id": "369504b05a38c01c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.8 active_days"
   ],
   "id": "9283fdb1bb27050b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:43:15.200306800Z",
     "start_time": "2025-12-19T00:43:10.456983200Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active_days computed\n",
      "sample_id\n",
      "1000025_2018-10-15    11\n",
      "1000035_2018-10-15     4\n",
      "1000103_2018-10-15     1\n",
      "1000164_2018-10-15     7\n",
      "1000168_2018-10-15     3\n",
      "dtype: int32\n"
     ]
    }
   ],
   "execution_count": 17,
   "source": [
    "active_days = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    # Extract dates (keep ordering by ts)\n",
    "    dates_u = df_u['ts'].dt.normalize().values  # remove time, keep date only\n",
    "\n",
    "    # Unique dates (sorted)\n",
    "    unique_days = np.unique(dates_u)\n",
    "    \n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "    \n",
    "    # Use searchsorted on date array:\n",
    "    # number of unique days <= cutoff_date\n",
    "    hi = np.searchsorted(unique_days, cutoffs, side='right')\n",
    "\n",
    "    # hi[j] is the number of active days up to cutoff\n",
    "    active_days.loc[sample_ids] = hi.astype('int32')\n",
    "\n",
    "print(\"active_days computed\")\n",
    "print(active_days.head())"
   ],
   "id": "d772375335fe74f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.9 total_listen_time"
   ],
   "id": "4f803a2982b13050"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:43:31.748509700Z",
     "start_time": "2025-12-19T00:43:15.203640300Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_listen_time computed\n",
      "sample_id\n",
      "1000025_2018-10-15    257291.02684\n",
      "1000035_2018-10-15     16741.11619\n",
      "1000103_2018-10-15      9769.04682\n",
      "1000164_2018-10-15     31069.38246\n",
      "1000168_2018-10-15     58290.17823\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 18,
   "source": [
    "total_listen_time = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    # Filter NextSong rows\n",
    "    df_song = df_u[df_u['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        continue\n",
    "    \n",
    "    ts_song_vals = df_song['ts'].values         # song timestamps (sorted)\n",
    "    len_song_vals = df_song['length'].values    # song durations (float)\n",
    "\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # Boundary for songs <= cutoff\n",
    "    hi = np.searchsorted(ts_song_vals, cutoffs, side='right')\n",
    "\n",
    "    # Prefix sum for faster cumulative duration\n",
    "    cum_len = np.cumsum(len_song_vals)\n",
    "\n",
    "    for j, sample_id in enumerate(sample_ids):\n",
    "        h = hi[j]\n",
    "        if h == 0:\n",
    "            total_listen_time.loc[sample_id] = 0.0\n",
    "        else:\n",
    "            total_listen_time.loc[sample_id] = float(cum_len[h-1])\n",
    "\n",
    "print(\"total_listen_time computed\")\n",
    "print(total_listen_time.head())"
   ],
   "id": "f626c348cd1020ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.10 events_last_1d/3d"
   ],
   "id": "354dbda41276ddbe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:43:35.683809800Z",
     "start_time": "2025-12-19T00:43:31.750545800Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events_last_1d / 3d computed\n",
      "sample_id\n",
      "1000025_2018-10-15    0\n",
      "1000035_2018-10-15    0\n",
      "1000103_2018-10-15    0\n",
      "1000164_2018-10-15    5\n",
      "1000168_2018-10-15    0\n",
      "dtype: int32\n",
      "sample_id\n",
      "1000025_2018-10-15      7\n",
      "1000035_2018-10-15      0\n",
      "1000103_2018-10-15      0\n",
      "1000164_2018-10-15     21\n",
      "1000168_2018-10-15    195\n",
      "dtype: int32\n"
     ]
    }
   ],
   "execution_count": 19,
   "source": [
    "events_last_1d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "events_last_3d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "one_day   = np.timedelta64(1, 'D')\n",
    "three_days = np.timedelta64(3, 'D')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi: number of events <= cutoff (same as n_events)\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "\n",
    "    # ---- last 1 day ----\n",
    "    window_start_1d = cutoffs - one_day\n",
    "    lo_1d = np.searchsorted(ts_vals, window_start_1d, side='right')\n",
    "    cnt_1d = hi - lo_1d\n",
    "    events_last_1d.loc[sample_ids] = cnt_1d.astype('int32')\n",
    "\n",
    "    # ---- last 3 days ----\n",
    "    window_start_3d = cutoffs - three_days\n",
    "    lo_3d = np.searchsorted(ts_vals, window_start_3d, side='right')\n",
    "    cnt_3d = hi - lo_3d\n",
    "    events_last_3d.loc[sample_ids] = cnt_3d.astype('int32')\n",
    "\n",
    "print(\"events_last_1d / 3d computed\")\n",
    "print(events_last_1d.head())\n",
    "print(events_last_3d.head())"
   ],
   "id": "a454c3a9be1d5a9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.11 songs_last_1d/3d"
   ],
   "id": "e7edfddb8739afef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:43:46.347544900Z",
     "start_time": "2025-12-19T00:43:35.686014100Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "songs_last_1d / 3d computed\n",
      "sample_id\n",
      "1000025_2018-10-15    0\n",
      "1000035_2018-10-15    0\n",
      "1000103_2018-10-15    0\n",
      "1000164_2018-10-15    3\n",
      "1000168_2018-10-15    0\n",
      "dtype: int32\n",
      "sample_id\n",
      "1000025_2018-10-15      6\n",
      "1000035_2018-10-15      0\n",
      "1000103_2018-10-15      0\n",
      "1000164_2018-10-15     14\n",
      "1000168_2018-10-15    154\n",
      "dtype: int32\n"
     ]
    }
   ],
   "execution_count": 20,
   "source": [
    "songs_last_1d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "songs_last_3d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "one_day    = np.timedelta64(1, 'D')\n",
    "three_days = np.timedelta64(3, 'D')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    df_song = df_u[df_u['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        continue\n",
    "    \n",
    "    ts_song_vals = df_song['ts'].values\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi: number of songs <= cutoff\n",
    "    hi = np.searchsorted(ts_song_vals, cutoffs, side='right')\n",
    "\n",
    "    # ---- last 1 day ----\n",
    "    window_start_1d = cutoffs - one_day\n",
    "    lo_1d = np.searchsorted(ts_song_vals, window_start_1d, side='right')\n",
    "    cnt_1d = hi - lo_1d\n",
    "    songs_last_1d.loc[sample_ids] = cnt_1d.astype('int32')\n",
    "\n",
    "    # ---- last 3 days ----\n",
    "    window_start_3d = cutoffs - three_days\n",
    "    lo_3d = np.searchsorted(ts_song_vals, window_start_3d, side='right')\n",
    "    cnt_3d = hi - lo_3d\n",
    "    songs_last_3d.loc[sample_ids] = cnt_3d.astype('int32')\n",
    "\n",
    "print(\"songs_last_1d / 3d computed\")\n",
    "print(songs_last_1d.head())\n",
    "print(songs_last_3d.head())"
   ],
   "id": "bdd7d058515627f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.12 Level-at-cutoff"
   ],
   "id": "44308cced60fd8bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:43:49.980867Z",
     "start_time": "2025-12-19T00:43:46.348574400Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level-at-cutoff computed\n",
      "sample_id\n",
      "1000025_2018-10-15    paid\n",
      "1000035_2018-10-15    free\n",
      "1000103_2018-10-15    paid\n",
      "1000164_2018-10-15    free\n",
      "1000168_2018-10-15    paid\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 21,
   "source": [
    "level_at_cutoff = pd.Series(\"unknown\", index=sliding_labels.index, dtype=object)\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals   = df_u['ts'].values\n",
    "    lvl_vals  = df_u['level'].astype(str).values   # level sequence (free/paid)\n",
    "    \n",
    "    cutoffs    = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "    \n",
    "    # hi[j] = number of events <= cutoff_j\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "    \n",
    "    # hi[j] == 0: no events before cutoff\n",
    "    # else: last level is lvl_vals[hi[j] - 1]\n",
    "    lvl_for_samples = []\n",
    "    for j, h in enumerate(hi):\n",
    "        if h == 0:\n",
    "            lvl_for_samples.append(\"unknown\")\n",
    "        else:\n",
    "            lvl_for_samples.append(lvl_vals[h-1])\n",
    "    \n",
    "    level_at_cutoff.loc[sample_ids] = lvl_for_samples\n",
    "    \n",
    "print(\"Level-at-cutoff computed\")\n",
    "print(level_at_cutoff.head())"
   ],
   "id": "c6d0859663453248"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:43:49.988042100Z",
     "start_time": "2025-12-19T00:43:49.980867Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 22,
   "source": [
    "# Write level_at_cutoff back to sliding_labels for one-hot later\n",
    "sliding_labels['level'] = level_at_cutoff"
   ],
   "id": "8bbc9cb6c2f9effc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.13 Sessions"
   ],
   "id": "2abd6b38e9f05ae2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:46:18.772786700Z",
     "start_time": "2025-12-19T00:43:49.990040900Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session features computed\n",
      "sample_id\n",
      "1000025_2018-10-15    11\n",
      "1000035_2018-10-15     4\n",
      "1000103_2018-10-15     1\n",
      "1000164_2018-10-15     7\n",
      "1000168_2018-10-15     3\n",
      "dtype: int32 sample_id\n",
      "1000025_2018-10-15    23176.090909\n",
      "1000035_2018-10-15     4109.750000\n",
      "1000103_2018-10-15     9744.000000\n",
      "1000164_2018-10-15     4364.000000\n",
      "1000168_2018-10-15    19311.333333\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 23,
   "source": [
    "# Initialize series (index is still sample_id)\n",
    "session_count = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "mean_session_duration = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "max_session_duration = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "min_session_duration = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "std_session_duration = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "\n",
    "mean_event_count_per_session = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "max_event_count_per_session = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "min_event_count_per_session = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "std_event_count_per_session = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "\n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    # Group by sessionId and compute start/end/count per session\n",
    "    sess_u = (\n",
    "        df_u\n",
    "        .groupby('sessionId')\n",
    "        .agg(\n",
    "            session_start=('ts', 'min'),\n",
    "            session_end=('ts', 'max'),\n",
    "            session_event_count=('ts', 'count'),\n",
    "        )\n",
    "        .sort_values('session_start')\n",
    "    )\n",
    "\n",
    "    if sess_u.empty:\n",
    "        continue\n",
    "\n",
    "    # Convert to numpy for faster searchsorted\n",
    "    sess_start_vals = sess_u['session_start'].values\n",
    "    sess_end_vals = sess_u['session_end'].values\n",
    "    sess_event_vals = sess_u['session_event_count'].values.astype('int32')\n",
    "    sess_dur_vals = (sess_end_vals - sess_start_vals) / np.timedelta64(1, 's')  # seconds\n",
    "\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi_sess[j] = number of sessions that started <= cutoff_j\n",
    "    hi_sess = np.searchsorted(sess_start_vals, cutoffs, side='right')\n",
    "\n",
    "    for j, sample_id in enumerate(sample_ids):\n",
    "        h = hi_sess[j]\n",
    "        if h == 0:\n",
    "            # No sessions up to this cutoff\n",
    "            session_count.loc[sample_id] = 0\n",
    "            continue\n",
    "\n",
    "        # Use the first h sessions for stats\n",
    "        dur_subset = sess_dur_vals[:h]\n",
    "        cnt_subset = sess_event_vals[:h]\n",
    "\n",
    "        session_count.loc[sample_id] = h\n",
    "        mean_session_duration.loc[sample_id] = dur_subset.mean()\n",
    "        max_session_duration.loc[sample_id] = dur_subset.max()\n",
    "        min_session_duration.loc[sample_id] = dur_subset.min()\n",
    "        std_session_duration.loc[sample_id] = dur_subset.std(ddof=0)\n",
    "\n",
    "        mean_event_count_per_session.loc[sample_id] = cnt_subset.mean()\n",
    "        max_event_count_per_session.loc[sample_id] = cnt_subset.max()\n",
    "        min_event_count_per_session.loc[sample_id] = cnt_subset.min()\n",
    "        std_event_count_per_session.loc[sample_id] = cnt_subset.std(ddof=0)\n",
    "\n",
    "print(\"Session features computed\")\n",
    "print(session_count.head(), mean_session_duration.head())"
   ],
   "id": "b309e7ef74ef6775"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.14 Page Ratio"
   ],
   "id": "f5c865bb57b5a2f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:47:16.857761500Z",
     "start_time": "2025-12-19T00:46:18.775833200Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page count features computed\n"
     ]
    }
   ],
   "execution_count": 24,
   "source": [
    "pages_of_interest = [\n",
    "    \"NextSong\",\n",
    "    \"Thumbs Up\",\n",
    "    \"Thumbs Down\",\n",
    "    \"Add to Playlist\",\n",
    "    \"Roll Advert\",\n",
    "    \"Help\",\n",
    "    \"Error\",\n",
    "    \"Submit Upgrade\",\n",
    "    \"Submit Downgrade\",\n",
    "]\n",
    "\n",
    "# Initialize: one Series per page\n",
    "page_count_series = {\n",
    "    p: pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "    for p in pages_of_interest\n",
    "}\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "\n",
    "    df_u = train_groups[uid]\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # For each page, build timestamp array and use searchsorted\n",
    "    for p in pages_of_interest:\n",
    "        df_p = df_u[df_u['page'] == p]\n",
    "        if df_p.empty:\n",
    "            continue\n",
    "\n",
    "        ts_p = df_p['ts'].values\n",
    "        hi_p = np.searchsorted(ts_p, cutoffs, side='right')  # count of this page <= cutoff\n",
    "\n",
    "        page_count_series[p].loc[sample_ids] = hi_p.astype('int32')\n",
    "\n",
    "print(\"Page count features computed\")"
   ],
   "id": "d73cb11c09ecaf46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.15 Other level features"
   ],
   "id": "61552afb8b6bc951"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:47:26.330749500Z",
     "start_time": "2025-12-19T00:47:16.860287500Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level features computed\n",
      "sample_id\n",
      "1000025_2018-10-15    1\n",
      "1000035_2018-10-15    1\n",
      "1000103_2018-10-15    1\n",
      "1000164_2018-10-15    1\n",
      "1000168_2018-10-15    1\n",
      "Name: userId, dtype: int8 sample_id\n",
      "1000025_2018-10-15    1\n",
      "1000035_2018-10-15    0\n",
      "1000103_2018-10-15    1\n",
      "1000164_2018-10-15    0\n",
      "1000168_2018-10-15    1\n",
      "dtype: int32\n"
     ]
    }
   ],
   "execution_count": 25,
   "source": [
    "# 1) ever_paid: whether the user ever had paid in history (static)\n",
    "ever_paid_uid = (\n",
    "    train_df\n",
    "    .groupby('userId')['level']\n",
    "    .apply(lambda s: int((s.astype(str) == \"paid\").any()))\n",
    ")\n",
    "\n",
    "ever_paid = sliding_labels['userId'].map(ever_paid_uid).fillna(0).astype('int8')\n",
    "\n",
    "# 2) n_level_change: number of free/paid switches up to cutoff\n",
    "n_level_change = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "\n",
    "    df_u = train_groups[uid].copy()\n",
    "    # level may have NaN; cast to string\n",
    "    lvl_vals = df_u['level'].astype(str).values\n",
    "    ts_vals = df_u['ts'].values\n",
    "\n",
    "    if len(lvl_vals) <= 1:\n",
    "        continue\n",
    "\n",
    "    # Find timestamps where level changes\n",
    "    change_mask = lvl_vals[1:] != lvl_vals[:-1]\n",
    "    change_ts = ts_vals[1:][change_mask]\n",
    "\n",
    "    if len(change_ts) == 0:\n",
    "        continue\n",
    "\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi_change[j] = number of level changes <= cutoff_j\n",
    "    hi_change = np.searchsorted(change_ts, cutoffs, side='right')\n",
    "    n_level_change.loc[sample_ids] = hi_change.astype('int32')\n",
    "\n",
    "print(\"Level features computed\")\n",
    "print(ever_paid.head(), n_level_change.head())"
   ],
   "id": "d910f1fe3c15e8dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.16 Status"
   ],
   "id": "a12a77ba5b562af8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:47:46.853170400Z",
     "start_time": "2025-12-19T00:47:26.332743500Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status count features computed\n"
     ]
    }
   ],
   "execution_count": 26,
   "source": [
    "status_codes_of_interest = [200, 404, 307]\n",
    "\n",
    "status_count_series = {\n",
    "    code: pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "    for code in status_codes_of_interest\n",
    "}\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "\n",
    "    df_u = train_groups[uid]\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    for code in status_codes_of_interest:\n",
    "        df_code = df_u[df_u['status'] == code]\n",
    "        if df_code.empty:\n",
    "            continue\n",
    "\n",
    "        ts_code = df_code['ts'].values\n",
    "        hi_code = np.searchsorted(ts_code, cutoffs, side='right')\n",
    "\n",
    "        status_count_series[code].loc[sample_ids] = hi_code.astype('int32')\n",
    "\n",
    "print(\"Status count features computed\")"
   ],
   "id": "8f49ffe3a28670dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.13 Combining the feature"
   ],
   "id": "da0fcfe162aab1a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:47:46.982819Z",
     "start_time": "2025-12-19T00:47:46.856223800Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page count + ratio features added to X_all\n",
      "Status features added to X_all\n",
      "                    n_status_404  frac_status_404\n",
      "sample_id                                        \n",
      "1000025_2018-10-15             0              0.0\n",
      "1000035_2018-10-15             0              0.0\n",
      "1000103_2018-10-15             0              0.0\n",
      "1000164_2018-10-15             0              0.0\n",
      "1000168_2018-10-15             0              0.0\n",
      "Numeric features merged into X_all\n",
      "X_all shape: (400774, 45)\n",
      "                    days_since_registration  n_events  recency_hours  \\\n",
      "sample_id                                                              \n",
      "1000025_2018-10-15                96.604073      1259      71.650002   \n",
      "1000035_2018-10-15                32.188633        88      77.664444   \n",
      "1000103_2018-10-15                22.689295        51     243.771942   \n",
      "1000164_2018-10-15                63.602768       166       6.444167   \n",
      "1000168_2018-10-15                67.329018       306      50.306667   \n",
      "\n",
      "                    events_last_7d  songs_last_7d  active_days  \\\n",
      "sample_id                                                        \n",
      "1000025_2018-10-15             356            292           11   \n",
      "1000035_2018-10-15              82             63            4   \n",
      "1000103_2018-10-15               0              0            1   \n",
      "1000164_2018-10-15             163            125            7   \n",
      "1000168_2018-10-15             195            154            3   \n",
      "\n",
      "                    total_listen_time  events_last_1d  events_last_3d  \\\n",
      "sample_id                                                               \n",
      "1000025_2018-10-15      257291.031250               0               7   \n",
      "1000035_2018-10-15       16741.115234               0               0   \n",
      "1000103_2018-10-15        9769.046875               0               0   \n",
      "1000164_2018-10-15       31069.382812               5              21   \n",
      "1000168_2018-10-15       58290.179688               0             195   \n",
      "\n",
      "                    songs_last_1d  ...  ratio_page_Submit_Upgrade  \\\n",
      "sample_id                          ...                              \n",
      "1000025_2018-10-15              0  ...                   0.000794   \n",
      "1000035_2018-10-15              0  ...                   0.000000   \n",
      "1000103_2018-10-15              0  ...                   0.019608   \n",
      "1000164_2018-10-15              3  ...                   0.000000   \n",
      "1000168_2018-10-15              0  ...                   0.003268   \n",
      "\n",
      "                    cnt_page_Submit_Downgrade  ratio_page_Submit_Downgrade  \\\n",
      "sample_id                                                                    \n",
      "1000025_2018-10-15                          0                          0.0   \n",
      "1000035_2018-10-15                          0                          0.0   \n",
      "1000103_2018-10-15                          0                          0.0   \n",
      "1000164_2018-10-15                          0                          0.0   \n",
      "1000168_2018-10-15                          0                          0.0   \n",
      "\n",
      "                    ever_paid  n_level_change  n_status_200  n_status_404  \\\n",
      "sample_id                                                                   \n",
      "1000025_2018-10-15          1               1          1145             0   \n",
      "1000035_2018-10-15          1               0            80             0   \n",
      "1000103_2018-10-15          1               1            48             0   \n",
      "1000164_2018-10-15          1               0           154             0   \n",
      "1000168_2018-10-15          1               1           266             0   \n",
      "\n",
      "                    n_status_307  frac_status_404  frac_status_200  \n",
      "sample_id                                                           \n",
      "1000025_2018-10-15           114              0.0         0.909452  \n",
      "1000035_2018-10-15             8              0.0         0.909091  \n",
      "1000103_2018-10-15             3              0.0         0.941176  \n",
      "1000164_2018-10-15            12              0.0         0.927711  \n",
      "1000168_2018-10-15            40              0.0         0.869281  \n",
      "\n",
      "[5 rows x 45 columns]\n",
      "\n",
      "X_all dtypes:\n",
      "days_since_registration         float32\n",
      "n_events                          int32\n",
      "recency_hours                   float32\n",
      "events_last_7d                    int32\n",
      "songs_last_7d                     int32\n",
      "active_days                       int32\n",
      "total_listen_time               float32\n",
      "events_last_1d                    int32\n",
      "events_last_3d                    int32\n",
      "songs_last_1d                     int32\n",
      "songs_last_3d                     int32\n",
      "session_count                     int32\n",
      "mean_session_duration           float64\n",
      "max_session_duration            float32\n",
      "min_session_duration            float32\n",
      "std_session_duration            float64\n",
      "mean_event_count_per_session    float64\n",
      "max_event_count_per_session     float32\n",
      "min_event_count_per_session     float32\n",
      "std_event_count_per_session     float64\n",
      "cnt_page_NextSong                 int32\n",
      "ratio_page_NextSong             float32\n",
      "cnt_page_Thumbs_Up                int32\n",
      "ratio_page_Thumbs_Up            float32\n",
      "cnt_page_Thumbs_Down              int32\n",
      "ratio_page_Thumbs_Down          float32\n",
      "cnt_page_Add_to_Playlist          int32\n",
      "ratio_page_Add_to_Playlist      float32\n",
      "cnt_page_Roll_Advert              int32\n",
      "ratio_page_Roll_Advert          float32\n",
      "cnt_page_Help                     int32\n",
      "ratio_page_Help                 float32\n",
      "cnt_page_Error                    int32\n",
      "ratio_page_Error                float32\n",
      "cnt_page_Submit_Upgrade           int32\n",
      "ratio_page_Submit_Upgrade       float32\n",
      "cnt_page_Submit_Downgrade         int32\n",
      "ratio_page_Submit_Downgrade     float32\n",
      "ever_paid                          int8\n",
      "n_level_change                    int32\n",
      "n_status_200                      int32\n",
      "n_status_404                      int32\n",
      "n_status_307                      int32\n",
      "frac_status_404                 float32\n",
      "frac_status_200                 float32\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 27,
   "source": [
    "# Empty feature table; ensure X_all index is sample_id\n",
    "X_all = pd.DataFrame(index=sliding_labels.index)\n",
    "\n",
    "# 1) Lifecycle\n",
    "X_all['days_since_registration'] = days_since_registration\n",
    "\n",
    "# 2) Cumulative event count\n",
    "X_all['n_events'] = n_events.astype('int32')\n",
    "\n",
    "# 3) Time since last event (hours)\n",
    "X_all['recency_hours'] = recency_hours.astype('float32')\n",
    "\n",
    "# 4) Last 7 days events and songs\n",
    "X_all['events_last_7d'] = events_last_7d.astype('int32')\n",
    "X_all['songs_last_7d']  = songs_last_7d.astype('int32')\n",
    "\n",
    "# 5) Active days\n",
    "X_all['active_days'] = active_days.astype('int32')\n",
    "\n",
    "# 6) Total listening time\n",
    "X_all['total_listen_time'] = total_listen_time.astype('float32')\n",
    "\n",
    "# 7) Last 1 day / 3 days event counts\n",
    "X_all['events_last_1d'] = events_last_1d.astype('int32')\n",
    "X_all['events_last_3d'] = events_last_3d.astype('int32')\n",
    "\n",
    "# 8) Last 1 day / 3 days song counts\n",
    "X_all['songs_last_1d'] = songs_last_1d.astype('int32')\n",
    "X_all['songs_last_3d'] = songs_last_3d.astype('int32')\n",
    "\n",
    "# Session-related features\n",
    "X_all['session_count']                = session_count\n",
    "X_all['mean_session_duration']        = mean_session_duration\n",
    "X_all['max_session_duration']         = max_session_duration\n",
    "X_all['min_session_duration']         = min_session_duration\n",
    "X_all['std_session_duration']         = std_session_duration\n",
    "\n",
    "X_all['mean_event_count_per_session'] = mean_event_count_per_session\n",
    "X_all['max_event_count_per_session']  = max_event_count_per_session\n",
    "X_all['min_event_count_per_session']  = min_event_count_per_session\n",
    "X_all['std_event_count_per_session']  = std_event_count_per_session\n",
    "\n",
    "# Add page counts and ratio features\n",
    "for p in pages_of_interest:\n",
    "    safe_name = p.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    col_cnt   = f\"cnt_page_{safe_name}\"\n",
    "    col_ratio = f\"ratio_page_{safe_name}\"\n",
    "    \n",
    "    X_all[col_cnt] = page_count_series[p]\n",
    "    # ratio = page count / total events (n_events)\n",
    "    X_all[col_ratio] = (\n",
    "        X_all[col_cnt] / (X_all['n_events'] + eps)\n",
    "    ).astype('float32')\n",
    "\n",
    "print(\"Page count + ratio features added to X_all\")\n",
    "\n",
    "# Paid-related\n",
    "X_all['ever_paid']      = ever_paid\n",
    "X_all['n_level_change'] = n_level_change\n",
    "\n",
    "# Status-related\n",
    "for code in status_codes_of_interest:\n",
    "    col_cnt = f\"n_status_{code}\"\n",
    "    X_all[col_cnt] = status_count_series[code]\n",
    "\n",
    "# 404 fraction\n",
    "X_all['frac_status_404'] = (\n",
    "    X_all['n_status_404'] / (X_all['n_events'] + eps)\n",
    ").astype('float32')\n",
    "\n",
    "# 200 fraction (optional)\n",
    "X_all['frac_status_200'] = (\n",
    "    X_all['n_status_200'] / (X_all['n_events'] + eps)\n",
    ").astype('float32')\n",
    "\n",
    "print(\"Status features added to X_all\")\n",
    "print(X_all[['n_status_404', 'frac_status_404']].head())\n",
    "\n",
    "\n",
    "print(\"Numeric features merged into X_all\")\n",
    "print(\"X_all shape:\", X_all.shape)\n",
    "print(X_all.head())\n",
    "print(\"\\nX_all dtypes:\")\n",
    "print(X_all.dtypes)"
   ],
   "id": "2bf90727e0673af1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.14 Category + One-Hot"
   ],
   "id": "c3777f4deb042163"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:47:47.362983300Z",
     "start_time": "2025-12-19T00:47:46.983780200Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical one-hot feature example:\n",
      "                    gender_F  gender_M  state_AK  state_AL  state_AR  \\\n",
      "sample_id                                                              \n",
      "1000025_2018-10-15     False      True     False     False     False   \n",
      "1000035_2018-10-15      True     False     False     False     False   \n",
      "1000103_2018-10-15      True     False     False     False     False   \n",
      "1000164_2018-10-15      True     False     False     False     False   \n",
      "1000168_2018-10-15     False      True     False     False     False   \n",
      "\n",
      "                    state_AZ  state_CA  state_CO  state_CT  state_DE  ...  \\\n",
      "sample_id                                                             ...   \n",
      "1000025_2018-10-15     False     False     False      True     False  ...   \n",
      "1000035_2018-10-15     False     False     False     False     False  ...   \n",
      "1000103_2018-10-15     False     False     False     False     False  ...   \n",
      "1000164_2018-10-15      True     False     False     False     False  ...   \n",
      "1000168_2018-10-15     False     False     False     False     False  ...   \n",
      "\n",
      "                    state_TX  state_UT  state_VA  state_VT  state_WA  \\\n",
      "sample_id                                                              \n",
      "1000025_2018-10-15     False     False     False     False     False   \n",
      "1000035_2018-10-15     False     False     False     False     False   \n",
      "1000103_2018-10-15     False     False     False     False     False   \n",
      "1000164_2018-10-15     False     False     False     False     False   \n",
      "1000168_2018-10-15     False     False     False     False     False   \n",
      "\n",
      "                    state_WI  state_WV  state_WY  level_free  level_paid  \n",
      "sample_id                                                                 \n",
      "1000025_2018-10-15     False     False     False       False        True  \n",
      "1000035_2018-10-15     False     False     False        True       False  \n",
      "1000103_2018-10-15     False     False     False       False        True  \n",
      "1000164_2018-10-15     False     False     False        True       False  \n",
      "1000168_2018-10-15     False     False     False       False        True  \n",
      "\n",
      "[5 rows x 53 columns]\n",
      "X_train shape: (400774, 98)\n",
      "X_train example:\n",
      "                    days_since_registration  n_events  recency_hours  \\\n",
      "sample_id                                                              \n",
      "1000025_2018-10-15                96.604073      1259      71.650002   \n",
      "1000035_2018-10-15                32.188633        88      77.664444   \n",
      "1000103_2018-10-15                22.689295        51     243.771942   \n",
      "1000164_2018-10-15                63.602768       166       6.444167   \n",
      "1000168_2018-10-15                67.329018       306      50.306667   \n",
      "\n",
      "                    events_last_7d  songs_last_7d  active_days  \\\n",
      "sample_id                                                        \n",
      "1000025_2018-10-15             356            292           11   \n",
      "1000035_2018-10-15              82             63            4   \n",
      "1000103_2018-10-15               0              0            1   \n",
      "1000164_2018-10-15             163            125            7   \n",
      "1000168_2018-10-15             195            154            3   \n",
      "\n",
      "                    total_listen_time  events_last_1d  events_last_3d  \\\n",
      "sample_id                                                               \n",
      "1000025_2018-10-15      257291.031250               0               7   \n",
      "1000035_2018-10-15       16741.115234               0               0   \n",
      "1000103_2018-10-15        9769.046875               0               0   \n",
      "1000164_2018-10-15       31069.382812               5              21   \n",
      "1000168_2018-10-15       58290.179688               0             195   \n",
      "\n",
      "                    songs_last_1d  ...  state_TX  state_UT  state_VA  \\\n",
      "sample_id                          ...                                 \n",
      "1000025_2018-10-15              0  ...     False     False     False   \n",
      "1000035_2018-10-15              0  ...     False     False     False   \n",
      "1000103_2018-10-15              0  ...     False     False     False   \n",
      "1000164_2018-10-15              3  ...     False     False     False   \n",
      "1000168_2018-10-15              0  ...     False     False     False   \n",
      "\n",
      "                    state_VT  state_WA  state_WI  state_WV  state_WY  \\\n",
      "sample_id                                                              \n",
      "1000025_2018-10-15     False     False     False     False     False   \n",
      "1000035_2018-10-15     False     False     False     False     False   \n",
      "1000103_2018-10-15     False     False     False     False     False   \n",
      "1000164_2018-10-15     False     False     False     False     False   \n",
      "1000168_2018-10-15     False     False     False     False     False   \n",
      "\n",
      "                    level_free  level_paid  \n",
      "sample_id                                   \n",
      "1000025_2018-10-15       False        True  \n",
      "1000035_2018-10-15        True       False  \n",
      "1000103_2018-10-15       False        True  \n",
      "1000164_2018-10-15        True       False  \n",
      "1000168_2018-10-15       False        True  \n",
      "\n",
      "[5 rows x 98 columns]\n"
     ]
    }
   ],
   "execution_count": 28,
   "source": [
    "# 4. One-hot encode categorical features on sliding_labels\n",
    "cat_cols = ['gender', 'state', 'level']\n",
    "sliding_labels[cat_cols] = sliding_labels[cat_cols].fillna(\"missing\")\n",
    "\n",
    "cat_ohe = pd.get_dummies(\n",
    "    sliding_labels[cat_cols],\n",
    "    columns=cat_cols,\n",
    "    prefix=cat_cols\n",
    ")\n",
    "\n",
    "print(\"Categorical one-hot feature example:\")\n",
    "print(cat_ohe.head())\n",
    "\n",
    "# Merge\n",
    "X_train = pd.concat([X_all, cat_ohe], axis=1)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_train example:\")\n",
    "print(X_train.head())\n",
    "\n",
    "# y_train\n",
    "y_train = sliding_labels['target']"
   ],
   "id": "c68fb157dc3cb15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:47:47.478033300Z",
     "start_time": "2025-12-19T00:47:47.362983300Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                    days_since_registration  n_events  recency_hours  \\\nsample_id                                                              \n1000025_2018-10-15                96.604073      1259      71.650002   \n1000035_2018-10-15                32.188633        88      77.664444   \n1000103_2018-10-15                22.689295        51     243.771942   \n1000164_2018-10-15                63.602768       166       6.444167   \n1000168_2018-10-15                67.329018       306      50.306667   \n...                                     ...       ...            ...   \n1005694_2018-10-15                52.234631       874      14.476389   \n1005715_2018-10-15                16.820833       308      47.740555   \n1005729_2018-10-15                46.348148       407      90.214447   \n1005742_2018-10-15                26.940775       453     192.843613   \n1005779_2018-10-15                18.719191       376      59.782776   \n\n                    events_last_7d  songs_last_7d  active_days  \\\nsample_id                                                        \n1000025_2018-10-15             356            292           11   \n1000035_2018-10-15              82             63            4   \n1000103_2018-10-15               0              0            1   \n1000164_2018-10-15             163            125            7   \n1000168_2018-10-15             195            154            3   \n...                            ...            ...          ...   \n1005694_2018-10-15             656            555            9   \n1005715_2018-10-15             228            179            9   \n1005729_2018-10-15             260            226            6   \n1005742_2018-10-15               0              0            3   \n1005779_2018-10-15             258            212            5   \n\n                    total_listen_time  events_last_1d  events_last_3d  \\\nsample_id                                                               \n1000025_2018-10-15      257291.031250               0               7   \n1000035_2018-10-15       16741.115234               0               0   \n1000103_2018-10-15        9769.046875               0               0   \n1000164_2018-10-15       31069.382812               5              21   \n1000168_2018-10-15       58290.179688               0             195   \n...                               ...             ...             ...   \n1005694_2018-10-15      182534.828125              95             149   \n1005715_2018-10-15       61015.347656               0              98   \n1005729_2018-10-15       81071.289062               0               0   \n1005742_2018-10-15       92688.765625               0               0   \n1005779_2018-10-15       77530.585938               0             100   \n\n                    songs_last_1d  ...  state_TX  state_UT  state_VA  \\\nsample_id                          ...                                 \n1000025_2018-10-15              0  ...     False     False     False   \n1000035_2018-10-15              0  ...     False     False     False   \n1000103_2018-10-15              0  ...     False     False     False   \n1000164_2018-10-15              3  ...     False     False     False   \n1000168_2018-10-15              0  ...     False     False     False   \n...                           ...  ...       ...       ...       ...   \n1005694_2018-10-15             86  ...     False     False     False   \n1005715_2018-10-15              0  ...     False     False     False   \n1005729_2018-10-15              0  ...     False     False     False   \n1005742_2018-10-15              0  ...     False     False     False   \n1005779_2018-10-15              0  ...      True     False     False   \n\n                    state_VT  state_WA  state_WI  state_WV  state_WY  \\\nsample_id                                                              \n1000025_2018-10-15     False     False     False     False     False   \n1000035_2018-10-15     False     False     False     False     False   \n1000103_2018-10-15     False     False     False     False     False   \n1000164_2018-10-15     False     False     False     False     False   \n1000168_2018-10-15     False     False     False     False     False   \n...                      ...       ...       ...       ...       ...   \n1005694_2018-10-15     False     False     False     False     False   \n1005715_2018-10-15     False     False     False     False     False   \n1005729_2018-10-15     False     False     False     False     False   \n1005742_2018-10-15     False     False     False     False     False   \n1005779_2018-10-15     False     False     False     False     False   \n\n                    level_free  level_paid  \nsample_id                                   \n1000025_2018-10-15       False        True  \n1000035_2018-10-15        True       False  \n1000103_2018-10-15       False        True  \n1000164_2018-10-15        True       False  \n1000168_2018-10-15       False        True  \n...                        ...         ...  \n1005694_2018-10-15       False        True  \n1005715_2018-10-15        True       False  \n1005729_2018-10-15       False        True  \n1005742_2018-10-15       False        True  \n1005779_2018-10-15       False        True  \n\n[100 rows x 98 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>days_since_registration</th>\n      <th>n_events</th>\n      <th>recency_hours</th>\n      <th>events_last_7d</th>\n      <th>songs_last_7d</th>\n      <th>active_days</th>\n      <th>total_listen_time</th>\n      <th>events_last_1d</th>\n      <th>events_last_3d</th>\n      <th>songs_last_1d</th>\n      <th>...</th>\n      <th>state_TX</th>\n      <th>state_UT</th>\n      <th>state_VA</th>\n      <th>state_VT</th>\n      <th>state_WA</th>\n      <th>state_WI</th>\n      <th>state_WV</th>\n      <th>state_WY</th>\n      <th>level_free</th>\n      <th>level_paid</th>\n    </tr>\n    <tr>\n      <th>sample_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1000025_2018-10-15</th>\n      <td>96.604073</td>\n      <td>1259</td>\n      <td>71.650002</td>\n      <td>356</td>\n      <td>292</td>\n      <td>11</td>\n      <td>257291.031250</td>\n      <td>0</td>\n      <td>7</td>\n      <td>0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1000035_2018-10-15</th>\n      <td>32.188633</td>\n      <td>88</td>\n      <td>77.664444</td>\n      <td>82</td>\n      <td>63</td>\n      <td>4</td>\n      <td>16741.115234</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1000103_2018-10-15</th>\n      <td>22.689295</td>\n      <td>51</td>\n      <td>243.771942</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>9769.046875</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1000164_2018-10-15</th>\n      <td>63.602768</td>\n      <td>166</td>\n      <td>6.444167</td>\n      <td>163</td>\n      <td>125</td>\n      <td>7</td>\n      <td>31069.382812</td>\n      <td>5</td>\n      <td>21</td>\n      <td>3</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1000168_2018-10-15</th>\n      <td>67.329018</td>\n      <td>306</td>\n      <td>50.306667</td>\n      <td>195</td>\n      <td>154</td>\n      <td>3</td>\n      <td>58290.179688</td>\n      <td>0</td>\n      <td>195</td>\n      <td>0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1005694_2018-10-15</th>\n      <td>52.234631</td>\n      <td>874</td>\n      <td>14.476389</td>\n      <td>656</td>\n      <td>555</td>\n      <td>9</td>\n      <td>182534.828125</td>\n      <td>95</td>\n      <td>149</td>\n      <td>86</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1005715_2018-10-15</th>\n      <td>16.820833</td>\n      <td>308</td>\n      <td>47.740555</td>\n      <td>228</td>\n      <td>179</td>\n      <td>9</td>\n      <td>61015.347656</td>\n      <td>0</td>\n      <td>98</td>\n      <td>0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1005729_2018-10-15</th>\n      <td>46.348148</td>\n      <td>407</td>\n      <td>90.214447</td>\n      <td>260</td>\n      <td>226</td>\n      <td>6</td>\n      <td>81071.289062</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1005742_2018-10-15</th>\n      <td>26.940775</td>\n      <td>453</td>\n      <td>192.843613</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>92688.765625</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1005779_2018-10-15</th>\n      <td>18.719191</td>\n      <td>376</td>\n      <td>59.782776</td>\n      <td>258</td>\n      <td>212</td>\n      <td>5</td>\n      <td>77530.585938</td>\n      <td>0</td>\n      <td>100</td>\n      <td>0</td>\n      <td>...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows  98 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29,
   "source": [
    "X_train.head(100)"
   ],
   "id": "5346ddf0945b07b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:47:47.491711300Z",
     "start_time": "2025-12-19T00:47:47.391660100Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "sample_id\n1000025_2018-10-15    1\n1000035_2018-10-15    0\n1000103_2018-10-15    0\n1000164_2018-10-15    0\n1000168_2018-10-15    0\n                     ..\n1005694_2018-10-15    0\n1005715_2018-10-15    0\n1005729_2018-10-15    0\n1005742_2018-10-15    0\n1005779_2018-10-15    0\nName: target, Length: 100, dtype: int32"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30,
   "source": [
    "y_train.head(100)"
   ],
   "id": "5bc58111f160eed6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Test set features"
   ],
   "id": "269fbfd3a1a015e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:47:54.542745200Z",
     "start_time": "2025-12-19T00:47:47.394650100Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test users: 2904\n",
      "Test global cutoff_date: 2018-11-20 00:00:00\n"
     ]
    }
   ],
   "execution_count": 31,
   "source": [
    "# 4.1 Sort by userId + ts, then group\n",
    "test_df_sorted = test_df.sort_values(['userId', 'ts']).copy()\n",
    "test_groups = dict(tuple(test_df_sorted.groupby('userId')))\n",
    "test_users = sorted(test_groups.keys())\n",
    "\n",
    "print(\"Number of test users:\", len(test_users))\n",
    "\n",
    "# 4.2 Global cutoff for test (observation end): use last day in test\n",
    "global_cutoff_test = test_df_sorted['ts'].max().normalize()\n",
    "print(\"Test global cutoff_date:\", global_cutoff_test)\n",
    "\n",
    "# 4.3 Initialize feature table: one sample per user\n",
    "X_test = pd.DataFrame(index=test_users)\n"
   ],
   "id": "30980f3e640d44fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.1 Lifetime"
   ],
   "id": "873f3bf24aab87f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:47:54.744793700Z",
     "start_time": "2025-12-19T00:47:54.563917900Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days_since_registration_test example:\n",
      "1000655    66.504227\n",
      "1000963    73.911598\n",
      "1001129    86.574089\n",
      "1001963    40.715820\n",
      "1002283    54.104652\n",
      "Name: days_since_registration, dtype: float32\n"
     ]
    }
   ],
   "execution_count": 32,
   "source": [
    "\n",
    "# Registration time per user (from test)\n",
    "uid_registration_test = (\n",
    "    test_df_sorted\n",
    "    .groupby('userId')['registration']\n",
    "    .first()\n",
    ")\n",
    "\n",
    "# Days from registration to cutoff\n",
    "days_since_registration_test = (\n",
    "    (global_cutoff_test - uid_registration_test) / np.timedelta64(1, 'D')\n",
    ").astype('float32')\n",
    "days_since_registration_test = days_since_registration_test.clip(lower=0)\n",
    "\n",
    "X_test['days_since_registration'] = days_since_registration_test.reindex(test_users)\n",
    "\n",
    "print(\"days_since_registration_test example:\")\n",
    "print(X_test['days_since_registration'].head())\n"
   ],
   "id": "6110d257e7baa4eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2 n_events / recency_hours / active_days"
   ],
   "id": "e6a9bc3457d17622"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:47:56.995255600Z",
     "start_time": "2025-12-19T00:47:54.747836600Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_events / recency_hours / active_days examples:\n",
      "         n_events  recency_hours  active_days\n",
      "1000655       346     101.752500           11\n",
      "1000963      2539      47.861389           26\n",
      "1001129       668      25.063889           10\n",
      "1001963       718       3.341389           16\n",
      "1002283      3837       5.711667           25\n"
     ]
    }
   ],
   "execution_count": 33,
   "source": [
    "\n",
    "n_events_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "recency_hours_test = pd.Series(0.0, index=test_users, dtype='float32')\n",
    "active_days_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    # Only consider events <= cutoff\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    \n",
    "    if df_before.empty:\n",
    "        n_events_test.loc[uid] = 0\n",
    "        recency_hours_test.loc[uid] = 9999.0\n",
    "        active_days_test.loc[uid] = 0\n",
    "        continue\n",
    "    \n",
    "    # Total events\n",
    "    n_events_test.loc[uid] = len(df_before)\n",
    "    \n",
    "    # recency_hours: cutoff - last event\n",
    "    last_ts = df_before['ts'].iloc[-1]\n",
    "    delta_h = (global_cutoff_test - last_ts) / np.timedelta64(1, 'h')\n",
    "    recency_hours_test.loc[uid] = float(delta_h)\n",
    "    \n",
    "    # Active days: number of distinct dates\n",
    "    active_days_test.loc[uid] = df_before['ts'].dt.normalize().nunique()\n",
    "\n",
    "X_test['n_events'] = n_events_test\n",
    "X_test['recency_hours'] = recency_hours_test\n",
    "X_test['active_days'] = active_days_test\n",
    "\n",
    "print(\"n_events / recency_hours / active_days examples:\")\n",
    "print(X_test[['n_events', 'recency_hours', 'active_days']].head())\n"
   ],
   "id": "950f18ce39c56de9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.3 recent 7 / 3 / 1 behavior"
   ],
   "id": "c9a6e300c9ae3756"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:47:58.432088500Z",
     "start_time": "2025-12-19T00:47:56.994243200Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events_last_*_test examples:\n",
      "         events_last_7d  events_last_3d  events_last_1d\n",
      "1000655              33               0               0\n",
      "1000963             472             259               0\n",
      "1001129              62               2               0\n",
      "1001963             326             159             159\n",
      "1002283             303              89              89\n"
     ]
    }
   ],
   "execution_count": 34,
   "source": [
    "\n",
    "events_last_7d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "events_last_3d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "events_last_1d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "\n",
    "seven_days  = np.timedelta64(7, 'D')\n",
    "three_days  = np.timedelta64(3, 'D')\n",
    "one_day     = np.timedelta64(1, 'D')\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    \n",
    "    ts_vals = df_before['ts'].values\n",
    "    \n",
    "    # Last 7 days\n",
    "    mask_7 = ts_vals > (global_cutoff_test - seven_days)\n",
    "    events_last_7d_test.loc[uid] = int(mask_7.sum())\n",
    "    \n",
    "    # Last 3 days\n",
    "    mask_3 = ts_vals > (global_cutoff_test - three_days)\n",
    "    events_last_3d_test.loc[uid] = int(mask_3.sum())\n",
    "    \n",
    "    # Last 1 day\n",
    "    mask_1 = ts_vals > (global_cutoff_test - one_day)\n",
    "    events_last_1d_test.loc[uid] = int(mask_1.sum())\n",
    "\n",
    "X_test['events_last_7d'] = events_last_7d_test\n",
    "X_test['events_last_3d'] = events_last_3d_test\n",
    "X_test['events_last_1d'] = events_last_1d_test\n",
    "\n",
    "print(\"events_last_*_test examples:\")\n",
    "print(X_test[['events_last_7d', 'events_last_3d', 'events_last_1d']].head())\n"
   ],
   "id": "9f9d0765a204c9cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.4 recent 7 / 3 / 1 songs"
   ],
   "id": "3149a796cd547cf5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:48:01.309687200Z",
     "start_time": "2025-12-19T00:47:58.434150700Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "songs_last_*_test & total_listen_time_test examples:\n",
      "         songs_last_7d  songs_last_3d  songs_last_1d  total_listen_time\n",
      "1000655             24              0              0        65479.87470\n",
      "1000963            402            221              0       526127.55185\n",
      "1001129             46              1              0       139026.22059\n",
      "1001963            250            123            123       134683.68053\n",
      "1002283            250             77             77       789821.80947\n"
     ]
    }
   ],
   "execution_count": 35,
   "source": [
    "\n",
    "songs_last_7d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "songs_last_3d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "songs_last_1d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "total_listen_time_test = pd.Series(0.0, index=test_users, dtype='float32')\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    \n",
    "    df_song = df_before[df_before['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        continue\n",
    "    \n",
    "    ts_song = df_song['ts'].values\n",
    "    len_song = df_song['length'].values\n",
    "    \n",
    "    # Total listening time (all songs <= cutoff)\n",
    "    total_listen_time_test.loc[uid] = float(len_song.sum())\n",
    "    \n",
    "    # Last 7 days song count\n",
    "    mask_7 = ts_song > (global_cutoff_test - seven_days)\n",
    "    songs_last_7d_test.loc[uid] = int(mask_7.sum())\n",
    "    \n",
    "    # Last 3 days\n",
    "    mask_3 = ts_song > (global_cutoff_test - three_days)\n",
    "    songs_last_3d_test.loc[uid] = int(mask_3.sum())\n",
    "    \n",
    "    # Last 1 day\n",
    "    mask_1 = ts_song > (global_cutoff_test - one_day)\n",
    "    songs_last_1d_test.loc[uid] = int(mask_1.sum())\n",
    "\n",
    "X_test['songs_last_7d']     = songs_last_7d_test\n",
    "X_test['songs_last_3d']     = songs_last_3d_test\n",
    "X_test['songs_last_1d']     = songs_last_1d_test\n",
    "X_test['total_listen_time'] = total_listen_time_test\n",
    "\n",
    "print(\"songs_last_*_test & total_listen_time_test examples:\")\n",
    "print(X_test[['songs_last_7d', 'songs_last_3d', 'songs_last_1d', 'total_listen_time']].head())\n"
   ],
   "id": "5aabb58fc0f9bf19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.5 Level-at-cutoff"
   ],
   "id": "b2d50a7ad69d6f78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:48:02.264497600Z",
     "start_time": "2025-12-19T00:48:01.310685800Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level_at_cutoff_test example:\n",
      "1000655    free\n",
      "1000963    paid\n",
      "1001129    free\n",
      "1001963    free\n",
      "1002283    paid\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 36,
   "source": [
    "level_at_cutoff_test = pd.Series(\"unknown\", index=test_users, dtype=object)\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    level_at_cutoff_test.loc[uid] = str(df_before['level'].iloc[-1])\n",
    "\n",
    "print(\"level_at_cutoff_test example:\")\n",
    "print(level_at_cutoff_test.head())\n"
   ],
   "id": "9044d9483e3bc9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.6 Page Count + Page Ratio"
   ],
   "id": "fa97d48d07e95cbc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:48:08.479006500Z",
     "start_time": "2025-12-19T00:48:02.266512Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 37,
   "source": [
    "\n",
    "# Universe of pages (union of train and test)\n",
    "all_pages = pd.concat([train_df['page'], test_df['page']]).unique()\n",
    "\n",
    "# Initialize features (two per page: count_XXX, ratio_XXX)\n",
    "for p in all_pages:\n",
    "    cname = f\"cnt_page_{p.replace(' ', '_')}\"\n",
    "    X_test[cname] = 0\n",
    "\n",
    "# Ratio features\n",
    "for p in all_pages:\n",
    "    cname = f\"ratio_page_{p.replace(' ', '_')}\"\n",
    "    X_test[cname] = 0.0\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "\n",
    "    # Page counts\n",
    "    page_counts = df_before['page'].value_counts()\n",
    "\n",
    "    for p, cnt in page_counts.items():\n",
    "        cname = f\"cnt_page_{p.replace(' ', '_')}\"\n",
    "        if cname in X_test.columns:\n",
    "            X_test.loc[uid, cname] = cnt\n",
    "\n",
    "    total_events = len(df_before)\n",
    "    if total_events > 0:\n",
    "        for p, cnt in page_counts.items():\n",
    "            rname = f\"ratio_page_{p.replace(' ', '_')}\"\n",
    "            if rname in X_test.columns:\n",
    "                X_test.loc[uid, rname] = cnt / total_events"
   ],
   "id": "3af8e51dd28231b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.7 Status Count + Status Ratio"
   ],
   "id": "4ec4071c29f43e78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:48:11.404210600Z",
     "start_time": "2025-12-19T00:48:08.483079400Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 38,
   "source": [
    "\n",
    "# Universe of status codes\n",
    "all_status = pd.concat([train_df['status'], test_df['status']]).dropna().unique()\n",
    "\n",
    "# Initialize\n",
    "for s in all_status:\n",
    "    X_test[f\"cnt_status_{s}\"] = 0\n",
    "    X_test[f\"ratio_status_{s}\"] = 0.0\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "\n",
    "    status_counts = df_before['status'].value_counts()\n",
    "    total_events = len(df_before)\n",
    "\n",
    "    for s, cnt in status_counts.items():\n",
    "        cname = f\"cnt_status_{s}\"\n",
    "        rname = f\"ratio_status_{s}\"\n",
    "        X_test.loc[uid, cname] = cnt\n",
    "        X_test.loc[uid, rname] = cnt / total_events"
   ],
   "id": "e073c335eb22a7f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.8 Level Features"
   ],
   "id": "e3570124974f1ae8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:48:13.082394500Z",
     "start_time": "2025-12-19T00:48:11.405209100Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 39,
   "source": [
    "X_test['is_paid_last'] = 0\n",
    "X_test['ever_paid'] = 0\n",
    "X_test['n_level_change'] = 0\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "\n",
    "    levels = df_before['level'].astype(str).values\n",
    "\n",
    "    # Last level\n",
    "    X_test.loc[uid, 'is_paid_last'] = 1 if levels[-1] == 'paid' else 0\n",
    "\n",
    "    # Ever paid in history\n",
    "    X_test.loc[uid, 'ever_paid'] = 1 if 'paid' in levels else 0\n",
    "\n",
    "    # Number of level switches\n",
    "    if len(levels) > 1:\n",
    "        X_test.loc[uid, 'n_level_change'] = np.sum(levels[1:] != levels[:-1])"
   ],
   "id": "b8491a5b67380dd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.9 Session Duration Features"
   ],
   "id": "99db7856c85b8e94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:48:40.131038100Z",
     "start_time": "2025-12-19T00:48:13.085420400Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 40,
   "source": [
    "X_test['session_mean_duration'] = 0.0\n",
    "X_test['session_max_duration']  = 0.0\n",
    "X_test['session_min_duration']  = 0.0\n",
    "X_test['session_std_duration']  = 0.0\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "\n",
    "    session_grp = df_before.groupby('sessionId')\n",
    "\n",
    "    durations = []\n",
    "    for sid, g in session_grp:\n",
    "        start = g['ts'].min()\n",
    "        end   = g['ts'].max()\n",
    "        durations.append((end - start) / np.timedelta64(1, 's'))   # seconds\n",
    "\n",
    "    if len(durations) > 0:\n",
    "        durations = np.array(durations)\n",
    "        X_test.loc[uid, 'session_mean_duration'] = durations.mean()\n",
    "        X_test.loc[uid, 'session_max_duration']  = durations.max()\n",
    "        X_test.loc[uid, 'session_min_duration']  = durations.min()\n",
    "        X_test.loc[uid, 'session_std_duration']  = durations.std()"
   ],
   "id": "250a58890393bdff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.10 Session Event Count Features"
   ],
   "id": "9904f68e0a3e5f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:48:42.419239900Z",
     "start_time": "2025-12-19T00:48:40.136095700Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 41,
   "source": [
    "\n",
    "X_test['session_mean_events'] = 0.0\n",
    "X_test['session_max_events']  = 0.0\n",
    "X_test['session_min_events']  = 0.0\n",
    "X_test['session_std_events']  = 0.0\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "\n",
    "    session_grp = df_before.groupby('sessionId')\n",
    "    event_counts = session_grp.size().values\n",
    "\n",
    "    if len(event_counts) > 0:\n",
    "        event_counts = np.array(event_counts)\n",
    "        X_test.loc[uid, 'session_mean_events'] = event_counts.mean()\n",
    "        X_test.loc[uid, 'session_max_events']  = event_counts.max()\n",
    "        X_test.loc[uid, 'session_min_events']  = event_counts.min()\n",
    "        X_test.loc[uid, 'session_std_events']  = event_counts.std()"
   ],
   "id": "eb7261ac10b1e552"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.11 Category + One-Hot"
   ],
   "id": "44bc2fd369325a3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:48:42.745135500Z",
     "start_time": "2025-12-19T00:48:42.517941600Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test categorical one-hot example:\n",
      "         gender_F  gender_M  gender_missing  state_AK  state_AL  state_AR  \\\n",
      "1000655      True     False           False     False     False     False   \n",
      "1000963     False      True           False     False     False     False   \n",
      "1001129     False      True           False     False     False     False   \n",
      "1001963      True     False           False     False     False     False   \n",
      "1002283     False      True           False     False     False     False   \n",
      "\n",
      "         state_AZ  state_CA  state_CO  state_CT  ...  state_UT  state_VA  \\\n",
      "1000655     False     False     False     False  ...     False      True   \n",
      "1000963     False     False     False     False  ...     False     False   \n",
      "1001129     False     False     False     False  ...     False     False   \n",
      "1001963     False     False     False     False  ...     False     False   \n",
      "1002283     False     False     False     False  ...     False     False   \n",
      "\n",
      "         state_VT  state_WA  state_WI  state_WV  state_WY  state_ne  \\\n",
      "1000655     False     False     False     False     False     False   \n",
      "1000963     False     False     False     False     False     False   \n",
      "1001129     False     False     False      True     False     False   \n",
      "1001963     False     False     False     False     False     False   \n",
      "1002283     False     False     False     False     False     False   \n",
      "\n",
      "         level_free  level_paid  \n",
      "1000655        True       False  \n",
      "1000963       False        True  \n",
      "1001129        True       False  \n",
      "1001963        True       False  \n",
      "1002283       False        True  \n",
      "\n",
      "[5 rows x 55 columns]\n",
      "cat_test_ohe shape after aligning to train: (2904, 53)\n"
     ]
    }
   ],
   "execution_count": 42,
   "source": [
    "# Static gender/state for test\n",
    "test_user_static = (\n",
    "    test_df_sorted\n",
    "    .groupby('userId')\n",
    "    .agg({\n",
    "        'gender': 'first',\n",
    "        'state':  'first',\n",
    "    })\n",
    ")\n",
    "\n",
    "cat_test = pd.DataFrame(index=test_users)\n",
    "cat_test['gender'] = test_user_static['gender']\n",
    "cat_test['state']  = test_user_static['state']\n",
    "cat_test['level']  = level_at_cutoff_test\n",
    "\n",
    "cat_test = cat_test.fillna(\"missing\")\n",
    "\n",
    "cat_test_ohe = pd.get_dummies(\n",
    "    cat_test,\n",
    "    columns=['gender', 'state', 'level'],\n",
    "    prefix=['gender', 'state', 'level']\n",
    ")\n",
    "\n",
    "print(\"Test categorical one-hot example:\")\n",
    "print(cat_test_ohe.head())\n",
    "\n",
    "# Align to train categorical columns\n",
    "cat_test_ohe = cat_test_ohe.reindex(columns=cat_ohe.columns, fill_value=0)\n",
    "\n",
    "print(\"cat_test_ohe shape after aligning to train:\", cat_test_ohe.shape)"
   ],
   "id": "c3cf63d034c6bcfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.12 Combining features"
   ],
   "id": "31c79fc4c7ab7c85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:48:42.756421Z",
     "start_time": "2025-12-19T00:48:42.745135500Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_full shape: (2904, 98)\n",
      "         days_since_registration  n_events  recency_hours  events_last_7d  \\\n",
      "1000655                66.504227       346     101.752500              33   \n",
      "1000963                73.911598      2539      47.861389             472   \n",
      "1001129                86.574089       668      25.063889              62   \n",
      "1001963                40.715820       718       3.341389             326   \n",
      "1002283                54.104652      3837       5.711667             303   \n",
      "\n",
      "         songs_last_7d  active_days  total_listen_time  events_last_1d  \\\n",
      "1000655             24           11        65479.87470               0   \n",
      "1000963            402           26       526127.55185               0   \n",
      "1001129             46           10       139026.22059               0   \n",
      "1001963            250           16       134683.68053             159   \n",
      "1002283            250           25       789821.80947              89   \n",
      "\n",
      "         events_last_3d  songs_last_1d  ...  state_TX  state_UT  state_VA  \\\n",
      "1000655               0              0  ...     False     False      True   \n",
      "1000963             259              0  ...     False     False     False   \n",
      "1001129               2              0  ...     False     False     False   \n",
      "1001963             159            123  ...     False     False     False   \n",
      "1002283              89             77  ...     False     False     False   \n",
      "\n",
      "         state_VT  state_WA  state_WI  state_WV  state_WY  level_free  \\\n",
      "1000655     False     False     False     False     False        True   \n",
      "1000963     False     False     False     False     False       False   \n",
      "1001129     False     False     False      True     False        True   \n",
      "1001963     False     False     False     False     False        True   \n",
      "1002283     False     False     False     False     False       False   \n",
      "\n",
      "         level_paid  \n",
      "1000655       False  \n",
      "1000963        True  \n",
      "1001129       False  \n",
      "1001963       False  \n",
      "1002283        True  \n",
      "\n",
      "[5 rows x 98 columns]\n"
     ]
    }
   ],
   "execution_count": 43,
   "source": [
    "# Final test features (align column order)\n",
    "X_test_full = pd.concat([X_test, cat_test_ohe], axis=1)\n",
    "X_test_full = X_test_full.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "print(\"X_test_full shape:\", X_test_full.shape)\n",
    "print(X_test_full.head())"
   ],
   "id": "871c66b0b0ea5c05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:48:42.771731700Z",
     "start_time": "2025-12-19T00:48:42.753369300Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "         days_since_registration  n_events  recency_hours  events_last_7d  \\\n1000655                66.504227       346     101.752500              33   \n1000963                73.911598      2539      47.861389             472   \n1001129                86.574089       668      25.063889              62   \n1001963                40.715820       718       3.341389             326   \n1002283                54.104652      3837       5.711667             303   \n...                          ...       ...            ...             ...   \n1037874                56.995277      1614      80.578333             224   \n1038109                59.933205      2437     174.817222               0   \n1038258                67.642838      1043     104.052778              60   \n1038741                79.969872        39     656.347500               0   \n1038830                83.776566       583     218.366944               0   \n\n         songs_last_7d  active_days  total_listen_time  events_last_1d  \\\n1000655             24           11        65479.87470               0   \n1000963            402           26       526127.55185               0   \n1001129             46           10       139026.22059               0   \n1001963            250           16       134683.68053             159   \n1002283            250           25       789821.80947              89   \n...                ...          ...                ...             ...   \n1037874            155           22       317178.30474               0   \n1038109              0           15       516133.09474               0   \n1038258             50           13       210747.90598               0   \n1038741              0            1         5773.78177               0   \n1038830              0           12       108830.09854               0   \n\n         events_last_3d  songs_last_1d  ...  state_TX  state_UT  state_VA  \\\n1000655               0              0  ...     False     False      True   \n1000963             259              0  ...     False     False     False   \n1001129               2              0  ...     False     False     False   \n1001963             159            123  ...     False     False     False   \n1002283              89             77  ...     False     False     False   \n...                 ...            ...  ...       ...       ...       ...   \n1037874               0              0  ...     False     False     False   \n1038109               0              0  ...     False     False     False   \n1038258               0              0  ...      True     False     False   \n1038741               0              0  ...     False     False     False   \n1038830               0              0  ...     False     False     False   \n\n         state_VT  state_WA  state_WI  state_WV  state_WY  level_free  \\\n1000655     False     False     False     False     False        True   \n1000963     False     False     False     False     False       False   \n1001129     False     False     False      True     False        True   \n1001963     False     False     False     False     False        True   \n1002283     False     False     False     False     False       False   \n...           ...       ...       ...       ...       ...         ...   \n1037874     False     False     False     False     False        True   \n1038109     False     False     False     False     False       False   \n1038258     False     False     False     False     False       False   \n1038741     False     False     False     False     False        True   \n1038830     False     False      True     False     False       False   \n\n         level_paid  \n1000655       False  \n1000963        True  \n1001129       False  \n1001963       False  \n1002283        True  \n...             ...  \n1037874       False  \n1038109        True  \n1038258        True  \n1038741       False  \n1038830        True  \n\n[100 rows x 98 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>days_since_registration</th>\n      <th>n_events</th>\n      <th>recency_hours</th>\n      <th>events_last_7d</th>\n      <th>songs_last_7d</th>\n      <th>active_days</th>\n      <th>total_listen_time</th>\n      <th>events_last_1d</th>\n      <th>events_last_3d</th>\n      <th>songs_last_1d</th>\n      <th>...</th>\n      <th>state_TX</th>\n      <th>state_UT</th>\n      <th>state_VA</th>\n      <th>state_VT</th>\n      <th>state_WA</th>\n      <th>state_WI</th>\n      <th>state_WV</th>\n      <th>state_WY</th>\n      <th>level_free</th>\n      <th>level_paid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1000655</th>\n      <td>66.504227</td>\n      <td>346</td>\n      <td>101.752500</td>\n      <td>33</td>\n      <td>24</td>\n      <td>11</td>\n      <td>65479.87470</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1000963</th>\n      <td>73.911598</td>\n      <td>2539</td>\n      <td>47.861389</td>\n      <td>472</td>\n      <td>402</td>\n      <td>26</td>\n      <td>526127.55185</td>\n      <td>0</td>\n      <td>259</td>\n      <td>0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1001129</th>\n      <td>86.574089</td>\n      <td>668</td>\n      <td>25.063889</td>\n      <td>62</td>\n      <td>46</td>\n      <td>10</td>\n      <td>139026.22059</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1001963</th>\n      <td>40.715820</td>\n      <td>718</td>\n      <td>3.341389</td>\n      <td>326</td>\n      <td>250</td>\n      <td>16</td>\n      <td>134683.68053</td>\n      <td>159</td>\n      <td>159</td>\n      <td>123</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1002283</th>\n      <td>54.104652</td>\n      <td>3837</td>\n      <td>5.711667</td>\n      <td>303</td>\n      <td>250</td>\n      <td>25</td>\n      <td>789821.80947</td>\n      <td>89</td>\n      <td>89</td>\n      <td>77</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1037874</th>\n      <td>56.995277</td>\n      <td>1614</td>\n      <td>80.578333</td>\n      <td>224</td>\n      <td>155</td>\n      <td>22</td>\n      <td>317178.30474</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1038109</th>\n      <td>59.933205</td>\n      <td>2437</td>\n      <td>174.817222</td>\n      <td>0</td>\n      <td>0</td>\n      <td>15</td>\n      <td>516133.09474</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1038258</th>\n      <td>67.642838</td>\n      <td>1043</td>\n      <td>104.052778</td>\n      <td>60</td>\n      <td>50</td>\n      <td>13</td>\n      <td>210747.90598</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1038741</th>\n      <td>79.969872</td>\n      <td>39</td>\n      <td>656.347500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5773.78177</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1038830</th>\n      <td>83.776566</td>\n      <td>583</td>\n      <td>218.366944</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>108830.09854</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows  98 columns</p>\n</div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44,
   "source": [
    "X_test_full.head(100)"
   ],
   "id": "46715f2f63a9c0fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Models"
   ],
   "id": "1887bbfca0c0d6b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:48:42.774769400Z",
     "start_time": "2025-12-19T00:48:42.769224Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 45,
   "source": [
    "# Simple oversampling helper\n",
    "def oversample(X, y):\n",
    "    X = X.copy()\n",
    "    X['target'] = y\n",
    "    major = X[X['target'] == 0]\n",
    "    minor = X[X['target'] == 1]\n",
    "\n",
    "    if len(minor) == 0:\n",
    "        raise ValueError(\"No positive samples\")\n",
    "\n",
    "    ratio = max(1, len(major) // len(minor))\n",
    "    minor_ov = pd.concat([minor] * ratio, ignore_index=True)\n",
    "\n",
    "    df_new = pd.concat([major, minor_ov], axis=0).sample(frac=1.0, random_state=42)\n",
    "    y_new = df_new['target'].values\n",
    "    X_new = df_new.drop(columns=['target'])\n",
    "\n",
    "    print(\"Positive rate after oversampling:\", y_new.mean())\n",
    "    return X_new, y_new"
   ],
   "id": "761fc6b7075915c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Grid Search "
   ],
   "id": "108efdf6e1777a9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T01:00:18.520096Z",
     "start_time": "2025-12-19T00:48:42.779891800Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rate after oversampling: 0.4878395343298524\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "LR params={'C': 0.1, 'class_weight': None}, mean AUC=0.741856\n",
      "Positive rate after oversampling: 0.4878395343298524\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "LR params={'C': 0.3, 'class_weight': None}, mean AUC=0.741847\n",
      "Positive rate after oversampling: 0.4878395343298524\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "LR params={'C': 1.0, 'class_weight': None}, mean AUC=0.741832\n",
      "Positive rate after oversampling: 0.4878395343298524\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "LR params={'C': 0.1, 'class_weight': 'balanced'}, mean AUC=0.741867\n",
      "Positive rate after oversampling: 0.4878395343298524\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "LR params={'C': 0.3, 'class_weight': 'balanced'}, mean AUC=0.741863\n",
      "Positive rate after oversampling: 0.4878395343298524\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "LR params={'C': 1.0, 'class_weight': 'balanced'}, mean AUC=0.741848\n",
      "\n",
      ">>> Best LR params: {'C': 0.1, 'class_weight': 'balanced'}\n",
      ">>> Best LR CV AUC: 0.7418672850881967\n"
     ]
    }
   ],
   "execution_count": 46,
   "source": [
    "### A. Grid Search for Logistic Regression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def cv_auc_lr(params, X, y, n_splits=3, random_state=42):\n",
    "    \"\"\"\n",
    "    For a given set of Logistic Regression parameters,\n",
    "    compute the mean AUC using K-fold cross-validation with oversampling.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    aucs = []\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), start=1):\n",
    "        X_tr = X.iloc[tr_idx].copy()\n",
    "        y_tr = y.iloc[tr_idx].copy()\n",
    "        X_val = X.iloc[val_idx].copy()\n",
    "        y_val = y.iloc[val_idx].copy()\n",
    "\n",
    "        # Apply oversampling only on the training fold to avoid data leakage\n",
    "        X_tr_os, y_tr_os = oversample(X_tr, y_tr.values)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_tr_os_scaled = scaler.fit_transform(X_tr_os)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        clf = LogisticRegression(\n",
    "            max_iter=2000,\n",
    "            solver=\"liblinear\",\n",
    "            **params\n",
    "        )\n",
    "        clf.fit(X_tr_os_scaled, y_tr_os)\n",
    "\n",
    "        val_proba = clf.predict_proba(X_val_scaled)[:, 1]\n",
    "        auc = roc_auc_score(y_val, val_proba)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return np.mean(aucs)\n",
    "\n",
    "\n",
    "# A relatively small parameter grid to keep computation time reasonable\n",
    "lr_param_grid = [\n",
    "    {\"C\": 0.1, \"class_weight\": None},\n",
    "    {\"C\": 0.3, \"class_weight\": None},\n",
    "    {\"C\": 1.0, \"class_weight\": None},\n",
    "    {\"C\": 0.1, \"class_weight\": \"balanced\"},\n",
    "    {\"C\": 0.3, \"class_weight\": \"balanced\"},\n",
    "    {\"C\": 1.0, \"class_weight\": \"balanced\"},\n",
    "]\n",
    "\n",
    "best_lr_auc = -1\n",
    "best_lr_params = None\n",
    "\n",
    "for params in lr_param_grid:\n",
    "    mean_auc = cv_auc_lr(params, X_train, y_train)\n",
    "    print(f\"LR params={params}, mean AUC={mean_auc:.6f}\")\n",
    "    if mean_auc > best_lr_auc:\n",
    "        best_lr_auc = mean_auc\n",
    "        best_lr_params = params\n",
    "\n",
    "print(\"\\n>>> Best LR params:\", best_lr_params)\n",
    "print(\">>> Best LR CV AUC:\", best_lr_auc)\n"
   ],
   "id": "9bcc34d09145e4ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T01:07:23.587973900Z",
     "start_time": "2025-12-19T01:00:18.520605600Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rate after oversampling: 0.4878395343298524\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "RF params={'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}, mean AUC=0.797769\n",
      "Positive rate after oversampling: 0.4878395343298524\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "RF params={'max_depth': 12, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}, mean AUC=0.893241\n",
      "Positive rate after oversampling: 0.4878395343298524\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "RF params={'max_depth': 12, 'min_samples_split': 20, 'min_samples_leaf': 10, 'max_features': 'sqrt'}, mean AUC=0.892673\n",
      "\n",
      ">>> Best RF params: {'max_depth': 12, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      ">>> Best RF CV AUC: 0.8932408920575773\n"
     ]
    }
   ],
   "execution_count": 47,
   "source": [
    "### B. Grid Search for Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def cv_auc_rf(params, X, y, n_splits=3, random_state=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    aucs = []\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), start=1):\n",
    "        X_tr = X.iloc[tr_idx].copy()\n",
    "        y_tr = y.iloc[tr_idx].copy()\n",
    "        X_val = X.iloc[val_idx].copy()\n",
    "        y_val = y.iloc[val_idx].copy()\n",
    "\n",
    "        # Oversampling on the training fold for RF (optional)\n",
    "        X_tr_os, y_tr_os = oversample(X_tr, y_tr.values)\n",
    "\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=300,  # Fixed to a moderate value to control complexity\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            **params\n",
    "        )\n",
    "        clf.fit(X_tr_os, y_tr_os)\n",
    "\n",
    "        val_proba = clf.predict_proba(X_val)[:, 1]\n",
    "        auc = roc_auc_score(y_val, val_proba)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return np.mean(aucs)\n",
    "\n",
    "\n",
    "rf_param_grid = [\n",
    "    {\"max_depth\": 8, \"min_samples_split\": 10, \"min_samples_leaf\": 5, \"max_features\": \"sqrt\"},\n",
    "    {\"max_depth\": 12, \"min_samples_split\": 10, \"min_samples_leaf\": 5, \"max_features\": \"sqrt\"},\n",
    "    {\"max_depth\": 12, \"min_samples_split\": 20, \"min_samples_leaf\": 10, \"max_features\": \"sqrt\"},\n",
    "]\n",
    "\n",
    "best_rf_auc = -1\n",
    "best_rf_params = None\n",
    "\n",
    "for params in rf_param_grid:\n",
    "    mean_auc = cv_auc_rf(params, X_train, y_train)\n",
    "    print(f\"RF params={params}, mean AUC={mean_auc:.6f}\")\n",
    "    if mean_auc > best_rf_auc:\n",
    "        best_rf_auc = mean_auc\n",
    "        best_rf_params = params\n",
    "\n",
    "print(\"\\n>>> Best RF params:\", best_rf_params)\n",
    "print(\">>> Best RF CV AUC:\", best_rf_auc)\n"
   ],
   "id": "d856685d0a05d3d9"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rate after oversampling: 0.4878395343298524\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=5. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=5. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 241704, number of negative: 253754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045477 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9163\n",
      "[LightGBM] [Info] Number of data points in the train set: 495458, number of used features: 98\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=5. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487840 -> initscore=-0.048651\n",
      "[LightGBM] [Info] Start training from score -0.048651\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=5. Current value: min_data_in_leaf=5\n",
      "[LGB] Fold 1 AUC = 0.984771, best_iter = 4429\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=5. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=5. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 241722, number of negative: 253754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9158\n",
      "[LightGBM] [Info] Number of data points in the train set: 495476, number of used features: 98\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=5. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487858 -> initscore=-0.048577\n",
      "[LightGBM] [Info] Start training from score -0.048577\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=5. Current value: min_data_in_leaf=5\n",
      "[LGB] Fold 2 AUC = 0.977733, best_iter = 2899\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=5. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=5. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Number of positive: 241722, number of negative: 253754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9165\n",
      "[LightGBM] [Info] Number of data points in the train set: 495476, number of used features: 98\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=5. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487858 -> initscore=-0.048577\n",
      "[LightGBM] [Info] Start training from score -0.048577\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=5. Current value: min_data_in_leaf=5\n",
      "[LGB] Fold 3 AUC = 0.984133, best_iter = 4113\n",
      "\n",
      "LGB params={'min_samples_split': 5, 'min_samples_leaf': 5, 'max_depth': 8}, mean AUC=0.982212\n",
      "Positive rate after oversampling: 0.4878395343298524\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=10. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=10. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 241704, number of negative: 253754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038357 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9163\n",
      "[LightGBM] [Info] Number of data points in the train set: 495458, number of used features: 98\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=10. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487840 -> initscore=-0.048651\n",
      "[LightGBM] [Info] Start training from score -0.048651\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=10. Current value: min_data_in_leaf=10\n",
      "[LGB] Fold 1 AUC = 0.985310, best_iter = 4987\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=10. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=10. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 241722, number of negative: 253754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051633 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9158\n",
      "[LightGBM] [Info] Number of data points in the train set: 495476, number of used features: 98\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=10. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487858 -> initscore=-0.048577\n",
      "[LightGBM] [Info] Start training from score -0.048577\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=10. Current value: min_data_in_leaf=10\n",
      "[LGB] Fold 2 AUC = 0.979005, best_iter = 3222\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=10. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=10. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 241722, number of negative: 253754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9165\n",
      "[LightGBM] [Info] Number of data points in the train set: 495476, number of used features: 98\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=10. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487858 -> initscore=-0.048577\n",
      "[LightGBM] [Info] Start training from score -0.048577\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=10. Current value: min_data_in_leaf=10\n",
      "[LGB] Fold 3 AUC = 0.984087, best_iter = 3819\n",
      "\n",
      "LGB params={'min_samples_split': 10, 'min_samples_leaf': 10, 'max_depth': 10}, mean AUC=0.982801\n",
      "Positive rate after oversampling: 0.4878395343298524\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=15. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=15. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Info] Number of positive: 241704, number of negative: 253754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9163\n",
      "[LightGBM] [Info] Number of data points in the train set: 495458, number of used features: 98\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=15. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487840 -> initscore=-0.048651\n",
      "[LightGBM] [Info] Start training from score -0.048651\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=15. Current value: min_data_in_leaf=15\n",
      "[LGB] Fold 1 AUC = 0.984112, best_iter = 3215\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=15. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=15. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Info] Number of positive: 241722, number of negative: 253754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050905 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9158\n",
      "[LightGBM] [Info] Number of data points in the train set: 495476, number of used features: 98\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=15. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487858 -> initscore=-0.048577\n",
      "[LightGBM] [Info] Start training from score -0.048577\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=15. Current value: min_data_in_leaf=15\n",
      "[LGB] Fold 2 AUC = 0.979424, best_iter = 3208\n",
      "Positive rate after oversampling: 0.48785814045483533\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=15. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=15. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Info] Number of positive: 241722, number of negative: 253754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9165\n",
      "[LightGBM] [Info] Number of data points in the train set: 495476, number of used features: 98\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=15. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487858 -> initscore=-0.048577\n",
      "[LightGBM] [Info] Start training from score -0.048577\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=15. Current value: min_data_in_leaf=15\n",
      "[LGB] Fold 3 AUC = 0.982587, best_iter = 2516\n",
      "\n",
      "LGB params={'min_samples_split': 15, 'min_samples_leaf': 15, 'max_depth': 12}, mean AUC=0.982041\n",
      "\n",
      "\n",
      ">>> Best LGB params: {'min_samples_split': 10, 'min_samples_leaf': 10, 'max_depth': 10}\n",
      ">>> Best LGB CV AUC: 0.9828007142161103\n",
      "Positive rate after oversampling: 0.48785193856338427\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=10. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=10. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 362574, number of negative: 380631\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9189\n",
      "[LightGBM] [Info] Number of data points in the train set: 743205, number of used features: 98\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487852 -> initscore=-0.048602\n",
      "[LightGBM] [Info] Start training from score -0.048602\n",
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=10. Current value: min_data_in_leaf=10\n",
      "LGB top-50% threshold = 0.0007087176581858496\n",
      "target\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Saved submission_LGBM_gridsearch.csv\n"
     ]
    }
   ],
   "source": [
    "### C. Grid Search for LightGBM\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def cv_auc_lgb(params, X, y, n_splits=3, random_state=42, early_stopping_rounds=100):\n",
    "    \"\"\"\n",
    "    For a given set of LightGBM parameters,\n",
    "    compute mean AUC using K-fold CV with oversampling on training folds only.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    aucs = []\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), start=1):\n",
    "        X_tr = X.iloc[tr_idx].copy()\n",
    "        y_tr = y.iloc[tr_idx].copy()\n",
    "        X_val = X.iloc[val_idx].copy()\n",
    "        y_val = y.iloc[val_idx].copy()\n",
    "\n",
    "        # Oversampling only on training fold\n",
    "        X_tr_os, y_tr_os = oversample(X_tr, y_tr.values)\n",
    "\n",
    "        clf = lgb.LGBMClassifier(\n",
    "            objective=\"binary\",\n",
    "            n_estimators=5000,  # large, rely on early stopping\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        clf.fit(\n",
    "            X_tr_os, y_tr_os,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric=\"auc\",\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_rounds, verbose=False)]\n",
    "        )\n",
    "\n",
    "        val_proba = clf.predict_proba(X_val)[:, 1]\n",
    "        auc = roc_auc_score(y_val, val_proba)\n",
    "        aucs.append(auc)\n",
    "\n",
    "        print(f\"[LGB] Fold {fold} AUC = {auc:.6f}, best_iter = {clf.best_iteration_}\")\n",
    "\n",
    "    return float(np.mean(aucs))\n",
    "\n",
    "\n",
    "# A small-ish grid (you can expand later)\n",
    "lgb_param_grid = [\n",
    "    # baseline-ish\n",
    "    {\"min_samples_split\": 5, \"min_samples_leaf\": 5, \"max_depth\": 8,},\n",
    "\n",
    "    {\"min_samples_split\": 10, \"min_samples_leaf\": 10, \"max_depth\": 10,},\n",
    "\n",
    "    {\"min_samples_split\": 15, \"min_samples_leaf\": 15, \"max_depth\": 12,},\n",
    "]\n",
    "\n",
    "best_lgb_auc = -1\n",
    "best_lgb_params = None\n",
    "\n",
    "for params in lgb_param_grid:\n",
    "    mean_auc = cv_auc_lgb(params, X_train, y_train, n_splits=3, random_state=42, early_stopping_rounds=100)\n",
    "    print(f\"\\nLGB params={params}, mean AUC={mean_auc:.6f}\\n\")\n",
    "    if mean_auc > best_lgb_auc:\n",
    "        best_lgb_auc = mean_auc\n",
    "        best_lgb_params = params\n",
    "\n",
    "print(\"\\n>>> Best LGB params:\", best_lgb_params)\n",
    "print(\">>> Best LGB CV AUC:\", best_lgb_auc)\n",
    "\n",
    "## 7. Refit best LGB on full train + predict + submission\n",
    "# Oversample full training set\n",
    "X_lgb_os, y_lgb_os = oversample(X_train.copy(), y_train.values)\n",
    "\n",
    "best_lgb_clf = lgb.LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=5000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    **best_lgb_params\n",
    ")\n",
    "\n",
    "best_lgb_clf.fit(X_lgb_os, y_lgb_os)\n",
    "\n",
    "pred_lgb = best_lgb_clf.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "# Align to example_submission order\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "user_ids = example_sub[\"id\"].astype(str)\n",
    "\n",
    "proba_lgb_aligned = (\n",
    "    pd.Series(pred_lgb, index=X_test_full.index)\n",
    "    .loc[user_ids]\n",
    "    .values\n",
    ")\n",
    "\n",
    "# keep your same strategy: top-50% as positive\n",
    "threshold = np.quantile(proba_lgb_aligned, 0.5)\n",
    "print(\"LGB top-50% threshold =\", threshold)\n",
    "\n",
    "pred_label = (proba_lgb_aligned >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": example_sub[\"id\"],\n",
    "    \"target\": pred_label\n",
    "})\n",
    "\n",
    "print(submission[\"target\"].value_counts(normalize=True))\n",
    "submission.to_csv(\"submission_LGBM_gridsearch.csv\", index=False)\n",
    "print(\"Saved submission_LGBM_gridsearch.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-19T01:17:22.966289900Z",
     "start_time": "2025-12-19T01:07:23.595484400Z"
    }
   },
   "id": "72395512ac9ec269"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T01:19:45.446129500Z",
     "start_time": "2025-12-19T01:17:22.966289900Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rate after oversampling: 0.48785193856338427\n",
      "Positive rate after oversampling: 0.48785193856338427\n"
     ]
    }
   ],
   "execution_count": 49,
   "source": [
    "### C. Refit Best LR & RF on Full Training Set + Voting\n",
    "\n",
    "# 1) Oversample the full training set for LR\n",
    "X_lr_os, y_lr_os = oversample(X_train.copy(), y_train.values)\n",
    "\n",
    "scaler_lr = StandardScaler()\n",
    "X_lr_os_scaled = scaler_lr.fit_transform(X_lr_os)\n",
    "X_test_lr_scaled = scaler_lr.transform(X_test_full)\n",
    "\n",
    "best_lr_clf = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    solver=\"liblinear\",\n",
    "    **best_lr_params\n",
    ")\n",
    "best_lr_clf.fit(X_lr_os_scaled, y_lr_os)\n",
    "\n",
    "pred_lr = best_lr_clf.predict_proba(X_test_lr_scaled)[:, 1]\n",
    "\n",
    "# 2) Train RF on the oversampled full training set\n",
    "X_rf_os, y_rf_os = oversample(X_train.copy(), y_train.values)\n",
    "\n",
    "best_rf_clf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    **best_rf_params\n",
    ")\n",
    "best_rf_clf.fit(X_rf_os, y_rf_os)\n",
    "\n",
    "pred_rf = best_rf_clf.predict_proba(X_test_full)[:, 1]\n"
   ],
   "id": "d1364c51eaae32ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T01:19:45.486894800Z",
     "start_time": "2025-12-19T01:19:45.447156300Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LR+RF voting top-50% threshold = 0.29589695455640014\n",
      "target\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Saved submission_LR_RF_voting_gridsearch.csv\n"
     ]
    }
   ],
   "execution_count": 50,
   "source": [
    "### D. LR + RF Soft Voting\n",
    "w_lr, w_rf = 1.0, 2.0\n",
    "\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "user_ids = example_sub[\"id\"].astype(str)\n",
    "\n",
    "proba_lr_aligned = (\n",
    "    pd.Series(pred_lr, index=X_test_full.index)\n",
    "    .loc[user_ids]\n",
    "    .values\n",
    ")\n",
    "\n",
    "proba_rf_aligned = (\n",
    "    pd.Series(pred_rf, index=X_test_full.index)\n",
    "    .loc[user_ids]\n",
    "    .values\n",
    ")\n",
    "\n",
    "voting_proba = (\n",
    "    w_lr * proba_lr_aligned +\n",
    "    w_rf * proba_rf_aligned\n",
    ") / (w_lr + w_rf)\n",
    "\n",
    "threshold = np.quantile(voting_proba, 0.5)\n",
    "print(\"Best LR+RF voting top-50% threshold =\", threshold)\n",
    "\n",
    "pred_label = (voting_proba >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": example_sub[\"id\"],\n",
    "    \"target\": pred_label\n",
    "})\n",
    "print(submission[\"target\"].value_counts(normalize=True))\n",
    "\n",
    "submission.to_csv(\"submission_LR_RF_voting_gridsearch.csv\", index=False)\n",
    "print(\"Saved submission_LR_RF_voting_gridsearch.csv\")"
   ],
   "id": "cd82b26d1bcb8357"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T01:19:45.487736Z",
     "start_time": "2025-12-19T01:19:45.484197700Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 50,
   "source": [],
   "id": "71ec74b4781df6be"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-19T01:19:45.491872400Z",
     "start_time": "2025-12-19T01:19:45.486894800Z"
    }
   },
   "id": "9cb9fc24f65a06ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
