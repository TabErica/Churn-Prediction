{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:03:39.682761Z",
     "start_time": "2025-12-19T00:03:39.274150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "id": "3d1043bcff59b542",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Preprocess",
   "id": "987bc04e2197fd51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:04:15.483874Z",
     "start_time": "2025-12-19T00:03:39.684341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Read raw parquet files\n",
    "train_df = pd.read_parquet(\"train.parquet\")\n",
    "test_df = pd.read_parquet(\"test.parquet\")\n",
    "\n",
    "# 3. Convert time-related columns\n",
    "train_df['ts'] = pd.to_datetime(train_df['ts'], unit='ms')\n",
    "test_df['ts'] = pd.to_datetime(test_df['ts'], unit='ms')\n",
    "\n",
    "train_df['registration'] = pd.to_datetime(train_df['registration'])\n",
    "test_df['registration'] = pd.to_datetime(test_df['registration'])\n",
    "\n",
    "train_df['time'] = pd.to_datetime(train_df['time'])\n",
    "test_df['time'] = pd.to_datetime(test_df['time'])\n",
    "\n",
    "# 4. Clean the 'page' field (strip spaces)\n",
    "train_df['page'] = train_df['page'].astype(str).str.strip()\n",
    "test_df['page'] = test_df['page'].astype(str).str.strip()\n",
    "\n",
    "# 5. Extract state from location (last two characters) as 'state'\n",
    "#    e.g., \"New York, NY\" -> \"NY\"\n",
    "train_df['location'] = train_df['location'].astype(str).str.strip()\n",
    "test_df['location'] = test_df['location'].astype(str).str.strip()\n",
    "\n",
    "train_df['state'] = train_df['location'].str[-2:]\n",
    "test_df['state'] = test_df['location'].str[-2:]\n",
    "\n",
    "# 6. Sort by userId + ts to ensure chronological order\n",
    "train_df = train_df.sort_values(['userId', 'ts'])\n",
    "test_df = test_df.sort_values(['userId', 'ts'])\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)"
   ],
   "id": "672d792e753867fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (17499636, 20)\n",
      "Test shape: (4393179, 20)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Window label",
   "id": "7b688d3559067ca5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:04:15.536497Z",
     "start_time": "2025-12-19T00:04:15.485964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Set sliding-window parameters\n",
    "horizon_days = 10  # prediction window length = 10 days (competition setting)\n",
    "\n",
    "# Manually set the earliest cutoff start date\n",
    "cutoff_start = pd.to_datetime(\"2018-10-01\")\n",
    "\n",
    "# Cutoff end must satisfy: cutoff + 10 days <= train_df['ts'].max().normalize()\n",
    "# Take max date (daily), then subtract horizon_days\n",
    "max_ts_date = train_df['ts'].max().normalize()          # usually 2018-11-20\n",
    "cutoff_end = max_ts_date - pd.Timedelta(days=horizon_days + 1)\n",
    "\n",
    "print(\"cutoff_start:\", cutoff_start.date())\n",
    "print(\"max_ts_date :\", max_ts_date.date())\n",
    "print(\"cutoff_end  :\", cutoff_end.date())"
   ],
   "id": "8d72dfefa5c54d31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutoff_start: 2018-10-01\n",
      "max_ts_date : 2018-11-20\n",
      "cutoff_end  : 2018-11-09\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:04:15.541451Z",
     "start_time": "2025-12-19T00:04:15.537274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate a list of cutoffs: one cutoff per day\n",
    "cutoff_dates = pd.date_range(start=cutoff_start, end=cutoff_end, freq=\"D\")\n",
    "print(\"cutoff_dates count:\", len(cutoff_dates))\n",
    "print(\"cutoff_dates preview:\", list(cutoff_dates[:5]), \"...\", list(cutoff_dates[-5:]))"
   ],
   "id": "ad2e1bfb0aa8d033",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutoff_dates count: 40\n",
      "cutoff_dates preview: [Timestamp('2018-10-01 00:00:00'), Timestamp('2018-10-02 00:00:00'), Timestamp('2018-10-03 00:00:00'), Timestamp('2018-10-04 00:00:00'), Timestamp('2018-10-05 00:00:00')] ... [Timestamp('2018-11-05 00:00:00'), Timestamp('2018-11-06 00:00:00'), Timestamp('2018-11-07 00:00:00'), Timestamp('2018-11-08 00:00:00'), Timestamp('2018-11-09 00:00:00')]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:04:16.408844Z",
     "start_time": "2025-12-19T00:04:15.543665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. For each cutoff_date, generate samples (userId, cutoff_date, target)\n",
    "\n",
    "# First compute each user's first churn timestamp (first time they hit Cancellation Confirmation)\n",
    "first_churn_ts = (\n",
    "    train_df[train_df['page'] == \"Cancellation Confirmation\"]\n",
    "    .groupby('userId')['ts']\n",
    "    .min()\n",
    ")\n",
    "\n",
    "print(\"Number of users who churned at least once:\", len(first_churn_ts))"
   ],
   "id": "5fc64745e92c4566",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users who churned at least once: 4271\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:04:54.014500Z",
     "start_time": "2025-12-19T00:04:16.409664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_samples_list = []\n",
    "\n",
    "for cutoff_date in cutoff_dates:\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"Current cutoff_date =\", cutoff_date.date())\n",
    "    \n",
    "    # ---- 2.1 Observation window & prediction window ----\n",
    "    # Observation window: ts <= cutoff_date\n",
    "    obs_mask = (train_df['ts'] <= cutoff_date)\n",
    "    \n",
    "    # Prediction window: cutoff_date < ts <= cutoff_date + horizon_days\n",
    "    future_end = cutoff_date + pd.Timedelta(days=horizon_days)\n",
    "    fut_mask = (\n",
    "        (train_df['ts'] > cutoff_date) &\n",
    "        (train_df['ts'] <= future_end)\n",
    "    )\n",
    "    \n",
    "    # ---- 2.2 Users observed within the observation window ----\n",
    "    users_obs = train_df.loc[obs_mask, 'userId'].unique()\n",
    "    users_obs = np.sort(users_obs)\n",
    "    print(\"Users in observation window (including churned):\", len(users_obs))\n",
    "    \n",
    "    if len(users_obs) == 0:\n",
    "        print(\"No observed users for this cutoff; skip\")\n",
    "        continue\n",
    "\n",
    "    # ---- 2.2.1 Filter out users who already churned ----\n",
    "    # Reindex first_churn_ts by users_obs to align:\n",
    "    #   index = users_obs, value = first churn ts or NaT\n",
    "    churn_ts_sub = first_churn_ts.reindex(users_obs)\n",
    "    \n",
    "    # Keep conditions:\n",
    "    #   - churn_ts is NaT -> never churned -> keep\n",
    "    #   - churn_ts > cutoff_date -> churn happens in the future -> keep\n",
    "    #   - churn_ts <= cutoff_date -> already churned (or churned on cutoff day) -> drop\n",
    "    alive_mask = (churn_ts_sub.isna()) | (churn_ts_sub > cutoff_date)\n",
    "    alive_users = users_obs[alive_mask.values]\n",
    "    \n",
    "    print(\"Alive users after filtering already-churned:\", len(alive_users))\n",
    "    \n",
    "    if len(alive_users) == 0:\n",
    "        print(\"No alive users for this cutoff; skip\")\n",
    "        continue\n",
    "    \n",
    "    # ---- 2.3 Users who hit 'Cancellation Confirmation' in the prediction window ----\n",
    "    cc_future_users = (\n",
    "        train_df.loc[fut_mask & (train_df['page'] == \"Cancellation Confirmation\"), 'userId']\n",
    "        .unique()\n",
    "    )\n",
    "    \n",
    "    # Keep only users who are alive and observed\n",
    "    cc_future_users = np.intersect1d(cc_future_users, alive_users)\n",
    "    print(\"Alive users who churn in prediction window (target=1):\", len(cc_future_users))\n",
    "    \n",
    "    # ---- 2.4 Build labels for current cutoff ----\n",
    "    # alive_users defines the sample user list for this cutoff\n",
    "    # Default target=0; set target=1 for users in cc_future_users\n",
    "    y_array = np.zeros(len(alive_users), dtype=int)\n",
    "    pos_mask = np.isin(alive_users, cc_future_users)\n",
    "    y_array[pos_mask] = 1\n",
    "    \n",
    "    # ---- 2.5 Create a DataFrame for this cutoff ----\n",
    "    tmp = pd.DataFrame({\n",
    "        \"userId\": alive_users,\n",
    "        \"cutoff_date\": cutoff_date,   # same value for all rows in this batch\n",
    "        \"target\": y_array,\n",
    "    })\n",
    "    \n",
    "    print(\"Samples at this cutoff:\", len(tmp),\n",
    "          \"  Positives:\", tmp['target'].sum(),\n",
    "          \"  Positive rate:\", tmp['target'].mean())\n",
    "    \n",
    "    all_samples_list.append(tmp)"
   ],
   "id": "a7b593f8e8467b14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-01\n",
      "Users in observation window (including churned): 0\n",
      "No observed users for this cutoff; skip\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-02\n",
      "Users in observation window (including churned): 5261\n",
      "Alive users after filtering already-churned: 5144\n",
      "Alive users who churn in prediction window (target=1): 494\n",
      "Samples at this cutoff: 5144   Positives: 494   Positive rate: 0.09603421461897356\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-03\n",
      "Users in observation window (including churned): 8150\n",
      "Alive users after filtering already-churned: 7904\n",
      "Alive users who churn in prediction window (target=1): 660\n",
      "Samples at this cutoff: 7904   Positives: 660   Positive rate: 0.08350202429149797\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-04\n",
      "Users in observation window (including churned): 10115\n",
      "Alive users after filtering already-churned: 9740\n",
      "Alive users who churn in prediction window (target=1): 715\n",
      "Samples at this cutoff: 9740   Positives: 715   Positive rate: 0.07340862422997947\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-05\n",
      "Users in observation window (including churned): 11521\n",
      "Alive users after filtering already-churned: 11020\n",
      "Alive users who churn in prediction window (target=1): 743\n",
      "Samples at this cutoff: 11020   Positives: 743   Positive rate: 0.06742286751361161\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-06\n",
      "Users in observation window (including churned): 12606\n",
      "Alive users after filtering already-churned: 11990\n",
      "Alive users who churn in prediction window (target=1): 759\n",
      "Samples at this cutoff: 11990   Positives: 759   Positive rate: 0.06330275229357799\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-07\n",
      "Users in observation window (including churned): 13043\n",
      "Alive users after filtering already-churned: 12371\n",
      "Alive users who churn in prediction window (target=1): 794\n",
      "Samples at this cutoff: 12371   Positives: 794   Positive rate: 0.06418236197558806\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-08\n",
      "Users in observation window (including churned): 13508\n",
      "Alive users after filtering already-churned: 12780\n",
      "Alive users who churn in prediction window (target=1): 854\n",
      "Samples at this cutoff: 12780   Positives: 854   Positive rate: 0.06682316118935837\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-09\n",
      "Users in observation window (including churned): 14203\n",
      "Alive users after filtering already-churned: 13367\n",
      "Alive users who churn in prediction window (target=1): 874\n",
      "Samples at this cutoff: 13367   Positives: 874   Positive rate: 0.06538490311962296\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-10\n",
      "Users in observation window (including churned): 14781\n",
      "Alive users after filtering already-churned: 13825\n",
      "Alive users who churn in prediction window (target=1): 871\n",
      "Samples at this cutoff: 13825   Positives: 871   Positive rate: 0.06300180831826402\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-11\n",
      "Users in observation window (including churned): 15254\n",
      "Alive users after filtering already-churned: 14176\n",
      "Alive users who churn in prediction window (target=1): 821\n",
      "Samples at this cutoff: 14176   Positives: 821   Positive rate: 0.0579147855530474\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-12\n",
      "Users in observation window (including churned): 15649\n",
      "Alive users after filtering already-churned: 14447\n",
      "Alive users who churn in prediction window (target=1): 772\n",
      "Samples at this cutoff: 14447   Positives: 772   Positive rate: 0.053436699660829236\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-13\n",
      "Users in observation window (including churned): 15966\n",
      "Alive users after filtering already-churned: 14655\n",
      "Alive users who churn in prediction window (target=1): 765\n",
      "Samples at this cutoff: 14655   Positives: 765   Positive rate: 0.052200614124872056\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-14\n",
      "Users in observation window (including churned): 16123\n",
      "Alive users after filtering already-churned: 14737\n",
      "Alive users who churn in prediction window (target=1): 787\n",
      "Samples at this cutoff: 14737   Positives: 787   Positive rate: 0.05340299925357943\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-15\n",
      "Users in observation window (including churned): 16271\n",
      "Alive users after filtering already-churned: 14832\n",
      "Alive users who churn in prediction window (target=1): 826\n",
      "Samples at this cutoff: 14832   Positives: 826   Positive rate: 0.05569039913700108\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-16\n",
      "Users in observation window (including churned): 16537\n",
      "Alive users after filtering already-churned: 15003\n",
      "Alive users who churn in prediction window (target=1): 837\n",
      "Samples at this cutoff: 15003   Positives: 837   Positive rate: 0.05578884223155369\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-17\n",
      "Users in observation window (including churned): 16754\n",
      "Alive users after filtering already-churned: 15135\n",
      "Alive users who churn in prediction window (target=1): 858\n",
      "Samples at this cutoff: 15135   Positives: 858   Positive rate: 0.05668979187314172\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-18\n",
      "Users in observation window (including churned): 16972\n",
      "Alive users after filtering already-churned: 15251\n",
      "Alive users who churn in prediction window (target=1): 819\n",
      "Samples at this cutoff: 15251   Positives: 819   Positive rate: 0.0537013966297292\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-19\n",
      "Users in observation window (including churned): 17196\n",
      "Alive users after filtering already-churned: 15377\n",
      "Alive users who churn in prediction window (target=1): 784\n",
      "Samples at this cutoff: 15377   Positives: 784   Positive rate: 0.050985237692657864\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-20\n",
      "Users in observation window (including churned): 17347\n",
      "Alive users after filtering already-churned: 15434\n",
      "Alive users who churn in prediction window (target=1): 777\n",
      "Samples at this cutoff: 15434   Positives: 777   Positive rate: 0.05034339769340417\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-21\n",
      "Users in observation window (including churned): 17416\n",
      "Alive users after filtering already-churned: 15452\n",
      "Alive users who churn in prediction window (target=1): 817\n",
      "Samples at this cutoff: 15452   Positives: 817   Positive rate: 0.05287341444473207\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-22\n",
      "Users in observation window (including churned): 17500\n",
      "Alive users after filtering already-churned: 15471\n",
      "Alive users who churn in prediction window (target=1): 830\n",
      "Samples at this cutoff: 15471   Positives: 830   Positive rate: 0.05364876220024562\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-23\n",
      "Users in observation window (including churned): 17630\n",
      "Alive users after filtering already-churned: 15503\n",
      "Alive users who churn in prediction window (target=1): 828\n",
      "Samples at this cutoff: 15503   Positives: 828   Positive rate: 0.05340901760949494\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-24\n",
      "Users in observation window (including churned): 17771\n",
      "Alive users after filtering already-churned: 15552\n",
      "Alive users who churn in prediction window (target=1): 842\n",
      "Samples at this cutoff: 15552   Positives: 842   Positive rate: 0.05414094650205761\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-25\n",
      "Users in observation window (including churned): 17888\n",
      "Alive users after filtering already-churned: 15571\n",
      "Alive users who churn in prediction window (target=1): 803\n",
      "Samples at this cutoff: 15571   Positives: 803   Positive rate: 0.05157022670348725\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-26\n",
      "Users in observation window (including churned): 17993\n",
      "Alive users after filtering already-churned: 15575\n",
      "Alive users who churn in prediction window (target=1): 757\n",
      "Samples at this cutoff: 15575   Positives: 757   Positive rate: 0.04860353130016051\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-27\n",
      "Users in observation window (including churned): 18096\n",
      "Alive users after filtering already-churned: 15574\n",
      "Alive users who churn in prediction window (target=1): 719\n",
      "Samples at this cutoff: 15574   Positives: 719   Positive rate: 0.046166688069860025\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-28\n",
      "Users in observation window (including churned): 18136\n",
      "Alive users after filtering already-churned: 15561\n",
      "Alive users who churn in prediction window (target=1): 765\n",
      "Samples at this cutoff: 15561   Positives: 765   Positive rate: 0.049161364950838636\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-29\n",
      "Users in observation window (including churned): 18191\n",
      "Alive users after filtering already-churned: 15563\n",
      "Alive users who churn in prediction window (target=1): 814\n",
      "Samples at this cutoff: 15563   Positives: 814   Positive rate: 0.05230354044849965\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-30\n",
      "Users in observation window (including churned): 18271\n",
      "Alive users after filtering already-churned: 15558\n",
      "Alive users who churn in prediction window (target=1): 814\n",
      "Samples at this cutoff: 15558   Positives: 814   Positive rate: 0.052320349659339245\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-10-31\n",
      "Users in observation window (including churned): 18333\n",
      "Alive users after filtering already-churned: 15530\n",
      "Alive users who churn in prediction window (target=1): 789\n",
      "Samples at this cutoff: 15530   Positives: 789   Positive rate: 0.05080489375402447\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-01\n",
      "Users in observation window (including churned): 18419\n",
      "Alive users after filtering already-churned: 15541\n",
      "Alive users who churn in prediction window (target=1): 765\n",
      "Samples at this cutoff: 15541   Positives: 765   Positive rate: 0.0492246316195869\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-02\n",
      "Users in observation window (including churned): 18493\n",
      "Alive users after filtering already-churned: 15519\n",
      "Alive users who churn in prediction window (target=1): 713\n",
      "Samples at this cutoff: 15519   Positives: 713   Positive rate: 0.04594368193826922\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-03\n",
      "Users in observation window (including churned): 18562\n",
      "Alive users after filtering already-churned: 15486\n",
      "Alive users who churn in prediction window (target=1): 678\n",
      "Samples at this cutoff: 15486   Positives: 678   Positive rate: 0.043781480046493605\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-04\n",
      "Users in observation window (including churned): 18592\n",
      "Alive users after filtering already-churned: 15460\n",
      "Alive users who churn in prediction window (target=1): 702\n",
      "Samples at this cutoff: 15460   Positives: 702   Positive rate: 0.04540750323415265\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-05\n",
      "Users in observation window (including churned): 18620\n",
      "Alive users after filtering already-churned: 15433\n",
      "Alive users who churn in prediction window (target=1): 735\n",
      "Samples at this cutoff: 15433   Positives: 735   Positive rate: 0.04762521868722867\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-06\n",
      "Users in observation window (including churned): 18670\n",
      "Alive users after filtering already-churned: 15417\n",
      "Alive users who churn in prediction window (target=1): 762\n",
      "Samples at this cutoff: 15417   Positives: 762   Positive rate: 0.04942595835765713\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-07\n",
      "Users in observation window (including churned): 18716\n",
      "Alive users after filtering already-churned: 15363\n",
      "Alive users who churn in prediction window (target=1): 749\n",
      "Samples at this cutoff: 15363   Positives: 749   Positive rate: 0.0487534986656252\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-08\n",
      "Users in observation window (including churned): 18775\n",
      "Alive users after filtering already-churned: 15324\n",
      "Alive users who churn in prediction window (target=1): 705\n",
      "Samples at this cutoff: 15324   Positives: 705   Positive rate: 0.046006264682850434\n",
      "\n",
      "==============================\n",
      "Current cutoff_date = 2018-11-09\n",
      "Users in observation window (including churned): 18827\n",
      "Alive users after filtering already-churned: 15289\n",
      "Alive users who churn in prediction window (target=1): 655\n",
      "Samples at this cutoff: 15289   Positives: 655   Positive rate: 0.042841258421087054\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:04:54.034055Z",
     "start_time": "2025-12-19T00:04:54.015753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Concatenate all cutoff samples into the full sliding-window label table\n",
    "\n",
    "sliding_labels = pd.concat(all_samples_list, ignore_index=True)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Overall after merging all cutoffs:\")\n",
    "print(\"Total samples:\", len(sliding_labels))\n",
    "print(\"Total positives:\", sliding_labels['target'].sum())\n",
    "print(\"Overall positive rate:\", sliding_labels['target'].mean())\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(sliding_labels['target'].value_counts(normalize=True))"
   ],
   "id": "9b4134c7795f360d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Overall after merging all cutoffs:\n",
      "Total samples: 556930\n",
      "Total positives: 30052\n",
      "Overall positive rate: 0.05396010270590559\n",
      "\n",
      "Target distribution:\n",
      "target\n",
      "0    0.94604\n",
      "1    0.05396\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Training set features",
   "id": "4fabd5d72f19a955"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Category features",
   "id": "ce95fd9c46b704fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:05:04.100247Z",
     "start_time": "2025-12-19T00:04:54.035007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Static user-level info (train_df is sufficient)\n",
    "user_static = (\n",
    "    train_df\n",
    "    .sort_values('ts')\n",
    "    .groupby('userId')\n",
    "    .agg({\n",
    "        'gender': 'first',   # or 'last' is fine; gender does not change\n",
    "        'state':  'first',   # extracted from location\n",
    "    })\n",
    ")\n",
    "\n",
    "# Map into sliding_labels\n",
    "sliding_labels['gender'] = sliding_labels['userId'].map(user_static['gender'])\n",
    "sliding_labels['state']  = sliding_labels['userId'].map(user_static['state'])\n",
    "\n",
    "print(\"sliding_labels with gender/state:\")\n",
    "print(sliding_labels[['userId', 'cutoff_date', 'target', 'gender', 'state']].head())"
   ],
   "id": "887ae17677bf26ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sliding_labels with gender/state:\n",
      "    userId cutoff_date  target gender state\n",
      "0  1000083  2018-10-02       0      M    IN\n",
      "1  1000164  2018-10-02       0      F    AZ\n",
      "2  1000280  2018-10-02       0      M    OH\n",
      "3  1000353  2018-10-02       0      F    TX\n",
      "4  1000407  2018-10-02       0      M    CO\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Multi-Index",
   "id": "76cb789eb2a3421b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:05:04.388890Z",
     "start_time": "2025-12-19T00:05:04.102028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use userId_cutoff-date as MultiIndex (required)\n",
    "sliding_labels['sample_id'] = (\n",
    "    sliding_labels['userId'].astype(str)\n",
    "    + \"_\" +\n",
    "    sliding_labels['cutoff_date'].astype(str)\n",
    ")\n",
    "\n",
    "# Set index\n",
    "sliding_labels = sliding_labels.set_index('sample_id')\n",
    "\n",
    "# Target y_all\n",
    "y_all = sliding_labels['target']\n",
    "\n",
    "# Cutoff timestamps (direct reference)\n",
    "cutoff_ts_all = sliding_labels['cutoff_date']\n",
    "\n",
    "print(\"sliding_labels head:\")\n",
    "print(sliding_labels.head())\n",
    "print(\"y_all head:\")\n",
    "print(y_all.head())\n",
    "print(\"cutoff_ts_all head:\")\n",
    "print(cutoff_ts_all.head())"
   ],
   "id": "edaba3023ead8154",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sliding_labels head:\n",
      "                     userId cutoff_date  target gender state\n",
      "sample_id                                                   \n",
      "1000083_2018-10-02  1000083  2018-10-02       0      M    IN\n",
      "1000164_2018-10-02  1000164  2018-10-02       0      F    AZ\n",
      "1000280_2018-10-02  1000280  2018-10-02       0      M    OH\n",
      "1000353_2018-10-02  1000353  2018-10-02       0      F    TX\n",
      "1000407_2018-10-02  1000407  2018-10-02       0      M    CO\n",
      "y_all head:\n",
      "sample_id\n",
      "1000083_2018-10-02    0\n",
      "1000164_2018-10-02    0\n",
      "1000280_2018-10-02    0\n",
      "1000353_2018-10-02    0\n",
      "1000407_2018-10-02    0\n",
      "Name: target, dtype: int64\n",
      "cutoff_ts_all head:\n",
      "sample_id\n",
      "1000083_2018-10-02   2018-10-02\n",
      "1000164_2018-10-02   2018-10-02\n",
      "1000280_2018-10-02   2018-10-02\n",
      "1000353_2018-10-02   2018-10-02\n",
      "1000407_2018-10-02   2018-10-02\n",
      "Name: cutoff_date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3 Lifetime",
   "id": "20f2c9fb1adb48e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:05:11.905723Z",
     "start_time": "2025-12-19T00:05:04.389657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Each user's registration timestamp\n",
    "uid_registration = (\n",
    "    train_df\n",
    "    .sort_values('ts')\n",
    "    .groupby('userId')['registration']\n",
    "    .first()\n",
    ")\n",
    "\n",
    "# Map into sliding_labels\n",
    "sliding_labels['registration_ts'] = sliding_labels['userId'].map(uid_registration)\n",
    "\n",
    "# Days from registration to cutoff\n",
    "days_since_registration = (\n",
    "    (sliding_labels['cutoff_date'] - sliding_labels['registration_ts'])\n",
    "    / np.timedelta64(1, 'D')\n",
    ").astype('float32')\n",
    "\n",
    "# Clip negative values (rare: abnormal registration timestamp)\n",
    "days_since_registration = days_since_registration.clip(lower=0)\n",
    "\n",
    "print(\"days_since_registration example:\")\n",
    "print(days_since_registration.head())"
   ],
   "id": "6018b85dc66e1176",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days_since_registration example:\n",
      "sample_id\n",
      "1000083_2018-10-02     24.248739\n",
      "1000164_2018-10-02     50.602768\n",
      "1000280_2018-10-02     34.345612\n",
      "1000353_2018-10-02    152.515518\n",
      "1000407_2018-10-02     12.492084\n",
      "dtype: float32\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.4 Behaviors",
   "id": "383154b67aadffba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:05:11.918327Z",
     "start_time": "2025-12-19T00:05:11.906940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure cutoff_date is datetime\n",
    "sliding_labels['cutoff_date'] = pd.to_datetime(sliding_labels['cutoff_date'])"
   ],
   "id": "efddf0df1d641f6f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:05:37.154025Z",
     "start_time": "2025-12-19T00:05:11.919422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Sort train_df by userId + ts to ensure per-user order\n",
    "train_df_sorted = train_df.sort_values(['userId', 'ts']).copy()\n",
    "\n",
    "# 2) Group behavior data by user (train side)\n",
    "train_groups = dict(tuple(train_df_sorted.groupby('userId')))\n",
    "\n",
    "# 3) Group label samples by user (sliding_labels index is sample_id)\n",
    "label_groups = dict(tuple(sliding_labels.groupby('userId')))  # user -> label table\n",
    "\n",
    "print(\"Number of users (train):\", len(train_groups))\n",
    "print(\"Number of users (sliding_labels):\", len(label_groups))  # users appearing only after 2018-11-10 may be missing"
   ],
   "id": "b43844f0c91ffacc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users (train): 19140\n",
      "Number of users (sliding_labels): 18442\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:05:39.261699Z",
     "start_time": "2025-12-19T00:05:37.154954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize series for behavior counts\n",
    "\n",
    "# Initialize: index = sample_id (sliding_labels.index), all zeros\n",
    "n_events = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    # Skip if user has no behavior data in train\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "\n",
    "    # All events for this user (already sorted by ts)\n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values  # datetime64[ns] array\n",
    "\n",
    "    # All samples for this user in sliding_labels\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # searchsorted: for each cutoff, find number of events <= cutoff\n",
    "    pos = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "\n",
    "    # Write back into n_events (aligned by sample_id)\n",
    "    n_events.loc[sample_ids] = pos.astype('int32')\n",
    "\n",
    "print(\"n_events computed\")\n",
    "print(n_events.head())"
   ],
   "id": "9003a32b9a4c05cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_events computed\n",
      "sample_id\n",
      "1000083_2018-10-02    14\n",
      "1000164_2018-10-02     2\n",
      "1000280_2018-10-02    37\n",
      "1000353_2018-10-02    97\n",
      "1000407_2018-10-02     5\n",
      "dtype: int32\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.5 recency_hours",
   "id": "a916f5eadfad55e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:05:47.629518Z",
     "start_time": "2025-12-19T00:05:39.265937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Formula: cutoff_date - last event timestamp\n",
    "\n",
    "# Initialize recency_hours (hours)\n",
    "recency_hours = pd.Series(np.nan, index=sliding_labels.index, dtype='float32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values  # sorted datetime64 array\n",
    "    \n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "    \n",
    "    # hi[j] = number of events <= cutoff\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "    \n",
    "    # hi==0 means no events <= cutoff; set recency to a large value (e.g., 9999 hours)\n",
    "    # hi>0 means last event index is hi-1\n",
    "    for idx_in_uid, sample_id in enumerate(sample_ids):\n",
    "        h = hi[idx_in_uid]\n",
    "        \n",
    "        if h == 0:\n",
    "            # No event before cutoff -> very large recency (9999 hours ~ 416 days)\n",
    "            recency_hours.loc[sample_id] = 9999.0\n",
    "        else:\n",
    "            last_ts = ts_vals[h-1]\n",
    "            delta = (cutoffs[idx_in_uid] - last_ts)\n",
    "            recency_hours.loc[sample_id] = delta / np.timedelta64(1, 'h')\n",
    "\n",
    "print(\"recency_hours computed\")\n",
    "print(recency_hours.head())"
   ],
   "id": "51fdd8f129718d3e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/xtlxv68s5sv1j3rj3ww3zh_h0000gn/T/ipykernel_97100/1883081265.py:30: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.05472222222222222' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  recency_hours.loc[sample_id] = delta / np.timedelta64(1, 'h')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recency_hours computed\n",
      "sample_id\n",
      "1000083_2018-10-02    12.570556\n",
      "1000164_2018-10-02     6.302222\n",
      "1000280_2018-10-02     1.177222\n",
      "1000353_2018-10-02    18.474167\n",
      "1000407_2018-10-02    13.384167\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.6 events_last_7d",
   "id": "fd4ef4b1b8af6ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:05:48.572065Z",
     "start_time": "2025-12-19T00:05:47.631870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "events_last_7d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "seven_days = np.timedelta64(7, 'D')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "    \n",
    "    # hi = number of events <= cutoff\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "\n",
    "    # lo = number of events <= cutoff - 7 days\n",
    "    window_starts = cutoffs - seven_days\n",
    "    lo = np.searchsorted(ts_vals, window_starts, side='right')\n",
    "    \n",
    "    # Events in last 7 days = hi - lo\n",
    "    cnt_7d = hi - lo\n",
    "    events_last_7d.loc[sample_ids] = cnt_7d.astype('int32')\n",
    "\n",
    "print(\"events_last_7d computed\")\n",
    "print(events_last_7d.head())"
   ],
   "id": "5b70f1b0c6018612",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events_last_7d computed\n",
      "sample_id\n",
      "1000083_2018-10-02    14\n",
      "1000164_2018-10-02     2\n",
      "1000280_2018-10-02    37\n",
      "1000353_2018-10-02    97\n",
      "1000407_2018-10-02     5\n",
      "dtype: int32\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.7 songs_last_7d",
   "id": "4c0978a29f98b279"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:05:57.572305Z",
     "start_time": "2025-12-19T00:05:48.572801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "songs_last_7d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "seven_days = np.timedelta64(7, 'D')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    # Filter song-play events (NextSong)\n",
    "    df_song = df_u[df_u['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        # User never listened to songs -> all zeros\n",
    "        continue\n",
    "    \n",
    "    ts_song_vals = df_song['ts'].values  # sorted datetime array\n",
    "\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi = number of songs <= cutoff\n",
    "    hi = np.searchsorted(ts_song_vals, cutoffs, side='right')\n",
    "\n",
    "    # lo = number of songs <= cutoff - 7 days\n",
    "    window_starts = cutoffs - seven_days\n",
    "    lo = np.searchsorted(ts_song_vals, window_starts, side='right')\n",
    "\n",
    "    cnt_7d = hi - lo\n",
    "    songs_last_7d.loc[sample_ids] = cnt_7d.astype('int32')\n",
    "\n",
    "print(\"songs_last_7d computed\")\n",
    "print(songs_last_7d.head())"
   ],
   "id": "3ab699bf5654bfd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "songs_last_7d computed\n",
      "sample_id\n",
      "1000083_2018-10-02    11\n",
      "1000164_2018-10-02     1\n",
      "1000280_2018-10-02    24\n",
      "1000353_2018-10-02    75\n",
      "1000407_2018-10-02     3\n",
      "dtype: int32\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.8 active_days",
   "id": "54368ab6f65a0a59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:01.944790Z",
     "start_time": "2025-12-19T00:05:57.573710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "active_days = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    # Extract dates only (sorted by ts)\n",
    "    dates_u = df_u['ts'].dt.normalize().values\n",
    "\n",
    "    # Unique dates (sorted)\n",
    "    unique_days = np.unique(dates_u)\n",
    "    \n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "    \n",
    "    # searchsorted on date array: count dates <= cutoff\n",
    "    hi = np.searchsorted(unique_days, cutoffs, side='right')\n",
    "\n",
    "    # hi[j] is the active day count for this sample\n",
    "    active_days.loc[sample_ids] = hi.astype('int32')\n",
    "\n",
    "print(\"active_days computed\")\n",
    "print(active_days.head())"
   ],
   "id": "9d922c152afd0074",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active_days computed\n",
      "sample_id\n",
      "1000083_2018-10-02    2\n",
      "1000164_2018-10-02    1\n",
      "1000280_2018-10-02    1\n",
      "1000353_2018-10-02    2\n",
      "1000407_2018-10-02    2\n",
      "dtype: int32\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.9 total_listen_time",
   "id": "12c33dfb7e71ecd4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:17.355683Z",
     "start_time": "2025-12-19T00:06:01.953084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_listen_time = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    # Filter NextSong events\n",
    "    df_song = df_u[df_u['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        continue\n",
    "    \n",
    "    ts_song_vals = df_song['ts'].values\n",
    "    len_song_vals = df_song['length'].values\n",
    "\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi = number of songs <= cutoff\n",
    "    hi = np.searchsorted(ts_song_vals, cutoffs, side='right')\n",
    "\n",
    "    # Prefix sum of song lengths for fast cumulative lookup\n",
    "    cum_len = np.cumsum(len_song_vals)\n",
    "\n",
    "    for j, sample_id in enumerate(sample_ids):\n",
    "        h = hi[j]\n",
    "        if h == 0:\n",
    "            total_listen_time.loc[sample_id] = 0.0\n",
    "        else:\n",
    "            total_listen_time.loc[sample_id] = float(cum_len[h-1])\n",
    "\n",
    "print(\"total_listen_time computed\")\n",
    "print(total_listen_time.head())"
   ],
   "id": "9070ac47b501b743",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/xtlxv68s5sv1j3rj3ww3zh_h0000gn/T/ipykernel_97100/3442636039.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '24922.314469999994' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  total_listen_time.loc[sample_id] = float(cum_len[h-1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_listen_time computed\n",
      "sample_id\n",
      "1000083_2018-10-02     2312.40644\n",
      "1000164_2018-10-02      227.42159\n",
      "1000280_2018-10-02     6200.23076\n",
      "1000353_2018-10-02    18886.05248\n",
      "1000407_2018-10-02      804.98803\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.10 events_last_1d/3d",
   "id": "9f6d4dc3a5addc07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:19.089699Z",
     "start_time": "2025-12-19T00:06:17.357080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "events_last_1d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "events_last_3d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "one_day   = np.timedelta64(1, 'D')\n",
    "three_days = np.timedelta64(3, 'D')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi: number of events <= cutoff (same as n_events)\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "\n",
    "    # ---- last 1 day ----\n",
    "    window_start_1d = cutoffs - one_day\n",
    "    lo_1d = np.searchsorted(ts_vals, window_start_1d, side='right')\n",
    "    cnt_1d = hi - lo_1d\n",
    "    events_last_1d.loc[sample_ids] = cnt_1d.astype('int32')\n",
    "\n",
    "    # ---- last 3 days ----\n",
    "    window_start_3d = cutoffs - three_days\n",
    "    lo_3d = np.searchsorted(ts_vals, window_start_3d, side='right')\n",
    "    cnt_3d = hi - lo_3d\n",
    "    events_last_3d.loc[sample_ids] = cnt_3d.astype('int32')\n",
    "\n",
    "print(\"events_last_1d / 3d computed\")\n",
    "print(events_last_1d.head())\n",
    "print(events_last_3d.head())"
   ],
   "id": "5eec1ead61f9e684",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events_last_1d / 3d computed\n",
      "sample_id\n",
      "1000083_2018-10-02    14\n",
      "1000164_2018-10-02     2\n",
      "1000280_2018-10-02    37\n",
      "1000353_2018-10-02    97\n",
      "1000407_2018-10-02     5\n",
      "dtype: int32\n",
      "sample_id\n",
      "1000083_2018-10-02    14\n",
      "1000164_2018-10-02     2\n",
      "1000280_2018-10-02    37\n",
      "1000353_2018-10-02    97\n",
      "1000407_2018-10-02     5\n",
      "dtype: int32\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.11 songs_last_1d/3d",
   "id": "d244585323a9bccb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:27.544340Z",
     "start_time": "2025-12-19T00:06:19.090419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "songs_last_1d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "songs_last_3d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "one_day    = np.timedelta64(1, 'D')\n",
    "three_days = np.timedelta64(3, 'D')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    df_song = df_u[df_u['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        continue\n",
    "    \n",
    "    ts_song_vals = df_song['ts'].values\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi: number of songs <= cutoff\n",
    "    hi = np.searchsorted(ts_song_vals, cutoffs, side='right')\n",
    "\n",
    "    # ---- last 1 day ----\n",
    "    window_start_1d = cutoffs - one_day\n",
    "    lo_1d = np.searchsorted(ts_song_vals, window_start_1d, side='right')\n",
    "    cnt_1d = hi - lo_1d\n",
    "    songs_last_1d.loc[sample_ids] = cnt_1d.astype('int32')\n",
    "\n",
    "    # ---- last 3 days ----\n",
    "    window_start_3d = cutoffs - three_days\n",
    "    lo_3d = np.searchsorted(ts_song_vals, window_start_3d, side='right')\n",
    "    cnt_3d = hi - lo_3d\n",
    "    songs_last_3d.loc[sample_ids] = cnt_3d.astype('int32')\n",
    "\n",
    "print(\"songs_last_1d / 3d computed\")\n",
    "print(songs_last_1d.head())\n",
    "print(songs_last_3d.head())"
   ],
   "id": "17f63a2a4d1d171c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "songs_last_1d / 3d computed\n",
      "sample_id\n",
      "1000083_2018-10-02    11\n",
      "1000164_2018-10-02     1\n",
      "1000280_2018-10-02    24\n",
      "1000353_2018-10-02    75\n",
      "1000407_2018-10-02     3\n",
      "dtype: int32\n",
      "sample_id\n",
      "1000083_2018-10-02    11\n",
      "1000164_2018-10-02     1\n",
      "1000280_2018-10-02    24\n",
      "1000353_2018-10-02    75\n",
      "1000407_2018-10-02     3\n",
      "dtype: int32\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.12 Level-at-cutoff",
   "id": "6a9157dcd23b2b39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:30.977385Z",
     "start_time": "2025-12-19T00:06:27.545847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "level_at_cutoff = pd.Series(\"unknown\", index=sliding_labels.index, dtype=object)\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals   = df_u['ts'].values\n",
    "    lvl_vals  = df_u['level'].astype(str).values   # level sequence (free/paid)\n",
    "    \n",
    "    cutoffs    = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "    \n",
    "    # hi[j] = number of events <= cutoff\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "    \n",
    "    # If hi[j] == 0, user has no event before cutoff -> unknown\n",
    "    # Otherwise, the last level is lvl_vals[hi[j] - 1]\n",
    "    lvl_for_samples = []\n",
    "    for j, h in enumerate(hi):\n",
    "        if h == 0:\n",
    "            lvl_for_samples.append(\"unknown\")\n",
    "        else:\n",
    "            lvl_for_samples.append(lvl_vals[h-1])\n",
    "    \n",
    "    level_at_cutoff.loc[sample_ids] = lvl_for_samples\n",
    "    \n",
    "print(\"Level-at-cutoff computed\")\n",
    "print(level_at_cutoff.head())"
   ],
   "id": "999cde234e3e2d15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level-at-cutoff computed\n",
      "sample_id\n",
      "1000083_2018-10-02    free\n",
      "1000164_2018-10-02    free\n",
      "1000280_2018-10-02    free\n",
      "1000353_2018-10-02    paid\n",
      "1000407_2018-10-02    free\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:30.983626Z",
     "start_time": "2025-12-19T00:06:30.978177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Write back to sliding_labels for later one-hot encoding\n",
    "sliding_labels['level'] = level_at_cutoff"
   ],
   "id": "d2f9baf468571910",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.13 Combining features",
   "id": "36530863bc303827"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:31.042648Z",
     "start_time": "2025-12-19T00:06:30.984316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Empty feature table; ensure X_all index is sample_id\n",
    "X_all = pd.DataFrame(index=sliding_labels.index)\n",
    "\n",
    "# 1) Lifetime features\n",
    "X_all['days_since_registration'] = days_since_registration\n",
    "\n",
    "# 2) Cumulative event count\n",
    "X_all['n_events'] = n_events.astype('int32')\n",
    "\n",
    "# 3) Time since last event (hours)\n",
    "X_all['recency_hours'] = recency_hours.astype('float32')\n",
    "\n",
    "# 4) Events / songs in last 7 days\n",
    "X_all['events_last_7d'] = events_last_7d.astype('int32')\n",
    "X_all['songs_last_7d']  = songs_last_7d.astype('int32')\n",
    "\n",
    "# 5) Active days (cumulative)\n",
    "X_all['active_days'] = active_days.astype('int32')\n",
    "\n",
    "# 6) Total listening time (cumulative)\n",
    "X_all['total_listen_time'] = total_listen_time.astype('float32')\n",
    "\n",
    "# 7) Events in last 1 day / 3 days\n",
    "X_all['events_last_1d'] = events_last_1d.astype('int32')\n",
    "X_all['events_last_3d'] = events_last_3d.astype('int32')\n",
    "\n",
    "# 8) Songs in last 1 day / 3 days\n",
    "X_all['songs_last_1d'] = songs_last_1d.astype('int32')\n",
    "X_all['songs_last_3d'] = songs_last_3d.astype('int32')\n",
    "\n",
    "print(\"Numeric features merged into X_all\")\n",
    "print(\"X_all shape:\", X_all.shape)\n",
    "print(X_all.head())\n",
    "print(\"\\nX_all dtypes:\")\n",
    "print(X_all.dtypes)"
   ],
   "id": "37b36a5c7229a8ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features merged into X_all\n",
      "X_all shape: (556930, 11)\n",
      "                    days_since_registration  n_events  recency_hours  \\\n",
      "sample_id                                                              \n",
      "1000083_2018-10-02                24.248739        14      12.570556   \n",
      "1000164_2018-10-02                50.602768         2       6.302222   \n",
      "1000280_2018-10-02                34.345612        37       1.177222   \n",
      "1000353_2018-10-02               152.515518        97      18.474167   \n",
      "1000407_2018-10-02                12.492084         5      13.384167   \n",
      "\n",
      "                    events_last_7d  songs_last_7d  active_days  \\\n",
      "sample_id                                                        \n",
      "1000083_2018-10-02              14             11            2   \n",
      "1000164_2018-10-02               2              1            1   \n",
      "1000280_2018-10-02              37             24            1   \n",
      "1000353_2018-10-02              97             75            2   \n",
      "1000407_2018-10-02               5              3            2   \n",
      "\n",
      "                    total_listen_time  events_last_1d  events_last_3d  \\\n",
      "sample_id                                                               \n",
      "1000083_2018-10-02        2312.406494              14              14   \n",
      "1000164_2018-10-02         227.421585               2               2   \n",
      "1000280_2018-10-02        6200.230957              37              37   \n",
      "1000353_2018-10-02       18886.052734              97              97   \n",
      "1000407_2018-10-02         804.988037               5               5   \n",
      "\n",
      "                    songs_last_1d  songs_last_3d  \n",
      "sample_id                                         \n",
      "1000083_2018-10-02             11             11  \n",
      "1000164_2018-10-02              1              1  \n",
      "1000280_2018-10-02             24             24  \n",
      "1000353_2018-10-02             75             75  \n",
      "1000407_2018-10-02              3              3  \n",
      "\n",
      "X_all dtypes:\n",
      "days_since_registration    float32\n",
      "n_events                     int32\n",
      "recency_hours              float32\n",
      "events_last_7d               int32\n",
      "songs_last_7d                int32\n",
      "active_days                  int32\n",
      "total_listen_time          float32\n",
      "events_last_1d               int32\n",
      "events_last_3d               int32\n",
      "songs_last_1d                int32\n",
      "songs_last_3d                int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.14 One-hot",
   "id": "e25d6ac7f4ad5bd4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:31.627525Z",
     "start_time": "2025-12-19T00:06:31.043830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# One-hot encode categorical features directly from sliding_labels\n",
    "cat_cols = ['gender', 'state', 'level']\n",
    "sliding_labels[cat_cols] = sliding_labels[cat_cols].fillna(\"missing\")\n",
    "\n",
    "cat_ohe = pd.get_dummies(\n",
    "    sliding_labels[cat_cols],\n",
    "    columns=cat_cols,\n",
    "    prefix=cat_cols\n",
    ")\n",
    "\n",
    "print(\"One-hot categorical feature example:\")\n",
    "print(cat_ohe.head())\n",
    "\n",
    "# Merge numeric + categorical\n",
    "X_train_raw = pd.concat([X_all, cat_ohe], axis=1)\n",
    "\n",
    "print(\"X_train shape:\", X_train_raw.shape)\n",
    "print(\"X_train example:\")\n",
    "print(X_train_raw.head())\n",
    "\n",
    "# y_train\n",
    "y_train_raw = sliding_labels['target']"
   ],
   "id": "167298278f1f8875",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot categorical feature example:\n",
      "                    gender_F  gender_M  state_AK  state_AL  state_AR  \\\n",
      "sample_id                                                              \n",
      "1000083_2018-10-02     False      True     False     False     False   \n",
      "1000164_2018-10-02      True     False     False     False     False   \n",
      "1000280_2018-10-02     False      True     False     False     False   \n",
      "1000353_2018-10-02      True     False     False     False     False   \n",
      "1000407_2018-10-02     False      True     False     False     False   \n",
      "\n",
      "                    state_AZ  state_CA  state_CO  state_CT  state_DE  ...  \\\n",
      "sample_id                                                             ...   \n",
      "1000083_2018-10-02     False     False     False     False     False  ...   \n",
      "1000164_2018-10-02      True     False     False     False     False  ...   \n",
      "1000280_2018-10-02     False     False     False     False     False  ...   \n",
      "1000353_2018-10-02     False     False     False     False     False  ...   \n",
      "1000407_2018-10-02     False     False      True     False     False  ...   \n",
      "\n",
      "                    state_TX  state_UT  state_VA  state_VT  state_WA  \\\n",
      "sample_id                                                              \n",
      "1000083_2018-10-02     False     False     False     False     False   \n",
      "1000164_2018-10-02     False     False     False     False     False   \n",
      "1000280_2018-10-02     False     False     False     False     False   \n",
      "1000353_2018-10-02      True     False     False     False     False   \n",
      "1000407_2018-10-02     False     False     False     False     False   \n",
      "\n",
      "                    state_WI  state_WV  state_WY  level_free  level_paid  \n",
      "sample_id                                                                 \n",
      "1000083_2018-10-02     False     False     False        True       False  \n",
      "1000164_2018-10-02     False     False     False        True       False  \n",
      "1000280_2018-10-02     False     False     False        True       False  \n",
      "1000353_2018-10-02     False     False     False       False        True  \n",
      "1000407_2018-10-02     False     False     False        True       False  \n",
      "\n",
      "[5 rows x 53 columns]\n",
      "X_train shape: (556930, 64)\n",
      "X_train example:\n",
      "                    days_since_registration  n_events  recency_hours  \\\n",
      "sample_id                                                              \n",
      "1000083_2018-10-02                24.248739        14      12.570556   \n",
      "1000164_2018-10-02                50.602768         2       6.302222   \n",
      "1000280_2018-10-02                34.345612        37       1.177222   \n",
      "1000353_2018-10-02               152.515518        97      18.474167   \n",
      "1000407_2018-10-02                12.492084         5      13.384167   \n",
      "\n",
      "                    events_last_7d  songs_last_7d  active_days  \\\n",
      "sample_id                                                        \n",
      "1000083_2018-10-02              14             11            2   \n",
      "1000164_2018-10-02               2              1            1   \n",
      "1000280_2018-10-02              37             24            1   \n",
      "1000353_2018-10-02              97             75            2   \n",
      "1000407_2018-10-02               5              3            2   \n",
      "\n",
      "                    total_listen_time  events_last_1d  events_last_3d  \\\n",
      "sample_id                                                               \n",
      "1000083_2018-10-02        2312.406494              14              14   \n",
      "1000164_2018-10-02         227.421585               2               2   \n",
      "1000280_2018-10-02        6200.230957              37              37   \n",
      "1000353_2018-10-02       18886.052734              97              97   \n",
      "1000407_2018-10-02         804.988037               5               5   \n",
      "\n",
      "                    songs_last_1d  ...  state_TX  state_UT  state_VA  \\\n",
      "sample_id                          ...                                 \n",
      "1000083_2018-10-02             11  ...     False     False     False   \n",
      "1000164_2018-10-02              1  ...     False     False     False   \n",
      "1000280_2018-10-02             24  ...     False     False     False   \n",
      "1000353_2018-10-02             75  ...      True     False     False   \n",
      "1000407_2018-10-02              3  ...     False     False     False   \n",
      "\n",
      "                    state_VT  state_WA  state_WI  state_WV  state_WY  \\\n",
      "sample_id                                                              \n",
      "1000083_2018-10-02     False     False     False     False     False   \n",
      "1000164_2018-10-02     False     False     False     False     False   \n",
      "1000280_2018-10-02     False     False     False     False     False   \n",
      "1000353_2018-10-02     False     False     False     False     False   \n",
      "1000407_2018-10-02     False     False     False     False     False   \n",
      "\n",
      "                    level_free  level_paid  \n",
      "sample_id                                   \n",
      "1000083_2018-10-02        True       False  \n",
      "1000164_2018-10-02        True       False  \n",
      "1000280_2018-10-02        True       False  \n",
      "1000353_2018-10-02       False        True  \n",
      "1000407_2018-10-02        True       False  \n",
      "\n",
      "[5 rows x 64 columns]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.15 drop_duplicates",
   "id": "cfb8419dbea6d48b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:31.971944Z",
     "start_time": "2025-12-19T00:06:31.628555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_for_dedup = X_train_raw.copy()\n",
    "train_for_dedup['target'] = y_train_raw.values\n",
    "\n",
    "train_dedup = train_for_dedup.drop_duplicates()\n",
    "\n",
    "X_train = train_dedup.drop(columns=['target'])\n",
    "y_train = train_dedup['target']"
   ],
   "id": "55d01a89c25d02ac",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:31.986773Z",
     "start_time": "2025-12-19T00:06:31.972954Z"
    }
   },
   "cell_type": "code",
   "source": "X_train.head(100)",
   "id": "ded5cb0b7713af29",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    days_since_registration  n_events  recency_hours  \\\n",
       "sample_id                                                              \n",
       "1000083_2018-10-02                24.248739        14      12.570556   \n",
       "1000164_2018-10-02                50.602768         2       6.302222   \n",
       "1000280_2018-10-02                34.345612        37       1.177222   \n",
       "1000353_2018-10-02               152.515518        97      18.474167   \n",
       "1000407_2018-10-02                12.492084         5      13.384167   \n",
       "...                                     ...       ...            ...   \n",
       "1014528_2018-10-02                23.091064        21      18.725555   \n",
       "1015404_2018-10-02                81.659561        17       4.081944   \n",
       "1015508_2018-10-02                 3.146308        16      12.596945   \n",
       "1015557_2018-10-02                23.717754        80      18.695278   \n",
       "1015607_2018-10-02                14.510312        10       7.958333   \n",
       "\n",
       "                    events_last_7d  songs_last_7d  active_days  \\\n",
       "sample_id                                                        \n",
       "1000083_2018-10-02              14             11            2   \n",
       "1000164_2018-10-02               2              1            1   \n",
       "1000280_2018-10-02              37             24            1   \n",
       "1000353_2018-10-02              97             75            2   \n",
       "1000407_2018-10-02               5              3            2   \n",
       "...                            ...            ...          ...   \n",
       "1014528_2018-10-02              21             16            1   \n",
       "1015404_2018-10-02              17             10            1   \n",
       "1015508_2018-10-02              16             13            2   \n",
       "1015557_2018-10-02              80             62            2   \n",
       "1015607_2018-10-02              10              7            2   \n",
       "\n",
       "                    total_listen_time  events_last_1d  events_last_3d  \\\n",
       "sample_id                                                               \n",
       "1000083_2018-10-02        2312.406494              14              14   \n",
       "1000164_2018-10-02         227.421585               2               2   \n",
       "1000280_2018-10-02        6200.230957              37              37   \n",
       "1000353_2018-10-02       18886.052734              97              97   \n",
       "1000407_2018-10-02         804.988037               5               5   \n",
       "...                               ...             ...             ...   \n",
       "1014528_2018-10-02        3484.074463              21              21   \n",
       "1015404_2018-10-02        2076.051025              17              17   \n",
       "1015508_2018-10-02        3862.067627              16              16   \n",
       "1015557_2018-10-02       14330.617188              80              80   \n",
       "1015607_2018-10-02        1935.800903              10              10   \n",
       "\n",
       "                    songs_last_1d  ...  state_TX  state_UT  state_VA  \\\n",
       "sample_id                          ...                                 \n",
       "1000083_2018-10-02             11  ...     False     False     False   \n",
       "1000164_2018-10-02              1  ...     False     False     False   \n",
       "1000280_2018-10-02             24  ...     False     False     False   \n",
       "1000353_2018-10-02             75  ...      True     False     False   \n",
       "1000407_2018-10-02              3  ...     False     False     False   \n",
       "...                           ...  ...       ...       ...       ...   \n",
       "1014528_2018-10-02             16  ...     False     False     False   \n",
       "1015404_2018-10-02             10  ...     False     False     False   \n",
       "1015508_2018-10-02             13  ...     False     False     False   \n",
       "1015557_2018-10-02             62  ...     False     False     False   \n",
       "1015607_2018-10-02              7  ...     False     False     False   \n",
       "\n",
       "                    state_VT  state_WA  state_WI  state_WV  state_WY  \\\n",
       "sample_id                                                              \n",
       "1000083_2018-10-02     False     False     False     False     False   \n",
       "1000164_2018-10-02     False     False     False     False     False   \n",
       "1000280_2018-10-02     False     False     False     False     False   \n",
       "1000353_2018-10-02     False     False     False     False     False   \n",
       "1000407_2018-10-02     False     False     False     False     False   \n",
       "...                      ...       ...       ...       ...       ...   \n",
       "1014528_2018-10-02     False     False     False     False     False   \n",
       "1015404_2018-10-02     False     False     False     False     False   \n",
       "1015508_2018-10-02     False     False     False     False     False   \n",
       "1015557_2018-10-02     False     False     False     False     False   \n",
       "1015607_2018-10-02     False     False     False     False     False   \n",
       "\n",
       "                    level_free  level_paid  \n",
       "sample_id                                   \n",
       "1000083_2018-10-02        True       False  \n",
       "1000164_2018-10-02        True       False  \n",
       "1000280_2018-10-02        True       False  \n",
       "1000353_2018-10-02       False        True  \n",
       "1000407_2018-10-02        True       False  \n",
       "...                        ...         ...  \n",
       "1014528_2018-10-02       False        True  \n",
       "1015404_2018-10-02        True       False  \n",
       "1015508_2018-10-02        True       False  \n",
       "1015557_2018-10-02        True       False  \n",
       "1015607_2018-10-02        True       False  \n",
       "\n",
       "[100 rows x 64 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>days_since_registration</th>\n",
       "      <th>n_events</th>\n",
       "      <th>recency_hours</th>\n",
       "      <th>events_last_7d</th>\n",
       "      <th>songs_last_7d</th>\n",
       "      <th>active_days</th>\n",
       "      <th>total_listen_time</th>\n",
       "      <th>events_last_1d</th>\n",
       "      <th>events_last_3d</th>\n",
       "      <th>songs_last_1d</th>\n",
       "      <th>...</th>\n",
       "      <th>state_TX</th>\n",
       "      <th>state_UT</th>\n",
       "      <th>state_VA</th>\n",
       "      <th>state_VT</th>\n",
       "      <th>state_WA</th>\n",
       "      <th>state_WI</th>\n",
       "      <th>state_WV</th>\n",
       "      <th>state_WY</th>\n",
       "      <th>level_free</th>\n",
       "      <th>level_paid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000083_2018-10-02</th>\n",
       "      <td>24.248739</td>\n",
       "      <td>14</td>\n",
       "      <td>12.570556</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2312.406494</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000164_2018-10-02</th>\n",
       "      <td>50.602768</td>\n",
       "      <td>2</td>\n",
       "      <td>6.302222</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>227.421585</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000280_2018-10-02</th>\n",
       "      <td>34.345612</td>\n",
       "      <td>37</td>\n",
       "      <td>1.177222</td>\n",
       "      <td>37</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>6200.230957</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000353_2018-10-02</th>\n",
       "      <td>152.515518</td>\n",
       "      <td>97</td>\n",
       "      <td>18.474167</td>\n",
       "      <td>97</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>18886.052734</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000407_2018-10-02</th>\n",
       "      <td>12.492084</td>\n",
       "      <td>5</td>\n",
       "      <td>13.384167</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>804.988037</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014528_2018-10-02</th>\n",
       "      <td>23.091064</td>\n",
       "      <td>21</td>\n",
       "      <td>18.725555</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>3484.074463</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015404_2018-10-02</th>\n",
       "      <td>81.659561</td>\n",
       "      <td>17</td>\n",
       "      <td>4.081944</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2076.051025</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015508_2018-10-02</th>\n",
       "      <td>3.146308</td>\n",
       "      <td>16</td>\n",
       "      <td>12.596945</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>3862.067627</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015557_2018-10-02</th>\n",
       "      <td>23.717754</td>\n",
       "      <td>80</td>\n",
       "      <td>18.695278</td>\n",
       "      <td>80</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>14330.617188</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015607_2018-10-02</th>\n",
       "      <td>14.510312</td>\n",
       "      <td>10</td>\n",
       "      <td>7.958333</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1935.800903</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  64 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:31.991330Z",
     "start_time": "2025-12-19T00:06:31.987791Z"
    }
   },
   "cell_type": "code",
   "source": "y_train.head(100)",
   "id": "ec25cb5c29018c90",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_id\n",
       "1000083_2018-10-02    0\n",
       "1000164_2018-10-02    0\n",
       "1000280_2018-10-02    0\n",
       "1000353_2018-10-02    0\n",
       "1000407_2018-10-02    0\n",
       "                     ..\n",
       "1014528_2018-10-02    0\n",
       "1015404_2018-10-02    0\n",
       "1015508_2018-10-02    1\n",
       "1015557_2018-10-02    0\n",
       "1015607_2018-10-02    0\n",
       "Name: target, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Test set features",
   "id": "f33aad0bb094dbd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:39.238895Z",
     "start_time": "2025-12-19T00:06:31.992838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4.1 Sort by userId + ts and group\n",
    "test_df_sorted = test_df.sort_values(['userId', 'ts']).copy()\n",
    "test_groups = dict(tuple(test_df_sorted.groupby('userId')))\n",
    "test_users = sorted(test_groups.keys())\n",
    "\n",
    "print(\"Number of test users:\", len(test_users))\n",
    "\n",
    "# 4.2 Global cutoff (observation end): last day in test\n",
    "global_cutoff_test = test_df_sorted['ts'].max().normalize()\n",
    "print(\"Test global cutoff_date:\", global_cutoff_test)\n",
    "\n",
    "# 4.3 Initialize test feature table: one row per user\n",
    "X_test = pd.DataFrame(index=test_users)\n"
   ],
   "id": "4a019e4b4d792b2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test users: 2904\n",
      "Test global cutoff_date: 2018-11-20 00:00:00\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.1 Lifetime",
   "id": "d6dcc08c8dd26a03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:39.395572Z",
     "start_time": "2025-12-19T00:06:39.242739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Registration time per user (from test)\n",
    "uid_registration_test = (\n",
    "    test_df_sorted\n",
    "    .groupby('userId')['registration']\n",
    "    .first()\n",
    ")\n",
    "\n",
    "# Days from registration to cutoff\n",
    "days_since_registration_test = (\n",
    "    (global_cutoff_test - uid_registration_test) / np.timedelta64(1, 'D')\n",
    ").astype('float32')\n",
    "days_since_registration_test = days_since_registration_test.clip(lower=0)\n",
    "\n",
    "X_test['days_since_registration'] = days_since_registration_test.reindex(test_users)\n",
    "\n",
    "print(\"days_since_registration_test example:\")\n",
    "print(X_test['days_since_registration'].head())\n"
   ],
   "id": "852f50ca43a2d034",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days_since_registration_test example:\n",
      "1000655    66.504227\n",
      "1000963    73.911598\n",
      "1001129    86.574089\n",
      "1001963    40.715820\n",
      "1002283    54.104652\n",
      "Name: days_since_registration, dtype: float32\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2 n_events / recency_hours / active_days",
   "id": "97c46d7754934277"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:40.943345Z",
     "start_time": "2025-12-19T00:06:39.396252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_events_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "recency_hours_test = pd.Series(0.0, index=test_users, dtype='float32')\n",
    "active_days_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    # Only consider events before cutoff\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    \n",
    "    if df_before.empty:\n",
    "        n_events_test.loc[uid] = 0\n",
    "        recency_hours_test.loc[uid] = 9999.0\n",
    "        active_days_test.loc[uid] = 0\n",
    "        continue\n",
    "    \n",
    "    # Total event count\n",
    "    n_events_test.loc[uid] = len(df_before)\n",
    "    \n",
    "    # recency_hours: cutoff - last event time\n",
    "    last_ts = df_before['ts'].iloc[-1]\n",
    "    delta_h = (global_cutoff_test - last_ts) / np.timedelta64(1, 'h')\n",
    "    recency_hours_test.loc[uid] = float(delta_h)\n",
    "    \n",
    "    # Active days: number of unique dates\n",
    "    active_days_test.loc[uid] = df_before['ts'].dt.normalize().nunique()\n",
    "\n",
    "X_test['n_events'] = n_events_test\n",
    "X_test['recency_hours'] = recency_hours_test\n",
    "X_test['active_days'] = active_days_test\n",
    "\n",
    "print(\"n_events / recency_hours / active_days example:\")\n",
    "print(X_test[['n_events', 'recency_hours', 'active_days']].head())\n"
   ],
   "id": "882c5d908fc6fc5e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/xtlxv68s5sv1j3rj3ww3zh_h0000gn/T/ipykernel_97100/213022006.py:21: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '101.7525' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  recency_hours_test.loc[uid] = float(delta_h)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_events / recency_hours / active_days example:\n",
      "         n_events  recency_hours  active_days\n",
      "1000655       346     101.752500           11\n",
      "1000963      2539      47.861389           26\n",
      "1001129       668      25.063889           10\n",
      "1001963       718       3.341389           16\n",
      "1002283      3837       5.711667           25\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.3 recent 7 / 3 / 1 behavior",
   "id": "230b1c2618f72166"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:42.218545Z",
     "start_time": "2025-12-19T00:06:40.945531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "events_last_7d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "events_last_3d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "events_last_1d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "\n",
    "seven_days  = np.timedelta64(7, 'D')\n",
    "three_days  = np.timedelta64(3, 'D')\n",
    "one_day     = np.timedelta64(1, 'D')\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    \n",
    "    ts_vals = df_before['ts'].values\n",
    "    \n",
    "    # Last 7 days\n",
    "    mask_7 = ts_vals > (global_cutoff_test - seven_days)\n",
    "    events_last_7d_test.loc[uid] = int(mask_7.sum())\n",
    "    \n",
    "    # Last 3 days\n",
    "    mask_3 = ts_vals > (global_cutoff_test - three_days)\n",
    "    events_last_3d_test.loc[uid] = int(mask_3.sum())\n",
    "    \n",
    "    # Last 1 day\n",
    "    mask_1 = ts_vals > (global_cutoff_test - one_day)\n",
    "    events_last_1d_test.loc[uid] = int(mask_1.sum())\n",
    "\n",
    "X_test['events_last_7d'] = events_last_7d_test\n",
    "X_test['events_last_3d'] = events_last_3d_test\n",
    "X_test['events_last_1d'] = events_last_1d_test\n",
    "\n",
    "print(\"events_last_*_test example:\")\n",
    "print(X_test[['events_last_7d', 'events_last_3d', 'events_last_1d']].head())\n"
   ],
   "id": "242bcc84d8668e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events_last_*_test example:\n",
      "         events_last_7d  events_last_3d  events_last_1d\n",
      "1000655              33               0               0\n",
      "1000963             472             259               0\n",
      "1001129              62               2               0\n",
      "1001963             326             159             159\n",
      "1002283             303              89              89\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.4 recent 7 / 3 / 1 songs",
   "id": "6ad6114a55579720"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:44.497101Z",
     "start_time": "2025-12-19T00:06:42.219456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "songs_last_7d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "songs_last_3d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "songs_last_1d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "total_listen_time_test = pd.Series(0.0, index=test_users, dtype='float32')\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    \n",
    "    df_song = df_before[df_before['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        continue\n",
    "    \n",
    "    ts_song = df_song['ts'].values\n",
    "    len_song = df_song['length'].values\n",
    "    \n",
    "    # Total listening time (all songs <= cutoff)\n",
    "    total_listen_time_test.loc[uid] = float(len_song.sum())\n",
    "    \n",
    "    # Songs in last 7 days\n",
    "    mask_7 = ts_song > (global_cutoff_test - seven_days)\n",
    "    songs_last_7d_test.loc[uid] = int(mask_7.sum())\n",
    "    \n",
    "    # Last 3 days\n",
    "    mask_3 = ts_song > (global_cutoff_test - three_days)\n",
    "    songs_last_3d_test.loc[uid] = int(mask_3.sum())\n",
    "    \n",
    "    # Last 1 day\n",
    "    mask_1 = ts_song > (global_cutoff_test - one_day)\n",
    "    songs_last_1d_test.loc[uid] = int(mask_1.sum())\n",
    "\n",
    "X_test['songs_last_7d']     = songs_last_7d_test\n",
    "X_test['songs_last_3d']     = songs_last_3d_test\n",
    "X_test['songs_last_1d']     = songs_last_1d_test\n",
    "X_test['total_listen_time'] = total_listen_time_test\n",
    "\n",
    "print(\"songs_last_*_test & total_listen_time_test example:\")\n",
    "print(X_test[['songs_last_7d', 'songs_last_3d', 'songs_last_1d', 'total_listen_time']].head())\n"
   ],
   "id": "c675970f98d0a3a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/xtlxv68s5sv1j3rj3ww3zh_h0000gn/T/ipykernel_97100/249041160.py:19: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '65479.8747' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  total_listen_time_test.loc[uid] = float(len_song.sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "songs_last_*_test & total_listen_time_test example:\n",
      "         songs_last_7d  songs_last_3d  songs_last_1d  total_listen_time\n",
      "1000655             24              0              0        65479.87470\n",
      "1000963            402            221              0       526127.55185\n",
      "1001129             46              1              0       139026.22059\n",
      "1001963            250            123            123       134683.68053\n",
      "1002283            250             77             77       789821.80947\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.5 Level-at-cutoff",
   "id": "dbef6a7b6f0f4ea6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:45.494881Z",
     "start_time": "2025-12-19T00:06:44.502804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "level_at_cutoff_test = pd.Series(\"unknown\", index=test_users, dtype=object)\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    level_at_cutoff_test.loc[uid] = str(df_before['level'].iloc[-1])\n",
    "\n",
    "print(\"level_at_cutoff_test example:\")\n",
    "print(level_at_cutoff_test.head())\n"
   ],
   "id": "1a410c0cdfe39159",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level_at_cutoff_test example:\n",
      "1000655    free\n",
      "1000963    paid\n",
      "1001129    free\n",
      "1001963    free\n",
      "1002283    paid\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.6 Category + One-Hot",
   "id": "299ceb5fcf275c79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:45.893234Z",
     "start_time": "2025-12-19T00:06:45.495541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Static gender/state in test\n",
    "test_user_static = (\n",
    "    test_df_sorted\n",
    "    .groupby('userId')\n",
    "    .agg({\n",
    "        'gender': 'first',\n",
    "        'state':  'first',\n",
    "    })\n",
    ")\n",
    "\n",
    "cat_test = pd.DataFrame(index=test_users)\n",
    "cat_test['gender'] = test_user_static['gender']\n",
    "cat_test['state']  = test_user_static['state']\n",
    "cat_test['level']  = level_at_cutoff_test\n",
    "\n",
    "cat_test = cat_test.fillna(\"missing\")\n",
    "\n",
    "cat_test_ohe = pd.get_dummies(\n",
    "    cat_test,\n",
    "    columns=['gender', 'state', 'level'],\n",
    "    prefix=['gender', 'state', 'level']\n",
    ")\n",
    "\n",
    "print(\"Test categorical one-hot example:\")\n",
    "print(cat_test_ohe.head())\n",
    "\n",
    "# Align to train categorical columns\n",
    "cat_test_ohe = cat_test_ohe.reindex(columns=cat_ohe.columns, fill_value=0)\n",
    "\n",
    "print(\"Aligned cat_test_ohe shape:\", cat_test_ohe.shape)"
   ],
   "id": "b02b03625ba64852",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test categorical one-hot example:\n",
      "         gender_F  gender_M  gender_missing  state_AK  state_AL  state_AR  \\\n",
      "1000655      True     False           False     False     False     False   \n",
      "1000963     False      True           False     False     False     False   \n",
      "1001129     False      True           False     False     False     False   \n",
      "1001963      True     False           False     False     False     False   \n",
      "1002283     False      True           False     False     False     False   \n",
      "\n",
      "         state_AZ  state_CA  state_CO  state_CT  ...  state_UT  state_VA  \\\n",
      "1000655     False     False     False     False  ...     False      True   \n",
      "1000963     False     False     False     False  ...     False     False   \n",
      "1001129     False     False     False     False  ...     False     False   \n",
      "1001963     False     False     False     False  ...     False     False   \n",
      "1002283     False     False     False     False  ...     False     False   \n",
      "\n",
      "         state_VT  state_WA  state_WI  state_WV  state_WY  state_ne  \\\n",
      "1000655     False     False     False     False     False     False   \n",
      "1000963     False     False     False     False     False     False   \n",
      "1001129     False     False     False      True     False     False   \n",
      "1001963     False     False     False     False     False     False   \n",
      "1002283     False     False     False     False     False     False   \n",
      "\n",
      "         level_free  level_paid  \n",
      "1000655        True       False  \n",
      "1000963       False        True  \n",
      "1001129        True       False  \n",
      "1001963        True       False  \n",
      "1002283       False        True  \n",
      "\n",
      "[5 rows x 55 columns]\n",
      "Aligned cat_test_ohe shape: (2904, 53)\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.7 Combining features",
   "id": "7e1f5a6ce09e4ad3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:45.900415Z",
     "start_time": "2025-12-19T00:06:45.894231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Final test features (align column order)\n",
    "X_test_full = pd.concat([X_test, cat_test_ohe], axis=1)\n",
    "X_test_full = X_test_full.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "print(\"X_test_full shape:\", X_test_full.shape)\n",
    "print(X_test_full.head())"
   ],
   "id": "7558acbe8790f169",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_full shape: (2904, 64)\n",
      "         days_since_registration  n_events  recency_hours  events_last_7d  \\\n",
      "1000655                66.504227       346     101.752500              33   \n",
      "1000963                73.911598      2539      47.861389             472   \n",
      "1001129                86.574089       668      25.063889              62   \n",
      "1001963                40.715820       718       3.341389             326   \n",
      "1002283                54.104652      3837       5.711667             303   \n",
      "\n",
      "         songs_last_7d  active_days  total_listen_time  events_last_1d  \\\n",
      "1000655             24           11        65479.87470               0   \n",
      "1000963            402           26       526127.55185               0   \n",
      "1001129             46           10       139026.22059               0   \n",
      "1001963            250           16       134683.68053             159   \n",
      "1002283            250           25       789821.80947              89   \n",
      "\n",
      "         events_last_3d  songs_last_1d  ...  state_TX  state_UT  state_VA  \\\n",
      "1000655               0              0  ...     False     False      True   \n",
      "1000963             259              0  ...     False     False     False   \n",
      "1001129               2              0  ...     False     False     False   \n",
      "1001963             159            123  ...     False     False     False   \n",
      "1002283              89             77  ...     False     False     False   \n",
      "\n",
      "         state_VT  state_WA  state_WI  state_WV  state_WY  level_free  \\\n",
      "1000655     False     False     False     False     False        True   \n",
      "1000963     False     False     False     False     False       False   \n",
      "1001129     False     False     False      True     False        True   \n",
      "1001963     False     False     False     False     False        True   \n",
      "1002283     False     False     False     False     False       False   \n",
      "\n",
      "         level_paid  \n",
      "1000655       False  \n",
      "1000963        True  \n",
      "1001129       False  \n",
      "1001963       False  \n",
      "1002283        True  \n",
      "\n",
      "[5 rows x 64 columns]\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:45.908935Z",
     "start_time": "2025-12-19T00:06:45.901363Z"
    }
   },
   "cell_type": "code",
   "source": "X_test_full.head(100)",
   "id": "5fc4481123e44284",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         days_since_registration  n_events  recency_hours  events_last_7d  \\\n",
       "1000655                66.504227       346     101.752500              33   \n",
       "1000963                73.911598      2539      47.861389             472   \n",
       "1001129                86.574089       668      25.063889              62   \n",
       "1001963                40.715820       718       3.341389             326   \n",
       "1002283                54.104652      3837       5.711667             303   \n",
       "...                          ...       ...            ...             ...   \n",
       "1037874                56.995277      1614      80.578333             224   \n",
       "1038109                59.933205      2437     174.817222               0   \n",
       "1038258                67.642838      1043     104.052778              60   \n",
       "1038741                79.969872        39     656.347500               0   \n",
       "1038830                83.776566       583     218.366944               0   \n",
       "\n",
       "         songs_last_7d  active_days  total_listen_time  events_last_1d  \\\n",
       "1000655             24           11        65479.87470               0   \n",
       "1000963            402           26       526127.55185               0   \n",
       "1001129             46           10       139026.22059               0   \n",
       "1001963            250           16       134683.68053             159   \n",
       "1002283            250           25       789821.80947              89   \n",
       "...                ...          ...                ...             ...   \n",
       "1037874            155           22       317178.30474               0   \n",
       "1038109              0           15       516133.09474               0   \n",
       "1038258             50           13       210747.90598               0   \n",
       "1038741              0            1         5773.78177               0   \n",
       "1038830              0           12       108830.09854               0   \n",
       "\n",
       "         events_last_3d  songs_last_1d  ...  state_TX  state_UT  state_VA  \\\n",
       "1000655               0              0  ...     False     False      True   \n",
       "1000963             259              0  ...     False     False     False   \n",
       "1001129               2              0  ...     False     False     False   \n",
       "1001963             159            123  ...     False     False     False   \n",
       "1002283              89             77  ...     False     False     False   \n",
       "...                 ...            ...  ...       ...       ...       ...   \n",
       "1037874               0              0  ...     False     False     False   \n",
       "1038109               0              0  ...     False     False     False   \n",
       "1038258               0              0  ...      True     False     False   \n",
       "1038741               0              0  ...     False     False     False   \n",
       "1038830               0              0  ...     False     False     False   \n",
       "\n",
       "         state_VT  state_WA  state_WI  state_WV  state_WY  level_free  \\\n",
       "1000655     False     False     False     False     False        True   \n",
       "1000963     False     False     False     False     False       False   \n",
       "1001129     False     False     False      True     False        True   \n",
       "1001963     False     False     False     False     False        True   \n",
       "1002283     False     False     False     False     False       False   \n",
       "...           ...       ...       ...       ...       ...         ...   \n",
       "1037874     False     False     False     False     False        True   \n",
       "1038109     False     False     False     False     False       False   \n",
       "1038258     False     False     False     False     False       False   \n",
       "1038741     False     False     False     False     False        True   \n",
       "1038830     False     False      True     False     False       False   \n",
       "\n",
       "         level_paid  \n",
       "1000655       False  \n",
       "1000963        True  \n",
       "1001129       False  \n",
       "1001963       False  \n",
       "1002283        True  \n",
       "...             ...  \n",
       "1037874       False  \n",
       "1038109        True  \n",
       "1038258        True  \n",
       "1038741       False  \n",
       "1038830        True  \n",
       "\n",
       "[100 rows x 64 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>days_since_registration</th>\n",
       "      <th>n_events</th>\n",
       "      <th>recency_hours</th>\n",
       "      <th>events_last_7d</th>\n",
       "      <th>songs_last_7d</th>\n",
       "      <th>active_days</th>\n",
       "      <th>total_listen_time</th>\n",
       "      <th>events_last_1d</th>\n",
       "      <th>events_last_3d</th>\n",
       "      <th>songs_last_1d</th>\n",
       "      <th>...</th>\n",
       "      <th>state_TX</th>\n",
       "      <th>state_UT</th>\n",
       "      <th>state_VA</th>\n",
       "      <th>state_VT</th>\n",
       "      <th>state_WA</th>\n",
       "      <th>state_WI</th>\n",
       "      <th>state_WV</th>\n",
       "      <th>state_WY</th>\n",
       "      <th>level_free</th>\n",
       "      <th>level_paid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000655</th>\n",
       "      <td>66.504227</td>\n",
       "      <td>346</td>\n",
       "      <td>101.752500</td>\n",
       "      <td>33</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>65479.87470</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000963</th>\n",
       "      <td>73.911598</td>\n",
       "      <td>2539</td>\n",
       "      <td>47.861389</td>\n",
       "      <td>472</td>\n",
       "      <td>402</td>\n",
       "      <td>26</td>\n",
       "      <td>526127.55185</td>\n",
       "      <td>0</td>\n",
       "      <td>259</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001129</th>\n",
       "      <td>86.574089</td>\n",
       "      <td>668</td>\n",
       "      <td>25.063889</td>\n",
       "      <td>62</td>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>139026.22059</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001963</th>\n",
       "      <td>40.715820</td>\n",
       "      <td>718</td>\n",
       "      <td>3.341389</td>\n",
       "      <td>326</td>\n",
       "      <td>250</td>\n",
       "      <td>16</td>\n",
       "      <td>134683.68053</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>123</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002283</th>\n",
       "      <td>54.104652</td>\n",
       "      <td>3837</td>\n",
       "      <td>5.711667</td>\n",
       "      <td>303</td>\n",
       "      <td>250</td>\n",
       "      <td>25</td>\n",
       "      <td>789821.80947</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037874</th>\n",
       "      <td>56.995277</td>\n",
       "      <td>1614</td>\n",
       "      <td>80.578333</td>\n",
       "      <td>224</td>\n",
       "      <td>155</td>\n",
       "      <td>22</td>\n",
       "      <td>317178.30474</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038109</th>\n",
       "      <td>59.933205</td>\n",
       "      <td>2437</td>\n",
       "      <td>174.817222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>516133.09474</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038258</th>\n",
       "      <td>67.642838</td>\n",
       "      <td>1043</td>\n",
       "      <td>104.052778</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>13</td>\n",
       "      <td>210747.90598</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038741</th>\n",
       "      <td>79.969872</td>\n",
       "      <td>39</td>\n",
       "      <td>656.347500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5773.78177</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038830</th>\n",
       "      <td>83.776566</td>\n",
       "      <td>583</td>\n",
       "      <td>218.366944</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>108830.09854</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  64 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Models and submission",
   "id": "2c027ac5fd2b52e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:45.913854Z",
     "start_time": "2025-12-19T00:06:45.909729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Simple oversampling utility\n",
    "def oversample(X, y):\n",
    "    X = X.copy()\n",
    "    X['target'] = y\n",
    "    major = X[X['target'] == 0]\n",
    "    minor = X[X['target'] == 1]\n",
    "\n",
    "    if len(minor) == 0:\n",
    "        raise ValueError(\"No positive samples\")\n",
    "\n",
    "    ratio = max(1, len(major) // len(minor))\n",
    "    minor_ov = pd.concat([minor] * ratio, ignore_index=True)\n",
    "\n",
    "    df_new = pd.concat([major, minor_ov], axis=0).sample(frac=1.0, random_state=42)\n",
    "    y_new = df_new['target'].values\n",
    "    X_new = df_new.drop(columns=['target'])\n",
    "\n",
    "    print(\"Positive rate after oversampling:\", y_new.mean())\n",
    "    return X_new, y_new"
   ],
   "id": "fa468bdfe07ae030",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.1 Light GBM (0.621)",
   "id": "d2c9e3306b7e296e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:06:59.567080Z",
     "start_time": "2025-12-19T00:06:45.914833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# 1) No oversampling\n",
    "X_lgb, y_lgb = X_train, y_train\n",
    "\n",
    "# 2) Model\n",
    "lgb_clf = lgb.LGBMClassifier(\n",
    "    n_estimators=800,\n",
    "    learning_rate=0.03,\n",
    "    # num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    # max_depth=-1,\n",
    "    random_state=42,\n",
    "    max_depth=7,          # added\n",
    "    num_leaves=32,        # added\n",
    "    min_data_in_leaf=50,  # added\n",
    "    feature_fraction=0.8, # added\n",
    "    bagging_fraction=0.8, # added\n",
    "    bagging_freq=5,       # added\n",
    ")\n",
    "\n",
    "lgb_clf.fit(X_lgb, y_lgb)\n",
    "\n",
    "# 3) Predict probabilities\n",
    "pred_lgb = lgb_clf.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "# 4) Align order with example_submission\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "user_ids = example_sub['id'].astype(str)\n",
    "\n",
    "proba_aligned = pd.Series(pred_lgb, index=X_test_full.index).loc[user_ids].values\n",
    "\n",
    "# 5) Top 50% rule\n",
    "threshold = np.quantile(proba_aligned, 0.5)\n",
    "pred_label = (proba_aligned >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": example_sub[\"id\"],\n",
    "    \"target\": pred_label\n",
    "})\n",
    "\n",
    "print(submission['target'].value_counts(normalize=True))\n",
    "submission.to_csv(\"submission_LightGBM.csv\", index=False)\n",
    "print(\"Saved submission_LightGBM.csv\")"
   ],
   "id": "f95394bef77aa4ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.8 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.8 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 30052, number of negative: 526878\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2697\n",
      "[LightGBM] [Info] Number of data points in the train set: 556930, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.053960 -> initscore=-2.864040\n",
      "[LightGBM] [Info] Start training from score -2.864040\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.8 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "target\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Saved submission_LightGBM.csv\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.2 Logistic Regression (0.624)",
   "id": "286209ddbb03adab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:07:23.632013Z",
     "start_time": "2025-12-19T00:06:59.568332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1) Oversample\n",
    "X_lr, y_lr = oversample(X_train, y_train)\n",
    "\n",
    "# 2) Standardize\n",
    "scaler_lr = StandardScaler()\n",
    "X_lr_scaled = scaler_lr.fit_transform(X_lr)\n",
    "X_test_lr_scaled = scaler_lr.transform(X_test_full)\n",
    "\n",
    "# 3) Train model\n",
    "lr_clf = LogisticRegression(\n",
    "    C=0.1,          # added: smaller than default 1.0 to reduce overfitting\n",
    "    penalty='l2',   # added\n",
    "    solver='liblinear',\n",
    "    class_weight='balanced',\n",
    "    max_iter=2000   # added\n",
    ")\n",
    "lr_clf.fit(X_lr_scaled, y_lr)\n",
    "\n",
    "# 4) Predict probabilities\n",
    "pred_lr = lr_clf.predict_proba(X_test_lr_scaled)[:, 1]\n",
    "\n",
    "# ========== Top 50% submission ==========\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "user_ids = example_sub['id'].astype(str)\n",
    "\n",
    "proba_align = pd.Series(pred_lr, index=X_test_full.index).loc[user_ids].values\n",
    "\n",
    "threshold = np.quantile(proba_align, 0.5)\n",
    "print(\"LR top50 threshold =\", threshold)\n",
    "\n",
    "pred_label = (proba_align >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": example_sub['id'],\n",
    "    \"target\": pred_label\n",
    "})\n",
    "print(submission['target'].value_counts(normalize=True))\n",
    "\n",
    "submission.to_csv(\"submission_LR.csv\", index=False)\n",
    "print(\"Saved submission_LR.csv\")"
   ],
   "id": "b789e97e6a78414",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rate after oversampling: 0.4922939941913464\n",
      "LR top50 threshold = 0.5464098247171293\n",
      "target\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Saved submission_LR.csv\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.3 ExtraTrees (0.610)",
   "id": "8c96b0d386d88272"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:09:47.657700Z",
     "start_time": "2025-12-19T00:07:23.633349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# 1) No oversampling for now\n",
    "X_et, y_et = X_train, y_train\n",
    "\n",
    "# 2) ExtraTrees model\n",
    "et_clf = ExtraTreesClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "et_clf.fit(X_et, y_et)\n",
    "\n",
    "# 3) Predict probabilities\n",
    "pred_et = et_clf.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "# ========== Top 50% ==========\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "user_ids = example_sub['id'].astype(str)\n",
    "\n",
    "proba_align = pd.Series(pred_et, index=X_test_full.index).loc[user_ids].values\n",
    "threshold = np.quantile(proba_align, 0.5)\n",
    "print(\"ET top50 threshold =\", threshold)\n",
    "\n",
    "pred_label = (proba_align >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": example_sub['id'],\n",
    "    \"target\": pred_label\n",
    "})\n",
    "print(submission['target'].value_counts(normalize=True))\n",
    "\n",
    "submission.to_csv(\"submission_ET.csv\", index=False)\n",
    "print(\"Saved submission_ET.csv\")"
   ],
   "id": "557d342fcc4774",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ET top50 threshold = 0.07022361197032842\n",
      "target\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Saved submission_ET.csv\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.4 KNN (0.56)",
   "id": "5ead72f99ca30304"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:10:05.515501Z",
     "start_time": "2025-12-19T00:09:47.672641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1) Oversample\n",
    "X_knn, y_knn = oversample(X_train, y_train)\n",
    "\n",
    "# 2) Standardize\n",
    "scaler_knn = StandardScaler()\n",
    "X_knn_scaled = scaler_knn.fit_transform(X_knn)\n",
    "X_test_knn_scaled = scaler_knn.transform(X_test_full)\n",
    "\n",
    "# 3) KNN\n",
    "knn_clf = KNeighborsClassifier(\n",
    "    n_neighbors=100,\n",
    "    weights='distance',\n",
    "    p=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "knn_clf.fit(X_knn_scaled, y_knn)\n",
    "\n",
    "# 4) Predict probabilities\n",
    "pred_knn = knn_clf.predict_proba(X_test_knn_scaled)[:, 1]\n",
    "\n",
    "# ========== Top 50% ==========\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "user_ids = example_sub['id'].astype(str)\n",
    "\n",
    "proba_align = pd.Series(pred_knn, index=X_test_full.index).loc[user_ids].values\n",
    "threshold = np.quantile(proba_align, 0.5)\n",
    "print(\"KNN top50 threshold =\", threshold)\n",
    "\n",
    "pred_label = (proba_align >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": example_sub['id'],\n",
    "    \"target\": pred_label\n",
    "})\n",
    "print(submission['target'].value_counts(normalize=True))\n",
    "\n",
    "submission.to_csv(\"submission_KNN.csv\", index=False)\n",
    "print(\"Saved submission_KNN.csv\")"
   ],
   "id": "fd98d16c1c39469d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rate after oversampling: 0.4922939941913464\n",
      "KNN top50 threshold = 0.4883727717931538\n",
      "target\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Saved submission_KNN.csv\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.5 RF (0.610)",
   "id": "582425c564f91442"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:10:38.436112Z",
     "start_time": "2025-12-19T00:10:05.517945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1) No oversampling for now\n",
    "X_rf, y_rf = X_train, y_train\n",
    "\n",
    "# 2) Train RF\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=7,  # modified\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "rf_clf.fit(X_rf, y_rf)\n",
    "\n",
    "# 3) Predict probabilities\n",
    "pred_rf = rf_clf.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "# ========== Top 50% ==========\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "user_ids = example_sub['id'].astype(str)\n",
    "\n",
    "proba_align = pd.Series(pred_rf, index=X_test_full.index).loc[user_ids].values\n",
    "threshold = np.quantile(proba_align, 0.5)\n",
    "print(\"RF top50 threshold =\", threshold)\n",
    "\n",
    "pred_label = (proba_align >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": example_sub['id'],\n",
    "    \"target\": pred_label\n",
    "})\n",
    "print(submission['target'].value_counts(normalize=True))\n",
    "\n",
    "submission.to_csv(\"submission_RF.csv\", index=False)\n",
    "print(\"Saved submission_RF.csv\")"
   ],
   "id": "c69175aeff5796a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF top50 threshold = 0.0700059917151174\n",
      "target\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Saved submission_RF.csv\n"
     ]
    }
   ],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
