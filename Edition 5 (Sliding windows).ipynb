{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "id": "3d1043bcff59b542"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Preprocess",
   "id": "987bc04e2197fd51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Read raw parquet files\n",
    "train_df = pd.read_parquet(\"train.parquet\")\n",
    "test_df = pd.read_parquet(\"test.parquet\")\n",
    "\n",
    "# 3. Convert time-related columns\n",
    "train_df['ts'] = pd.to_datetime(train_df['ts'], unit='ms')\n",
    "test_df['ts'] = pd.to_datetime(test_df['ts'], unit='ms')\n",
    "\n",
    "train_df['registration'] = pd.to_datetime(train_df['registration'])\n",
    "test_df['registration'] = pd.to_datetime(test_df['registration'])\n",
    "\n",
    "train_df['time'] = pd.to_datetime(train_df['time'])\n",
    "test_df['time'] = pd.to_datetime(test_df['time'])\n",
    "\n",
    "# 4. Clean the 'page' field (strip spaces)\n",
    "train_df['page'] = train_df['page'].astype(str).str.strip()\n",
    "test_df['page'] = test_df['page'].astype(str).str.strip()\n",
    "\n",
    "# 5. Extract state from location (last two characters) as 'state'\n",
    "#    e.g., \"New York, NY\" -> \"NY\"\n",
    "train_df['location'] = train_df['location'].astype(str).str.strip()\n",
    "test_df['location'] = test_df['location'].astype(str).str.strip()\n",
    "\n",
    "train_df['state'] = train_df['location'].str[-2:]\n",
    "test_df['state'] = test_df['location'].str[-2:]\n",
    "\n",
    "# 6. Sort by userId + ts to ensure chronological order\n",
    "train_df = train_df.sort_values(['userId', 'ts'])\n",
    "test_df = test_df.sort_values(['userId', 'ts'])\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)"
   ],
   "id": "672d792e753867fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Window label",
   "id": "7b688d3559067ca5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Set sliding-window parameters\n",
    "horizon_days = 10  # prediction window length = 10 days (competition setting)\n",
    "\n",
    "# Manually set the earliest cutoff start date\n",
    "cutoff_start = pd.to_datetime(\"2018-10-01\")\n",
    "\n",
    "# Cutoff end must satisfy: cutoff + 10 days <= train_df['ts'].max().normalize()\n",
    "# Take max date (daily), then subtract horizon_days\n",
    "max_ts_date = train_df['ts'].max().normalize()          # usually 2018-11-20\n",
    "cutoff_end = max_ts_date - pd.Timedelta(days=horizon_days + 1)\n",
    "\n",
    "print(\"cutoff_start:\", cutoff_start.date())\n",
    "print(\"max_ts_date :\", max_ts_date.date())\n",
    "print(\"cutoff_end  :\", cutoff_end.date())"
   ],
   "id": "8d72dfefa5c54d31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Generate a list of cutoffs: one cutoff per day\n",
    "cutoff_dates = pd.date_range(start=cutoff_start, end=cutoff_end, freq=\"D\")\n",
    "print(\"cutoff_dates count:\", len(cutoff_dates))\n",
    "print(\"cutoff_dates preview:\", list(cutoff_dates[:5]), \"...\", list(cutoff_dates[-5:]))"
   ],
   "id": "ad2e1bfb0aa8d033"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. For each cutoff_date, generate samples (userId, cutoff_date, target)\n",
    "\n",
    "# First compute each user's first churn timestamp (first time they hit Cancellation Confirmation)\n",
    "first_churn_ts = (\n",
    "    train_df[train_df['page'] == \"Cancellation Confirmation\"]\n",
    "    .groupby('userId')['ts']\n",
    "    .min()\n",
    ")\n",
    "\n",
    "print(\"Number of users who churned at least once:\", len(first_churn_ts))"
   ],
   "id": "5fc64745e92c4566"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "all_samples_list = []\n",
    "\n",
    "for cutoff_date in cutoff_dates:\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"Current cutoff_date =\", cutoff_date.date())\n",
    "    \n",
    "    # ---- 2.1 Observation window & prediction window ----\n",
    "    # Observation window: ts <= cutoff_date\n",
    "    obs_mask = (train_df['ts'] <= cutoff_date)\n",
    "    \n",
    "    # Prediction window: cutoff_date < ts <= cutoff_date + horizon_days\n",
    "    future_end = cutoff_date + pd.Timedelta(days=horizon_days)\n",
    "    fut_mask = (\n",
    "        (train_df['ts'] > cutoff_date) &\n",
    "        (train_df['ts'] <= future_end)\n",
    "    )\n",
    "    \n",
    "    # ---- 2.2 Users observed within the observation window ----\n",
    "    users_obs = train_df.loc[obs_mask, 'userId'].unique()\n",
    "    users_obs = np.sort(users_obs)\n",
    "    print(\"Users in observation window (including churned):\", len(users_obs))\n",
    "    \n",
    "    if len(users_obs) == 0:\n",
    "        print(\"No observed users for this cutoff; skip\")\n",
    "        continue\n",
    "\n",
    "    # ---- 2.2.1 Filter out users who already churned ----\n",
    "    # Reindex first_churn_ts by users_obs to align:\n",
    "    #   index = users_obs, value = first churn ts or NaT\n",
    "    churn_ts_sub = first_churn_ts.reindex(users_obs)\n",
    "    \n",
    "    # Keep conditions:\n",
    "    #   - churn_ts is NaT -> never churned -> keep\n",
    "    #   - churn_ts > cutoff_date -> churn happens in the future -> keep\n",
    "    #   - churn_ts <= cutoff_date -> already churned (or churned on cutoff day) -> drop\n",
    "    alive_mask = (churn_ts_sub.isna()) | (churn_ts_sub > cutoff_date)\n",
    "    alive_users = users_obs[alive_mask.values]\n",
    "    \n",
    "    print(\"Alive users after filtering already-churned:\", len(alive_users))\n",
    "    \n",
    "    if len(alive_users) == 0:\n",
    "        print(\"No alive users for this cutoff; skip\")\n",
    "        continue\n",
    "    \n",
    "    # ---- 2.3 Users who hit 'Cancellation Confirmation' in the prediction window ----\n",
    "    cc_future_users = (\n",
    "        train_df.loc[fut_mask & (train_df['page'] == \"Cancellation Confirmation\"), 'userId']\n",
    "        .unique()\n",
    "    )\n",
    "    \n",
    "    # Keep only users who are alive and observed\n",
    "    cc_future_users = np.intersect1d(cc_future_users, alive_users)\n",
    "    print(\"Alive users who churn in prediction window (target=1):\", len(cc_future_users))\n",
    "    \n",
    "    # ---- 2.4 Build labels for current cutoff ----\n",
    "    # alive_users defines the sample user list for this cutoff\n",
    "    # Default target=0; set target=1 for users in cc_future_users\n",
    "    y_array = np.zeros(len(alive_users), dtype=int)\n",
    "    pos_mask = np.isin(alive_users, cc_future_users)\n",
    "    y_array[pos_mask] = 1\n",
    "    \n",
    "    # ---- 2.5 Create a DataFrame for this cutoff ----\n",
    "    tmp = pd.DataFrame({\n",
    "        \"userId\": alive_users,\n",
    "        \"cutoff_date\": cutoff_date,   # same value for all rows in this batch\n",
    "        \"target\": y_array,\n",
    "    })\n",
    "    \n",
    "    print(\"Samples at this cutoff:\", len(tmp),\n",
    "          \"  Positives:\", tmp['target'].sum(),\n",
    "          \"  Positive rate:\", tmp['target'].mean())\n",
    "    \n",
    "    all_samples_list.append(tmp)"
   ],
   "id": "a7b593f8e8467b14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Concatenate all cutoff samples into the full sliding-window label table\n",
    "\n",
    "sliding_labels = pd.concat(all_samples_list, ignore_index=True)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Overall after merging all cutoffs:\")\n",
    "print(\"Total samples:\", len(sliding_labels))\n",
    "print(\"Total positives:\", sliding_labels['target'].sum())\n",
    "print(\"Overall positive rate:\", sliding_labels['target'].mean())\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(sliding_labels['target'].value_counts(normalize=True))"
   ],
   "id": "9b4134c7795f360d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Training set features",
   "id": "4fabd5d72f19a955"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Category features",
   "id": "ce95fd9c46b704fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Static user-level info (train_df is sufficient)\n",
    "user_static = (\n",
    "    train_df\n",
    "    .sort_values('ts')\n",
    "    .groupby('userId')\n",
    "    .agg({\n",
    "        'gender': 'first',   # or 'last' is fine; gender does not change\n",
    "        'state':  'first',   # extracted from location\n",
    "    })\n",
    ")\n",
    "\n",
    "# Map into sliding_labels\n",
    "sliding_labels['gender'] = sliding_labels['userId'].map(user_static['gender'])\n",
    "sliding_labels['state']  = sliding_labels['userId'].map(user_static['state'])\n",
    "\n",
    "print(\"sliding_labels with gender/state:\")\n",
    "print(sliding_labels[['userId', 'cutoff_date', 'target', 'gender', 'state']].head())"
   ],
   "id": "887ae17677bf26ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Multi-Index",
   "id": "76cb789eb2a3421b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use userId_cutoff-date as MultiIndex (required)\n",
    "sliding_labels['sample_id'] = (\n",
    "    sliding_labels['userId'].astype(str)\n",
    "    + \"_\" +\n",
    "    sliding_labels['cutoff_date'].astype(str)\n",
    ")\n",
    "\n",
    "# Set index\n",
    "sliding_labels = sliding_labels.set_index('sample_id')\n",
    "\n",
    "# Target y_all\n",
    "y_all = sliding_labels['target']\n",
    "\n",
    "# Cutoff timestamps (direct reference)\n",
    "cutoff_ts_all = sliding_labels['cutoff_date']\n",
    "\n",
    "print(\"sliding_labels head:\")\n",
    "print(sliding_labels.head())\n",
    "print(\"y_all head:\")\n",
    "print(y_all.head())\n",
    "print(\"cutoff_ts_all head:\")\n",
    "print(cutoff_ts_all.head())"
   ],
   "id": "edaba3023ead8154"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3 Lifetime",
   "id": "20f2c9fb1adb48e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Each user's registration timestamp\n",
    "uid_registration = (\n",
    "    train_df\n",
    "    .sort_values('ts')\n",
    "    .groupby('userId')['registration']\n",
    "    .first()\n",
    ")\n",
    "\n",
    "# Map into sliding_labels\n",
    "sliding_labels['registration_ts'] = sliding_labels['userId'].map(uid_registration)\n",
    "\n",
    "# Days from registration to cutoff\n",
    "days_since_registration = (\n",
    "    (sliding_labels['cutoff_date'] - sliding_labels['registration_ts'])\n",
    "    / np.timedelta64(1, 'D')\n",
    ").astype('float32')\n",
    "\n",
    "# Clip negative values (rare: abnormal registration timestamp)\n",
    "days_since_registration = days_since_registration.clip(lower=0)\n",
    "\n",
    "print(\"days_since_registration example:\")\n",
    "print(days_since_registration.head())"
   ],
   "id": "6018b85dc66e1176"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.4 Behaviors",
   "id": "383154b67aadffba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Ensure cutoff_date is datetime\n",
    "sliding_labels['cutoff_date'] = pd.to_datetime(sliding_labels['cutoff_date'])"
   ],
   "id": "efddf0df1d641f6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1) Sort train_df by userId + ts to ensure per-user order\n",
    "train_df_sorted = train_df.sort_values(['userId', 'ts']).copy()\n",
    "\n",
    "# 2) Group behavior data by user (train side)\n",
    "train_groups = dict(tuple(train_df_sorted.groupby('userId')))\n",
    "\n",
    "# 3) Group label samples by user (sliding_labels index is sample_id)\n",
    "label_groups = dict(tuple(sliding_labels.groupby('userId')))  # user -> label table\n",
    "\n",
    "print(\"Number of users (train):\", len(train_groups))\n",
    "print(\"Number of users (sliding_labels):\", len(label_groups))  # users appearing only after 2018-11-10 may be missing"
   ],
   "id": "b43844f0c91ffacc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize series for behavior counts\n",
    "\n",
    "# Initialize: index = sample_id (sliding_labels.index), all zeros\n",
    "n_events = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    # Skip if user has no behavior data in train\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "\n",
    "    # All events for this user (already sorted by ts)\n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values  # datetime64[ns] array\n",
    "\n",
    "    # All samples for this user in sliding_labels\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # searchsorted: for each cutoff, find number of events <= cutoff\n",
    "    pos = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "\n",
    "    # Write back into n_events (aligned by sample_id)\n",
    "    n_events.loc[sample_ids] = pos.astype('int32')\n",
    "\n",
    "print(\"n_events computed\")\n",
    "print(n_events.head())"
   ],
   "id": "9003a32b9a4c05cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.5 recency_hours",
   "id": "a916f5eadfad55e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Formula: cutoff_date - last event timestamp\n",
    "\n",
    "# Initialize recency_hours (hours)\n",
    "recency_hours = pd.Series(np.nan, index=sliding_labels.index, dtype='float32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values  # sorted datetime64 array\n",
    "    \n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "    \n",
    "    # hi[j] = number of events <= cutoff\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "    \n",
    "    # hi==0 means no events <= cutoff; set recency to a large value (e.g., 9999 hours)\n",
    "    # hi>0 means last event index is hi-1\n",
    "    for idx_in_uid, sample_id in enumerate(sample_ids):\n",
    "        h = hi[idx_in_uid]\n",
    "        \n",
    "        if h == 0:\n",
    "            # No event before cutoff -> very large recency (9999 hours ~ 416 days)\n",
    "            recency_hours.loc[sample_id] = 9999.0\n",
    "        else:\n",
    "            last_ts = ts_vals[h-1]\n",
    "            delta = (cutoffs[idx_in_uid] - last_ts)\n",
    "            recency_hours.loc[sample_id] = delta / np.timedelta64(1, 'h')\n",
    "\n",
    "print(\"recency_hours computed\")\n",
    "print(recency_hours.head())"
   ],
   "id": "51fdd8f129718d3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.6 events_last_7d",
   "id": "fd4ef4b1b8af6ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "events_last_7d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "seven_days = np.timedelta64(7, 'D')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "    \n",
    "    # hi = number of events <= cutoff\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "\n",
    "    # lo = number of events <= cutoff - 7 days\n",
    "    window_starts = cutoffs - seven_days\n",
    "    lo = np.searchsorted(ts_vals, window_starts, side='right')\n",
    "    \n",
    "    # Events in last 7 days = hi - lo\n",
    "    cnt_7d = hi - lo\n",
    "    events_last_7d.loc[sample_ids] = cnt_7d.astype('int32')\n",
    "\n",
    "print(\"events_last_7d computed\")\n",
    "print(events_last_7d.head())"
   ],
   "id": "5b70f1b0c6018612"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.7 songs_last_7d",
   "id": "4c0978a29f98b279"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "songs_last_7d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "seven_days = np.timedelta64(7, 'D')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    # Filter song-play events (NextSong)\n",
    "    df_song = df_u[df_u['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        # User never listened to songs -> all zeros\n",
    "        continue\n",
    "    \n",
    "    ts_song_vals = df_song['ts'].values  # sorted datetime array\n",
    "\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi = number of songs <= cutoff\n",
    "    hi = np.searchsorted(ts_song_vals, cutoffs, side='right')\n",
    "\n",
    "    # lo = number of songs <= cutoff - 7 days\n",
    "    window_starts = cutoffs - seven_days\n",
    "    lo = np.searchsorted(ts_song_vals, window_starts, side='right')\n",
    "\n",
    "    cnt_7d = hi - lo\n",
    "    songs_last_7d.loc[sample_ids] = cnt_7d.astype('int32')\n",
    "\n",
    "print(\"songs_last_7d computed\")\n",
    "print(songs_last_7d.head())"
   ],
   "id": "3ab699bf5654bfd0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.8 active_days",
   "id": "54368ab6f65a0a59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "active_days = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    # Extract dates only (sorted by ts)\n",
    "    dates_u = df_u['ts'].dt.normalize().values\n",
    "\n",
    "    # Unique dates (sorted)\n",
    "    unique_days = np.unique(dates_u)\n",
    "    \n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "    \n",
    "    # searchsorted on date array: count dates <= cutoff\n",
    "    hi = np.searchsorted(unique_days, cutoffs, side='right')\n",
    "\n",
    "    # hi[j] is the active day count for this sample\n",
    "    active_days.loc[sample_ids] = hi.astype('int32')\n",
    "\n",
    "print(\"active_days computed\")\n",
    "print(active_days.head())"
   ],
   "id": "9d922c152afd0074"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.9 total_listen_time",
   "id": "12c33dfb7e71ecd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "total_listen_time = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    # Filter NextSong events\n",
    "    df_song = df_u[df_u['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        continue\n",
    "    \n",
    "    ts_song_vals = df_song['ts'].values\n",
    "    len_song_vals = df_song['length'].values\n",
    "\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi = number of songs <= cutoff\n",
    "    hi = np.searchsorted(ts_song_vals, cutoffs, side='right')\n",
    "\n",
    "    # Prefix sum of song lengths for fast cumulative lookup\n",
    "    cum_len = np.cumsum(len_song_vals)\n",
    "\n",
    "    for j, sample_id in enumerate(sample_ids):\n",
    "        h = hi[j]\n",
    "        if h == 0:\n",
    "            total_listen_time.loc[sample_id] = 0.0\n",
    "        else:\n",
    "            total_listen_time.loc[sample_id] = float(cum_len[h-1])\n",
    "\n",
    "print(\"total_listen_time computed\")\n",
    "print(total_listen_time.head())"
   ],
   "id": "9070ac47b501b743"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.10 events_last_1d/3d",
   "id": "9f6d4dc3a5addc07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "events_last_1d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "events_last_3d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "one_day   = np.timedelta64(1, 'D')\n",
    "three_days = np.timedelta64(3, 'D')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi: number of events <= cutoff (same as n_events)\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "\n",
    "    # ---- last 1 day ----\n",
    "    window_start_1d = cutoffs - one_day\n",
    "    lo_1d = np.searchsorted(ts_vals, window_start_1d, side='right')\n",
    "    cnt_1d = hi - lo_1d\n",
    "    events_last_1d.loc[sample_ids] = cnt_1d.astype('int32')\n",
    "\n",
    "    # ---- last 3 days ----\n",
    "    window_start_3d = cutoffs - three_days\n",
    "    lo_3d = np.searchsorted(ts_vals, window_start_3d, side='right')\n",
    "    cnt_3d = hi - lo_3d\n",
    "    events_last_3d.loc[sample_ids] = cnt_3d.astype('int32')\n",
    "\n",
    "print(\"events_last_1d / 3d computed\")\n",
    "print(events_last_1d.head())\n",
    "print(events_last_3d.head())"
   ],
   "id": "5eec1ead61f9e684"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.11 songs_last_1d/3d",
   "id": "d244585323a9bccb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "songs_last_1d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "songs_last_3d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "one_day    = np.timedelta64(1, 'D')\n",
    "three_days = np.timedelta64(3, 'D')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    df_song = df_u[df_u['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        continue\n",
    "    \n",
    "    ts_song_vals = df_song['ts'].values\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi: number of songs <= cutoff\n",
    "    hi = np.searchsorted(ts_song_vals, cutoffs, side='right')\n",
    "\n",
    "    # ---- last 1 day ----\n",
    "    window_start_1d = cutoffs - one_day\n",
    "    lo_1d = np.searchsorted(ts_song_vals, window_start_1d, side='right')\n",
    "    cnt_1d = hi - lo_1d\n",
    "    songs_last_1d.loc[sample_ids] = cnt_1d.astype('int32')\n",
    "\n",
    "    # ---- last 3 days ----\n",
    "    window_start_3d = cutoffs - three_days\n",
    "    lo_3d = np.searchsorted(ts_song_vals, window_start_3d, side='right')\n",
    "    cnt_3d = hi - lo_3d\n",
    "    songs_last_3d.loc[sample_ids] = cnt_3d.astype('int32')\n",
    "\n",
    "print(\"songs_last_1d / 3d computed\")\n",
    "print(songs_last_1d.head())\n",
    "print(songs_last_3d.head())"
   ],
   "id": "17f63a2a4d1d171c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.12 Level-at-cutoff",
   "id": "6a9157dcd23b2b39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "level_at_cutoff = pd.Series(\"unknown\", index=sliding_labels.index, dtype=object)\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals   = df_u['ts'].values\n",
    "    lvl_vals  = df_u['level'].astype(str).values   # level sequence (free/paid)\n",
    "    \n",
    "    cutoffs    = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "    \n",
    "    # hi[j] = number of events <= cutoff\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "    \n",
    "    # If hi[j] == 0, user has no event before cutoff -> unknown\n",
    "    # Otherwise, the last level is lvl_vals[hi[j] - 1]\n",
    "    lvl_for_samples = []\n",
    "    for j, h in enumerate(hi):\n",
    "        if h == 0:\n",
    "            lvl_for_samples.append(\"unknown\")\n",
    "        else:\n",
    "            lvl_for_samples.append(lvl_vals[h-1])\n",
    "    \n",
    "    level_at_cutoff.loc[sample_ids] = lvl_for_samples\n",
    "    \n",
    "print(\"Level-at-cutoff computed\")\n",
    "print(level_at_cutoff.head())"
   ],
   "id": "999cde234e3e2d15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Write back to sliding_labels for later one-hot encoding\n",
    "sliding_labels['level'] = level_at_cutoff"
   ],
   "id": "d2f9baf468571910"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.13 Combining features",
   "id": "36530863bc303827"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Empty feature table; ensure X_all index is sample_id\n",
    "X_all = pd.DataFrame(index=sliding_labels.index)\n",
    "\n",
    "# 1) Lifetime features\n",
    "X_all['days_since_registration'] = days_since_registration\n",
    "\n",
    "# 2) Cumulative event count\n",
    "X_all['n_events'] = n_events.astype('int32')\n",
    "\n",
    "# 3) Time since last event (hours)\n",
    "X_all['recency_hours'] = recency_hours.astype('float32')\n",
    "\n",
    "# 4) Events / songs in last 7 days\n",
    "X_all['events_last_7d'] = events_last_7d.astype('int32')\n",
    "X_all['songs_last_7d']  = songs_last_7d.astype('int32')\n",
    "\n",
    "# 5) Active days (cumulative)\n",
    "X_all['active_days'] = active_days.astype('int32')\n",
    "\n",
    "# 6) Total listening time (cumulative)\n",
    "X_all['total_listen_time'] = total_listen_time.astype('float32')\n",
    "\n",
    "# 7) Events in last 1 day / 3 days\n",
    "X_all['events_last_1d'] = events_last_1d.astype('int32')\n",
    "X_all['events_last_3d'] = events_last_3d.astype('int32')\n",
    "\n",
    "# 8) Songs in last 1 day / 3 days\n",
    "X_all['songs_last_1d'] = songs_last_1d.astype('int32')\n",
    "X_all['songs_last_3d'] = songs_last_3d.astype('int32')\n",
    "\n",
    "print(\"Numeric features merged into X_all\")\n",
    "print(\"X_all shape:\", X_all.shape)\n",
    "print(X_all.head())\n",
    "print(\"\\nX_all dtypes:\")\n",
    "print(X_all.dtypes)"
   ],
   "id": "37b36a5c7229a8ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.14 One-hot",
   "id": "e25d6ac7f4ad5bd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# One-hot encode categorical features directly from sliding_labels\n",
    "cat_cols = ['gender', 'state', 'level']\n",
    "sliding_labels[cat_cols] = sliding_labels[cat_cols].fillna(\"missing\")\n",
    "\n",
    "cat_ohe = pd.get_dummies(\n",
    "    sliding_labels[cat_cols],\n",
    "    columns=cat_cols,\n",
    "    prefix=cat_cols\n",
    ")\n",
    "\n",
    "print(\"One-hot categorical feature example:\")\n",
    "print(cat_ohe.head())\n",
    "\n",
    "# Merge numeric + categorical\n",
    "X_train_raw = pd.concat([X_all, cat_ohe], axis=1)\n",
    "\n",
    "print(\"X_train shape:\", X_train_raw.shape)\n",
    "print(\"X_train example:\")\n",
    "print(X_train_raw.head())\n",
    "\n",
    "# y_train\n",
    "y_train_raw = sliding_labels['target']"
   ],
   "id": "167298278f1f8875"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.15 drop_duplicates",
   "id": "cfb8419dbea6d48b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_for_dedup = X_train_raw.copy()\n",
    "train_for_dedup['target'] = y_train_raw.values\n",
    "\n",
    "train_dedup = train_for_dedup.drop_duplicates()\n",
    "\n",
    "X_train = train_dedup.drop(columns=['target'])\n",
    "y_train = train_dedup['target']"
   ],
   "id": "55d01a89c25d02ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_train.head(100)",
   "id": "ded5cb0b7713af29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_train.head(100)",
   "id": "ec25cb5c29018c90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Test set features",
   "id": "f33aad0bb094dbd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4.1 Sort by userId + ts and group\n",
    "test_df_sorted = test_df.sort_values(['userId', 'ts']).copy()\n",
    "test_groups = dict(tuple(test_df_sorted.groupby('userId')))\n",
    "test_users = sorted(test_groups.keys())\n",
    "\n",
    "print(\"Number of test users:\", len(test_users))\n",
    "\n",
    "# 4.2 Global cutoff (observation end): last day in test\n",
    "global_cutoff_test = test_df_sorted['ts'].max().normalize()\n",
    "print(\"Test global cutoff_date:\", global_cutoff_test)\n",
    "\n",
    "# 4.3 Initialize test feature table: one row per user\n",
    "X_test = pd.DataFrame(index=test_users)\n"
   ],
   "id": "4a019e4b4d792b2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.1 Lifetime",
   "id": "d6dcc08c8dd26a03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Registration time per user (from test)\n",
    "uid_registration_test = (\n",
    "    test_df_sorted\n",
    "    .groupby('userId')['registration']\n",
    "    .first()\n",
    ")\n",
    "\n",
    "# Days from registration to cutoff\n",
    "days_since_registration_test = (\n",
    "    (global_cutoff_test - uid_registration_test) / np.timedelta64(1, 'D')\n",
    ").astype('float32')\n",
    "days_since_registration_test = days_since_registration_test.clip(lower=0)\n",
    "\n",
    "X_test['days_since_registration'] = days_since_registration_test.reindex(test_users)\n",
    "\n",
    "print(\"days_since_registration_test example:\")\n",
    "print(X_test['days_since_registration'].head())\n"
   ],
   "id": "852f50ca43a2d034"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2 n_events / recency_hours / active_days",
   "id": "97c46d7754934277"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_events_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "recency_hours_test = pd.Series(0.0, index=test_users, dtype='float32')\n",
    "active_days_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    # Only consider events before cutoff\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    \n",
    "    if df_before.empty:\n",
    "        n_events_test.loc[uid] = 0\n",
    "        recency_hours_test.loc[uid] = 9999.0\n",
    "        active_days_test.loc[uid] = 0\n",
    "        continue\n",
    "    \n",
    "    # Total event count\n",
    "    n_events_test.loc[uid] = len(df_before)\n",
    "    \n",
    "    # recency_hours: cutoff - last event time\n",
    "    last_ts = df_before['ts'].iloc[-1]\n",
    "    delta_h = (global_cutoff_test - last_ts) / np.timedelta64(1, 'h')\n",
    "    recency_hours_test.loc[uid] = float(delta_h)\n",
    "    \n",
    "    # Active days: number of unique dates\n",
    "    active_days_test.loc[uid] = df_before['ts'].dt.normalize().nunique()\n",
    "\n",
    "X_test['n_events'] = n_events_test\n",
    "X_test['recency_hours'] = recency_hours_test\n",
    "X_test['active_days'] = active_days_test\n",
    "\n",
    "print(\"n_events / recency_hours / active_days example:\")\n",
    "print(X_test[['n_events', 'recency_hours', 'active_days']].head())\n"
   ],
   "id": "882c5d908fc6fc5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.3 recent 7 / 3 / 1 behavior",
   "id": "230b1c2618f72166"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "events_last_7d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "events_last_3d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "events_last_1d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "\n",
    "seven_days  = np.timedelta64(7, 'D')\n",
    "three_days  = np.timedelta64(3, 'D')\n",
    "one_day     = np.timedelta64(1, 'D')\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    \n",
    "    ts_vals = df_before['ts'].values\n",
    "    \n",
    "    # Last 7 days\n",
    "    mask_7 = ts_vals > (global_cutoff_test - seven_days)\n",
    "    events_last_7d_test.loc[uid] = int(mask_7.sum())\n",
    "    \n",
    "    # Last 3 days\n",
    "    mask_3 = ts_vals > (global_cutoff_test - three_days)\n",
    "    events_last_3d_test.loc[uid] = int(mask_3.sum())\n",
    "    \n",
    "    # Last 1 day\n",
    "    mask_1 = ts_vals > (global_cutoff_test - one_day)\n",
    "    events_last_1d_test.loc[uid] = int(mask_1.sum())\n",
    "\n",
    "X_test['events_last_7d'] = events_last_7d_test\n",
    "X_test['events_last_3d'] = events_last_3d_test\n",
    "X_test['events_last_1d'] = events_last_1d_test\n",
    "\n",
    "print(\"events_last_*_test example:\")\n",
    "print(X_test[['events_last_7d', 'events_last_3d', 'events_last_1d']].head())\n"
   ],
   "id": "242bcc84d8668e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.4 recent 7 / 3 / 1 songs",
   "id": "6ad6114a55579720"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "songs_last_7d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "songs_last_3d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "songs_last_1d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "total_listen_time_test = pd.Series(0.0, index=test_users, dtype='float32')\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    \n",
    "    df_song = df_before[df_before['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        continue\n",
    "    \n",
    "    ts_song = df_song['ts'].values\n",
    "    len_song = df_song['length'].values\n",
    "    \n",
    "    # Total listening time (all songs <= cutoff)\n",
    "    total_listen_time_test.loc[uid] = float(len_song.sum())\n",
    "    \n",
    "    # Songs in last 7 days\n",
    "    mask_7 = ts_song > (global_cutoff_test - seven_days)\n",
    "    songs_last_7d_test.loc[uid] = int(mask_7.sum())\n",
    "    \n",
    "    # Last 3 days\n",
    "    mask_3 = ts_song > (global_cutoff_test - three_days)\n",
    "    songs_last_3d_test.loc[uid] = int(mask_3.sum())\n",
    "    \n",
    "    # Last 1 day\n",
    "    mask_1 = ts_song > (global_cutoff_test - one_day)\n",
    "    songs_last_1d_test.loc[uid] = int(mask_1.sum())\n",
    "\n",
    "X_test['songs_last_7d']     = songs_last_7d_test\n",
    "X_test['songs_last_3d']     = songs_last_3d_test\n",
    "X_test['songs_last_1d']     = songs_last_1d_test\n",
    "X_test['total_listen_time'] = total_listen_time_test\n",
    "\n",
    "print(\"songs_last_*_test & total_listen_time_test example:\")\n",
    "print(X_test[['songs_last_7d', 'songs_last_3d', 'songs_last_1d', 'total_listen_time']].head())\n"
   ],
   "id": "c675970f98d0a3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.5 Level-at-cutoff",
   "id": "dbef6a7b6f0f4ea6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "level_at_cutoff_test = pd.Series(\"unknown\", index=test_users, dtype=object)\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    level_at_cutoff_test.loc[uid] = str(df_before['level'].iloc[-1])\n",
    "\n",
    "print(\"level_at_cutoff_test example:\")\n",
    "print(level_at_cutoff_test.head())\n"
   ],
   "id": "1a410c0cdfe39159"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.6 Category + One-Hot",
   "id": "299ceb5fcf275c79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Static gender/state in test\n",
    "test_user_static = (\n",
    "    test_df_sorted\n",
    "    .groupby('userId')\n",
    "    .agg({\n",
    "        'gender': 'first',\n",
    "        'state':  'first',\n",
    "    })\n",
    ")\n",
    "\n",
    "cat_test = pd.DataFrame(index=test_users)\n",
    "cat_test['gender'] = test_user_static['gender']\n",
    "cat_test['state']  = test_user_static['state']\n",
    "cat_test['level']  = level_at_cutoff_test\n",
    "\n",
    "cat_test = cat_test.fillna(\"missing\")\n",
    "\n",
    "cat_test_ohe = pd.get_dummies(\n",
    "    cat_test,\n",
    "    columns=['gender', 'state', 'level'],\n",
    "    prefix=['gender', 'state', 'level']\n",
    ")\n",
    "\n",
    "print(\"Test categorical one-hot example:\")\n",
    "print(cat_test_ohe.head())\n",
    "\n",
    "# Align to train categorical columns\n",
    "cat_test_ohe = cat_test_ohe.reindex(columns=cat_ohe.columns, fill_value=0)\n",
    "\n",
    "print(\"Aligned cat_test_ohe shape:\", cat_test_ohe.shape)"
   ],
   "id": "b02b03625ba64852"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.7 Combining features",
   "id": "7e1f5a6ce09e4ad3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Final test features (align column order)\n",
    "X_test_full = pd.concat([X_test, cat_test_ohe], axis=1)\n",
    "X_test_full = X_test_full.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "print(\"X_test_full shape:\", X_test_full.shape)\n",
    "print(X_test_full.head())"
   ],
   "id": "7558acbe8790f169"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_test_full.head(100)",
   "id": "5fc4481123e44284"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Models and submission",
   "id": "2c027ac5fd2b52e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Simple oversampling utility\n",
    "def oversample(X, y):\n",
    "    X = X.copy()\n",
    "    X['target'] = y\n",
    "    major = X[X['target'] == 0]\n",
    "    minor = X[X['target'] == 1]\n",
    "\n",
    "    if len(minor) == 0:\n",
    "        raise ValueError(\"No positive samples\")\n",
    "\n",
    "    ratio = max(1, len(major) // len(minor))\n",
    "    minor_ov = pd.concat([minor] * ratio, ignore_index=True)\n",
    "\n",
    "    df_new = pd.concat([major, minor_ov], axis=0).sample(frac=1.0, random_state=42)\n",
    "    y_new = df_new['target'].values\n",
    "    X_new = df_new.drop(columns=['target'])\n",
    "\n",
    "    print(\"Positive rate after oversampling:\", y_new.mean())\n",
    "    return X_new, y_new"
   ],
   "id": "fa468bdfe07ae030"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.1 Light GBM (0.621)",
   "id": "d2c9e3306b7e296e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# 1) No oversampling\n",
    "X_lgb, y_lgb = X_train, y_train\n",
    "\n",
    "# 2) Model\n",
    "lgb_clf = lgb.LGBMClassifier(\n",
    "    n_estimators=800,\n",
    "    learning_rate=0.03,\n",
    "    # num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    # max_depth=-1,\n",
    "    random_state=42,\n",
    "    max_depth=7,          # added\n",
    "    num_leaves=32,        # added\n",
    "    min_data_in_leaf=50,  # added\n",
    "    feature_fraction=0.8, # added\n",
    "    bagging_fraction=0.8, # added\n",
    "    bagging_freq=5,       # added\n",
    ")\n",
    "\n",
    "lgb_clf.fit(X_lgb, y_lgb)\n",
    "\n",
    "# 3) Predict probabilities\n",
    "pred_lgb = lgb_clf.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "# 4) Align order with example_submission\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "user_ids = example_sub['id'].astype(str)\n",
    "\n",
    "proba_aligned = pd.Series(pred_lgb, index=X_test_full.index).loc[user_ids].values\n",
    "\n",
    "# 5) Top 50% rule\n",
    "threshold = np.quantile(proba_aligned, 0.5)\n",
    "pred_label = (proba_aligned >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": example_sub[\"id\"],\n",
    "    \"target\": pred_label\n",
    "})\n",
    "\n",
    "print(submission['target'].value_counts(normalize=True))\n",
    "submission.to_csv(\"submission_LightGBM.csv\", index=False)\n",
    "print(\"Saved submission_LightGBM.csv\")"
   ],
   "id": "f95394bef77aa4ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.2 Logistic Regression (0.624)",
   "id": "286209ddbb03adab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1) Oversample\n",
    "X_lr, y_lr = oversample(X_train, y_train)\n",
    "\n",
    "# 2) Standardize\n",
    "scaler_lr = StandardScaler()\n",
    "X_lr_scaled = scaler_lr.fit_transform(X_lr)\n",
    "X_test_lr_scaled = scaler_lr.transform(X_test_full)\n",
    "\n",
    "# 3) Train model\n",
    "lr_clf = LogisticRegression(\n",
    "    C=0.1,          # added: smaller than default 1.0 to reduce overfitting\n",
    "    penalty='l2',   # added\n",
    "    solver='liblinear',\n",
    "    class_weight='balanced',\n",
    "    max_iter=2000   # added\n",
    ")\n",
    "lr_clf.fit(X_lr_scaled, y_lr)\n",
    "\n",
    "# 4) Predict probabilities\n",
    "pred_lr = lr_clf.predict_proba(X_test_lr_scaled)[:, 1]\n",
    "\n",
    "# ========== Top 50% submission ==========\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "user_ids = example_sub['id'].astype(str)\n",
    "\n",
    "proba_align = pd.Series(pred_lr, index=X_test_full.index).loc[user_ids].values\n",
    "\n",
    "threshold = np.quantile(proba_align, 0.5)\n",
    "print(\"LR top50 threshold =\", threshold)\n",
    "\n",
    "pred_label = (proba_align >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": example_sub['id'],\n",
    "    \"target\": pred_label\n",
    "})\n",
    "print(submission['target'].value_counts(normalize=True))\n",
    "\n",
    "submission.to_csv(\"submission_LR.csv\", index=False)\n",
    "print(\"Saved submission_LR.csv\")"
   ],
   "id": "b789e97e6a78414"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.3 ExtraTrees (0.610)",
   "id": "8c96b0d386d88272"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# 1) No oversampling for now\n",
    "X_et, y_et = X_train, y_train\n",
    "\n",
    "# 2) ExtraTrees model\n",
    "et_clf = ExtraTreesClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "et_clf.fit(X_et, y_et)\n",
    "\n",
    "# 3) Predict probabilities\n",
    "pred_et = et_clf.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "# ========== Top 50% ==========\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "user_ids = example_sub['id'].astype(str)\n",
    "\n",
    "proba_align = pd.Series(pred_et, index=X_test_full.index).loc[user_ids].values\n",
    "threshold = np.quantile(proba_align, 0.5)\n",
    "print(\"ET top50 threshold =\", threshold)\n",
    "\n",
    "pred_label = (proba_align >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": example_sub['id'],\n",
    "    \"target\": pred_label\n",
    "})\n",
    "print(submission['target'].value_counts(normalize=True))\n",
    "\n",
    "submission.to_csv(\"submission_ET.csv\", index=False)\n",
    "print(\"Saved submission_ET.csv\")"
   ],
   "id": "557d342fcc4774"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.4 KNN (0.56)",
   "id": "5ead72f99ca30304"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1) Oversample\n",
    "X_knn, y_knn = oversample(X_train, y_train)\n",
    "\n",
    "# 2) Standardize\n",
    "scaler_knn = StandardScaler()\n",
    "X_knn_scaled = scaler_knn.fit_transform(X_knn)\n",
    "X_test_knn_scaled = scaler_knn.transform(X_test_full)\n",
    "\n",
    "# 3) KNN\n",
    "knn_clf = KNeighborsClassifier(\n",
    "    n_neighbors=100,\n",
    "    weights='distance',\n",
    "    p=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "knn_clf.fit(X_knn_scaled, y_knn)\n",
    "\n",
    "# 4) Predict probabilities\n",
    "pred_knn = knn_clf.predict_proba(X_test_knn_scaled)[:, 1]\n",
    "\n",
    "# ========== Top 50% ==========\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "user_ids = example_sub['id'].astype(str)\n",
    "\n",
    "proba_align = pd.Series(pred_knn, index=X_test_full.index).loc[user_ids].values\n",
    "threshold = np.quantile(proba_align, 0.5)\n",
    "print(\"KNN top50 threshold =\", threshold)\n",
    "\n",
    "pred_label = (proba_align >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": example_sub['id'],\n",
    "    \"target\": pred_label\n",
    "})\n",
    "print(submission['target'].value_counts(normalize=True))\n",
    "\n",
    "submission.to_csv(\"submission_KNN.csv\", index=False)\n",
    "print(\"Saved submission_KNN.csv\")"
   ],
   "id": "fd98d16c1c39469d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.5 RF (0.610)",
   "id": "582425c564f91442"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1) No oversampling for now\n",
    "X_rf, y_rf = X_train, y_train\n",
    "\n",
    "# 2) Train RF\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=7,  # modified\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "rf_clf.fit(X_rf, y_rf)\n",
    "\n",
    "# 3) Predict probabilities\n",
    "pred_rf = rf_clf.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "# ========== Top 50% ==========\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "user_ids = example_sub['id'].astype(str)\n",
    "\n",
    "proba_align = pd.Series(pred_rf, index=X_test_full.index).loc[user_ids].values\n",
    "threshold = np.quantile(proba_align, 0.5)\n",
    "print(\"RF top50 threshold =\", threshold)\n",
    "\n",
    "pred_label = (proba_align >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": example_sub['id'],\n",
    "    \"target\": pred_label\n",
    "})\n",
    "print(submission['target'].value_counts(normalize=True))\n",
    "\n",
    "submission.to_csv(\"submission_RF.csv\", index=False)\n",
    "print(\"Saved submission_RF.csv\")"
   ],
   "id": "c69175aeff5796a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
