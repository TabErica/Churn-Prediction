{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "id": "eaf8eac82a5b59b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Preprocess",
   "id": "67b63d8196e8b390"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Read raw parquet files\n",
    "train_df = pd.read_parquet(\"train.parquet\")\n",
    "test_df = pd.read_parquet(\"test.parquet\")\n",
    "\n",
    "# 3. Convert time fields\n",
    "train_df['ts'] = pd.to_datetime(train_df['ts'], unit='ms')\n",
    "test_df['ts'] = pd.to_datetime(test_df['ts'], unit='ms')\n",
    "\n",
    "train_df['registration'] = pd.to_datetime(train_df['registration'])\n",
    "test_df['registration'] = pd.to_datetime(test_df['registration'])\n",
    "\n",
    "train_df['time'] = pd.to_datetime(train_df['time'])\n",
    "test_df['time'] = pd.to_datetime(test_df['time'])\n",
    "\n",
    "# 4. Clean the 'page' field (strip spaces)\n",
    "train_df['page'] = train_df['page'].astype(str).str.strip()\n",
    "test_df['page'] = test_df['page'].astype(str).str.strip()\n",
    "\n",
    "# 5. Extract state from location (last 2 chars) as 'state'\n",
    "#    e.g. \"New York, NY\" -> \"NY\"\n",
    "train_df['location'] = train_df['location'].astype(str).str.strip()\n",
    "test_df['location'] = test_df['location'].astype(str).str.strip()\n",
    "\n",
    "train_df['state'] = train_df['location'].str[-2:]\n",
    "test_df['state'] = test_df['location'].str[-2:]\n",
    "\n",
    "# 6. Sort by userId + ts to ensure chronological order\n",
    "train_df = train_df.sort_values(['userId', 'ts'])\n",
    "test_df = test_df.sort_values(['userId', 'ts'])\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)"
   ],
   "id": "5c159ef378c8d521"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Window label",
   "id": "eddaee2febced3b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Set sliding-window parameters\n",
    "horizon_days = 10  # prediction horizon length = 10 days (competition setting)\n",
    "\n",
    "# Manually set an earliest cutoff start date\n",
    "cutoff_start = pd.to_datetime(\"2018-10-15\")\n",
    "\n",
    "# The last cutoff must satisfy: cutoff + 10 days <= train_df['ts'].max().normalize()\n",
    "# We take the max date (normalized to day) and subtract horizon_days\n",
    "max_ts_date = train_df['ts'].max().normalize()          # typically 2018-11-20\n",
    "cutoff_end = max_ts_date - pd.Timedelta(days=horizon_days + 1)\n",
    "\n",
    "\n",
    "print(\"cutoff_start:\", cutoff_start.date())\n",
    "print(\"max_ts_date :\", max_ts_date.date())\n",
    "print(\"cutoff_end  :\", cutoff_end.date())"
   ],
   "id": "d312751b36c77bcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Generate a list of cutoff dates (one per day)\n",
    "cutoff_dates = pd.date_range(start=cutoff_start, end=cutoff_end, freq=\"D\")\n",
    "print(\"Number of cutoff_dates:\", len(cutoff_dates))\n",
    "print(\"cutoff_dates preview:\", list(cutoff_dates[:5]), \"...\", list(cutoff_dates[-5:]))"
   ],
   "id": "692e06dc5e932b21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. For each cutoff_date, generate samples (userId, cutoff_date, target)\n",
    "\n",
    "# Compute each user's first churn timestamp (first time seeing Cancellation Confirmation)\n",
    "first_churn_ts = (\n",
    "    train_df[train_df['page'] == \"Cancellation Confirmation\"]\n",
    "    .groupby('userId')['ts']\n",
    "    .min()\n",
    ")\n",
    "\n",
    "print(\"Number of users who churned at least once:\", len(first_churn_ts))"
   ],
   "id": "414476464183483f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "all_samples_list = []\n",
    "\n",
    "for cutoff_date in cutoff_dates:\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"Current cutoff_date =\", cutoff_date.date())\n",
    "    \n",
    "    # ---- 2.1 Observation window & prediction window ----\n",
    "    # Observation window: ts <= cutoff_date\n",
    "    obs_mask = (train_df['ts'] <= cutoff_date)\n",
    "    \n",
    "    # Prediction window: cutoff_date < ts <= cutoff_date + horizon_days\n",
    "    future_end = cutoff_date + pd.Timedelta(days=horizon_days)\n",
    "    fut_mask = (\n",
    "        (train_df['ts'] > cutoff_date) &\n",
    "        (train_df['ts'] <= future_end)\n",
    "    )\n",
    "    \n",
    "    # ---- 2.2 Users present in the observation window ----\n",
    "    users_obs = train_df.loc[obs_mask, 'userId'].unique()\n",
    "    users_obs = np.sort(users_obs)\n",
    "    print(\"Users in observation window (including already churned):\", len(users_obs))\n",
    "    \n",
    "    if len(users_obs) == 0:\n",
    "        print(\"No observed users for this cutoff, skipping\")\n",
    "        continue\n",
    "\n",
    "    # ---- 2.2.1 Filter out users who already churned ----\n",
    "    # Reindex first_churn_ts on users_obs to get churn time or NaT\n",
    "    churn_ts_sub = first_churn_ts.reindex(users_obs)\n",
    "    \n",
    "    # Keep if:\n",
    "    #   - churn_ts is NaT  -> never churned  -> keep\n",
    "    #   - churn_ts > cutoff_date -> churn in the future -> keep\n",
    "    # Drop if:\n",
    "    #   - churn_ts <= cutoff_date -> already churned (or churned today) -> drop\n",
    "    alive_mask = (churn_ts_sub.isna()) | (churn_ts_sub > cutoff_date)\n",
    "    alive_users = users_obs[alive_mask.values]\n",
    "    \n",
    "    print(\"Alive users after filtering churned:\", len(alive_users))\n",
    "    \n",
    "    if len(alive_users) == 0:\n",
    "        print(\"No alive users for this cutoff, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # ---- 2.3 Users who click 'Cancellation Confirmation' in the prediction window ----\n",
    "    cc_future_users = (\n",
    "        train_df.loc[fut_mask & (train_df['page'] == \"Cancellation Confirmation\"), 'userId']\n",
    "        .unique()\n",
    "    )\n",
    "    \n",
    "    # Keep only users that are both alive and present in observation window\n",
    "    cc_future_users = np.intersect1d(cc_future_users, alive_users)\n",
    "    print(\"Alive users with churn(=1) in prediction window:\", len(cc_future_users))\n",
    "    \n",
    "    # ---- 2.4 Build label array for this cutoff ----\n",
    "    # alive_users is the true sample user list for this cutoff\n",
    "    # Default target=0; set target=1 if user is in cc_future_users\n",
    "    y_array = np.zeros(len(alive_users), dtype=int)\n",
    "    pos_mask = np.isin(alive_users, cc_future_users)\n",
    "    y_array[pos_mask] = 1\n",
    "    \n",
    "    # ---- 2.5 Assemble into a DataFrame ----\n",
    "    tmp = pd.DataFrame({\n",
    "        \"userId\": alive_users,\n",
    "        \"cutoff_date\": cutoff_date,   # same for the whole batch\n",
    "        \"target\": y_array,\n",
    "    })\n",
    "    \n",
    "    print(\"Samples for this cutoff:\", len(tmp),\n",
    "          \"  Positives:\", tmp['target'].sum(),\n",
    "          \"  Positive rate:\", tmp['target'].mean())\n",
    "    \n",
    "    all_samples_list.append(tmp)"
   ],
   "id": "d5c7850a59bf3b20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Concatenate all cutoffs to obtain the sliding-window label table\n",
    "\n",
    "sliding_labels = pd.concat(all_samples_list, ignore_index=True)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Overall after merging all cutoffs:\")\n",
    "print(\"Total samples:\", len(sliding_labels))\n",
    "print(\"Total positives:\", sliding_labels['target'].sum())\n",
    "print(\"Overall positive rate:\", sliding_labels['target'].mean())\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(sliding_labels['target'].value_counts(normalize=True))"
   ],
   "id": "553fb636251d2d02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Training set features",
   "id": "55083a7aa845f983"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Category features",
   "id": "41f126281b87af13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Static user info (one row per user; train_df is enough)\n",
    "user_static = (\n",
    "    train_df\n",
    "    .sort_values('ts')\n",
    "    .groupby('userId')\n",
    "    .agg({\n",
    "        'gender': 'first',   # or last; gender should not change\n",
    "        'state':  'first',   # last 2 chars extracted from location\n",
    "    })\n",
    ")\n",
    "\n",
    "# Map to sliding_labels\n",
    "sliding_labels['gender'] = sliding_labels['userId'].map(user_static['gender'])\n",
    "sliding_labels['state']  = sliding_labels['userId'].map(user_static['state'])\n",
    "\n",
    "print(\"sliding_labels with gender/state:\")\n",
    "print(sliding_labels[['userId', 'cutoff_date', 'target', 'gender', 'state']].head())"
   ],
   "id": "85c40477fa2d23e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Multi-Index",
   "id": "cdc0410b2d357690"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use userId_cutoff_date as MultiIndex (required)\n",
    "sliding_labels['sample_id'] = (\n",
    "    sliding_labels['userId'].astype(str)\n",
    "    + \"_\" +\n",
    "    sliding_labels['cutoff_date'].astype(str)\n",
    ")\n",
    "\n",
    "# Set index\n",
    "sliding_labels = sliding_labels.set_index('sample_id')\n",
    "\n",
    "# Target variable y_all\n",
    "y_all = sliding_labels['target']\n",
    "\n",
    "# Cutoff timestamps (direct reference)\n",
    "cutoff_ts_all = sliding_labels['cutoff_date']\n",
    "\n",
    "\n",
    "print(\"First rows of sliding_labels:\")\n",
    "print(sliding_labels.head())\n",
    "print(\"First rows of y_all:\")\n",
    "print(y_all.head())\n",
    "print(\"First rows of cutoff_ts_all:\")\n",
    "print(cutoff_ts_all.head())"
   ],
   "id": "dccde7cc42206986"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3 Lifetime",
   "id": "aea9361ef69f45a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Registration date per user\n",
    "uid_registration = (\n",
    "    train_df\n",
    "    .sort_values('ts')\n",
    "    .groupby('userId')['registration']\n",
    "    .first()\n",
    ")\n",
    "\n",
    "# Map to sliding_labels\n",
    "sliding_labels['registration_ts'] = sliding_labels['userId'].map(uid_registration)\n",
    "\n",
    "# Days from registration to cutoff\n",
    "days_since_registration = (\n",
    "    (sliding_labels['cutoff_date'] - sliding_labels['registration_ts'])\n",
    "    / np.timedelta64(1, 'D')\n",
    ").astype('float32')\n",
    "\n",
    "# Clip negative values (rare: abnormal registration timestamp)\n",
    "days_since_registration = days_since_registration.clip(lower=0)\n",
    "\n",
    "print(\"days_since_registration examples:\")\n",
    "print(days_since_registration.head())"
   ],
   "id": "6f2509742c54d5d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.4 Behaviors",
   "id": "103ada0d035d42bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Ensure cutoff_date is datetime\n",
    "sliding_labels['cutoff_date'] = pd.to_datetime(sliding_labels['cutoff_date'])"
   ],
   "id": "9262528ec14bbd4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1) Sort train_df by userId + ts to ensure per-user chronological order\n",
    "train_df_sorted = train_df.sort_values(['userId', 'ts']).copy()\n",
    "\n",
    "# 2) Group behavior data by user for train\n",
    "train_groups = dict(tuple(train_df_sorted.groupby('userId')))\n",
    "\n",
    "# 3) Group samples by user on label side (sliding_labels index is sample_id)\n",
    "label_groups = dict(tuple(sliding_labels.groupby('userId')))  # user : label table\n",
    "\n",
    "print(\"Number of users (train):\", len(train_groups))\n",
    "print(\"Number of users (sliding_labels):\", len(label_groups))  # users appearing only after 2018-11-10 may be missing"
   ],
   "id": "d90fe346ec5dafa3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize series for event counts\n",
    "\n",
    "# Initialize: index = sample_id (sliding_labels.index), fill with 0\n",
    "n_events = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    # Skip if user has no behavior in train\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "\n",
    "    # All events for this user (already sorted by ts)\n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values  # datetime64[ns] array\n",
    "\n",
    "    # All samples (cutoffs) for this user\n",
    "    cutoffs = lbl_u['cutoff_date'].values  # cutoff times\n",
    "    sample_ids = lbl_u.index  # sample_id list\n",
    "\n",
    "    # For each cutoff, find number of events <= cutoff\n",
    "    pos = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "\n",
    "    # Write back to n_events aligned by sample_id\n",
    "    n_events.loc[sample_ids] = pos.astype('int32')\n",
    "\n",
    "print(\"n_events computed\")\n",
    "print(n_events.head())"
   ],
   "id": "731fee63105c3003"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.5 recency_hours",
   "id": "a3058ec4bfba401b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Definition: cutoff_date - last event time\n",
    "\n",
    "# Initialize recency_hours (in hours)\n",
    "recency_hours = pd.Series(np.nan, index=sliding_labels.index, dtype='float32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values   # sorted datetime64 array\n",
    "    \n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "    \n",
    "    # hi[j] = number of events <= cutoff_j\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "    \n",
    "    # hi==0: no event before cutoff -> set large recency, e.g., 9999 hours\n",
    "    # hi>0: last event index is hi-1\n",
    "    for idx_in_uid, sample_id in enumerate(sample_ids):\n",
    "        h = hi[idx_in_uid]\n",
    "        \n",
    "        if h == 0:\n",
    "            # No events before cutoff -> very large recency (9999 hours ~ 416 days)\n",
    "            recency_hours.loc[sample_id] = 9999.0\n",
    "        else:\n",
    "            last_ts = ts_vals[h-1]\n",
    "            delta = (cutoffs[idx_in_uid] - last_ts)\n",
    "            recency_hours.loc[sample_id] = delta / np.timedelta64(1, 'h')\n",
    "\n",
    "print(\"recency_hours computed\")\n",
    "print(recency_hours.head())"
   ],
   "id": "47b2571740fd699"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.6 events_last_7d",
   "id": "4d1e16d8ce394995"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "events_last_7d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "seven_days = np.timedelta64(7, 'D')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values             # sorted datetime64 array\n",
    "    cutoffs = lbl_u['cutoff_date'].values   # cutoff array\n",
    "    sample_ids = lbl_u.index\n",
    "    \n",
    "    # hi = number of events <= cutoff\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "\n",
    "    # lo = number of events <= cutoff - 7 days\n",
    "    window_starts = cutoffs - seven_days\n",
    "    lo = np.searchsorted(ts_vals, window_starts, side='right')\n",
    "    \n",
    "    # last 7 days events = hi - lo\n",
    "    cnt_7d = hi - lo\n",
    "    events_last_7d.loc[sample_ids] = cnt_7d.astype('int32')\n",
    "\n",
    "print(\"events_last_7d computed\")\n",
    "print(events_last_7d.head())"
   ],
   "id": "8577293c6c8744ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.7 songs_last_7d",
   "id": "38138798bff65ea0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "songs_last_7d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "seven_days = np.timedelta64(7, 'D')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    # Filter song-play events (NextSong)\n",
    "    df_song = df_u[df_u['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        # User never listened -> all zeros\n",
    "        continue\n",
    "    \n",
    "    ts_song_vals = df_song['ts'].values  # sorted datetime array\n",
    "\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi = number of songs <= cutoff\n",
    "    hi = np.searchsorted(ts_song_vals, cutoffs, side='right')\n",
    "\n",
    "    # lo = number of songs <= cutoff - 7 days\n",
    "    window_starts = cutoffs - seven_days\n",
    "    lo = np.searchsorted(ts_song_vals, window_starts, side='right')\n",
    "\n",
    "    cnt_7d = hi - lo\n",
    "    songs_last_7d.loc[sample_ids] = cnt_7d.astype('int32')\n",
    "\n",
    "print(\"songs_last_7d computed\")\n",
    "print(songs_last_7d.head())"
   ],
   "id": "369504b05a38c01c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.8 active_days",
   "id": "9283fdb1bb27050b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "active_days = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    # Extract dates (keep ordering by ts)\n",
    "    dates_u = df_u['ts'].dt.normalize().values  # remove time, keep date only\n",
    "\n",
    "    # Unique dates (sorted)\n",
    "    unique_days = np.unique(dates_u)\n",
    "    \n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "    \n",
    "    # Use searchsorted on date array:\n",
    "    # number of unique days <= cutoff_date\n",
    "    hi = np.searchsorted(unique_days, cutoffs, side='right')\n",
    "\n",
    "    # hi[j] is the number of active days up to cutoff\n",
    "    active_days.loc[sample_ids] = hi.astype('int32')\n",
    "\n",
    "print(\"active_days computed\")\n",
    "print(active_days.head())"
   ],
   "id": "d772375335fe74f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.9 total_listen_time",
   "id": "4f803a2982b13050"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "total_listen_time = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    # Filter NextSong rows\n",
    "    df_song = df_u[df_u['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        continue\n",
    "    \n",
    "    ts_song_vals = df_song['ts'].values         # song timestamps (sorted)\n",
    "    len_song_vals = df_song['length'].values    # song durations (float)\n",
    "\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # Boundary for songs <= cutoff\n",
    "    hi = np.searchsorted(ts_song_vals, cutoffs, side='right')\n",
    "\n",
    "    # Prefix sum for faster cumulative duration\n",
    "    cum_len = np.cumsum(len_song_vals)\n",
    "\n",
    "    for j, sample_id in enumerate(sample_ids):\n",
    "        h = hi[j]\n",
    "        if h == 0:\n",
    "            total_listen_time.loc[sample_id] = 0.0\n",
    "        else:\n",
    "            total_listen_time.loc[sample_id] = float(cum_len[h-1])\n",
    "\n",
    "print(\"total_listen_time computed\")\n",
    "print(total_listen_time.head())"
   ],
   "id": "f626c348cd1020ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.10 events_last_1d/3d",
   "id": "354dbda41276ddbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "events_last_1d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "events_last_3d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "one_day   = np.timedelta64(1, 'D')\n",
    "three_days = np.timedelta64(3, 'D')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi: number of events <= cutoff (same as n_events)\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "\n",
    "    # ---- last 1 day ----\n",
    "    window_start_1d = cutoffs - one_day\n",
    "    lo_1d = np.searchsorted(ts_vals, window_start_1d, side='right')\n",
    "    cnt_1d = hi - lo_1d\n",
    "    events_last_1d.loc[sample_ids] = cnt_1d.astype('int32')\n",
    "\n",
    "    # ---- last 3 days ----\n",
    "    window_start_3d = cutoffs - three_days\n",
    "    lo_3d = np.searchsorted(ts_vals, window_start_3d, side='right')\n",
    "    cnt_3d = hi - lo_3d\n",
    "    events_last_3d.loc[sample_ids] = cnt_3d.astype('int32')\n",
    "\n",
    "print(\"events_last_1d / 3d computed\")\n",
    "print(events_last_1d.head())\n",
    "print(events_last_3d.head())"
   ],
   "id": "a454c3a9be1d5a9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.11 songs_last_1d/3d",
   "id": "e7edfddb8739afef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "songs_last_1d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "songs_last_3d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "one_day    = np.timedelta64(1, 'D')\n",
    "three_days = np.timedelta64(3, 'D')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    df_song = df_u[df_u['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        continue\n",
    "    \n",
    "    ts_song_vals = df_song['ts'].values\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi: number of songs <= cutoff\n",
    "    hi = np.searchsorted(ts_song_vals, cutoffs, side='right')\n",
    "\n",
    "    # ---- last 1 day ----\n",
    "    window_start_1d = cutoffs - one_day\n",
    "    lo_1d = np.searchsorted(ts_song_vals, window_start_1d, side='right')\n",
    "    cnt_1d = hi - lo_1d\n",
    "    songs_last_1d.loc[sample_ids] = cnt_1d.astype('int32')\n",
    "\n",
    "    # ---- last 3 days ----\n",
    "    window_start_3d = cutoffs - three_days\n",
    "    lo_3d = np.searchsorted(ts_song_vals, window_start_3d, side='right')\n",
    "    cnt_3d = hi - lo_3d\n",
    "    songs_last_3d.loc[sample_ids] = cnt_3d.astype('int32')\n",
    "\n",
    "print(\"songs_last_1d / 3d computed\")\n",
    "print(songs_last_1d.head())\n",
    "print(songs_last_3d.head())"
   ],
   "id": "bdd7d058515627f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.12 Level-at-cutoff",
   "id": "44308cced60fd8bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "level_at_cutoff = pd.Series(\"unknown\", index=sliding_labels.index, dtype=object)\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    \n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals   = df_u['ts'].values\n",
    "    lvl_vals  = df_u['level'].astype(str).values   # level sequence (free/paid)\n",
    "    \n",
    "    cutoffs    = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "    \n",
    "    # hi[j] = number of events <= cutoff_j\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "    \n",
    "    # hi[j] == 0: no events before cutoff\n",
    "    # else: last level is lvl_vals[hi[j] - 1]\n",
    "    lvl_for_samples = []\n",
    "    for j, h in enumerate(hi):\n",
    "        if h == 0:\n",
    "            lvl_for_samples.append(\"unknown\")\n",
    "        else:\n",
    "            lvl_for_samples.append(lvl_vals[h-1])\n",
    "    \n",
    "    level_at_cutoff.loc[sample_ids] = lvl_for_samples\n",
    "    \n",
    "print(\"Level-at-cutoff computed\")\n",
    "print(level_at_cutoff.head())"
   ],
   "id": "c6d0859663453248"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Write level_at_cutoff back to sliding_labels for one-hot later\n",
    "sliding_labels['level'] = level_at_cutoff"
   ],
   "id": "8bbc9cb6c2f9effc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.13 Sessions",
   "id": "2abd6b38e9f05ae2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize series (index is still sample_id)\n",
    "session_count = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "mean_session_duration = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "max_session_duration = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "min_session_duration = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "std_session_duration = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "\n",
    "mean_event_count_per_session = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "max_event_count_per_session = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "min_event_count_per_session = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "std_event_count_per_session = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "\n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    # Group by sessionId and compute start/end/count per session\n",
    "    sess_u = (\n",
    "        df_u\n",
    "        .groupby('sessionId')\n",
    "        .agg(\n",
    "            session_start=('ts', 'min'),\n",
    "            session_end=('ts', 'max'),\n",
    "            session_event_count=('ts', 'count'),\n",
    "        )\n",
    "        .sort_values('session_start')\n",
    "    )\n",
    "\n",
    "    if sess_u.empty:\n",
    "        continue\n",
    "\n",
    "    # Convert to numpy for faster searchsorted\n",
    "    sess_start_vals = sess_u['session_start'].values\n",
    "    sess_end_vals = sess_u['session_end'].values\n",
    "    sess_event_vals = sess_u['session_event_count'].values.astype('int32')\n",
    "    sess_dur_vals = (sess_end_vals - sess_start_vals) / np.timedelta64(1, 's')  # seconds\n",
    "\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi_sess[j] = number of sessions that started <= cutoff_j\n",
    "    hi_sess = np.searchsorted(sess_start_vals, cutoffs, side='right')\n",
    "\n",
    "    for j, sample_id in enumerate(sample_ids):\n",
    "        h = hi_sess[j]\n",
    "        if h == 0:\n",
    "            # No sessions up to this cutoff\n",
    "            session_count.loc[sample_id] = 0\n",
    "            continue\n",
    "\n",
    "        # Use the first h sessions for stats\n",
    "        dur_subset = sess_dur_vals[:h]\n",
    "        cnt_subset = sess_event_vals[:h]\n",
    "\n",
    "        session_count.loc[sample_id] = h\n",
    "        mean_session_duration.loc[sample_id] = dur_subset.mean()\n",
    "        max_session_duration.loc[sample_id] = dur_subset.max()\n",
    "        min_session_duration.loc[sample_id] = dur_subset.min()\n",
    "        std_session_duration.loc[sample_id] = dur_subset.std(ddof=0)\n",
    "\n",
    "        mean_event_count_per_session.loc[sample_id] = cnt_subset.mean()\n",
    "        max_event_count_per_session.loc[sample_id] = cnt_subset.max()\n",
    "        min_event_count_per_session.loc[sample_id] = cnt_subset.min()\n",
    "        std_event_count_per_session.loc[sample_id] = cnt_subset.std(ddof=0)\n",
    "\n",
    "print(\"Session features computed\")\n",
    "print(session_count.head(), mean_session_duration.head())"
   ],
   "id": "b309e7ef74ef6775"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.14 Page Ratio",
   "id": "f5c865bb57b5a2f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pages_of_interest = [\n",
    "    \"NextSong\",\n",
    "    \"Thumbs Up\",\n",
    "    \"Thumbs Down\",\n",
    "    \"Add to Playlist\",\n",
    "    \"Roll Advert\",\n",
    "    \"Help\",\n",
    "    \"Error\",\n",
    "    \"Submit Upgrade\",\n",
    "    \"Submit Downgrade\",\n",
    "]\n",
    "\n",
    "# Initialize: one Series per page\n",
    "page_count_series = {\n",
    "    p: pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "    for p in pages_of_interest\n",
    "}\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "\n",
    "    df_u = train_groups[uid]\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # For each page, build timestamp array and use searchsorted\n",
    "    for p in pages_of_interest:\n",
    "        df_p = df_u[df_u['page'] == p]\n",
    "        if df_p.empty:\n",
    "            continue\n",
    "\n",
    "        ts_p = df_p['ts'].values\n",
    "        hi_p = np.searchsorted(ts_p, cutoffs, side='right')  # count of this page <= cutoff\n",
    "\n",
    "        page_count_series[p].loc[sample_ids] = hi_p.astype('int32')\n",
    "\n",
    "print(\"Page count features computed\")"
   ],
   "id": "d73cb11c09ecaf46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.15 Other level features",
   "id": "61552afb8b6bc951"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1) ever_paid: whether the user ever had paid in history (static)\n",
    "ever_paid_uid = (\n",
    "    train_df\n",
    "    .groupby('userId')['level']\n",
    "    .apply(lambda s: int((s.astype(str) == \"paid\").any()))\n",
    ")\n",
    "\n",
    "ever_paid = sliding_labels['userId'].map(ever_paid_uid).fillna(0).astype('int8')\n",
    "\n",
    "# 2) n_level_change: number of free/paid switches up to cutoff\n",
    "n_level_change = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "\n",
    "    df_u = train_groups[uid].copy()\n",
    "    # level may have NaN; cast to string\n",
    "    lvl_vals = df_u['level'].astype(str).values\n",
    "    ts_vals = df_u['ts'].values\n",
    "\n",
    "    if len(lvl_vals) <= 1:\n",
    "        continue\n",
    "\n",
    "    # Find timestamps where level changes\n",
    "    change_mask = lvl_vals[1:] != lvl_vals[:-1]\n",
    "    change_ts = ts_vals[1:][change_mask]\n",
    "\n",
    "    if len(change_ts) == 0:\n",
    "        continue\n",
    "\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    # hi_change[j] = number of level changes <= cutoff_j\n",
    "    hi_change = np.searchsorted(change_ts, cutoffs, side='right')\n",
    "    n_level_change.loc[sample_ids] = hi_change.astype('int32')\n",
    "\n",
    "print(\"Level features computed\")\n",
    "print(ever_paid.head(), n_level_change.head())"
   ],
   "id": "d910f1fe3c15e8dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.16 Status",
   "id": "a12a77ba5b562af8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "status_codes_of_interest = [200, 404, 307]\n",
    "\n",
    "status_count_series = {\n",
    "    code: pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "    for code in status_codes_of_interest\n",
    "}\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "\n",
    "    df_u = train_groups[uid]\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    for code in status_codes_of_interest:\n",
    "        df_code = df_u[df_u['status'] == code]\n",
    "        if df_code.empty:\n",
    "            continue\n",
    "\n",
    "        ts_code = df_code['ts'].values\n",
    "        hi_code = np.searchsorted(ts_code, cutoffs, side='right')\n",
    "\n",
    "        status_count_series[code].loc[sample_ids] = hi_code.astype('int32')\n",
    "\n",
    "print(\"Status count features computed\")"
   ],
   "id": "8f49ffe3a28670dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.13 Combining the feature",
   "id": "da0fcfe162aab1a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Empty feature table; ensure X_all index is sample_id\n",
    "X_all = pd.DataFrame(index=sliding_labels.index)\n",
    "\n",
    "# 1) Lifecycle\n",
    "X_all['days_since_registration'] = days_since_registration\n",
    "\n",
    "# 2) Cumulative event count\n",
    "X_all['n_events'] = n_events.astype('int32')\n",
    "\n",
    "# 3) Time since last event (hours)\n",
    "X_all['recency_hours'] = recency_hours.astype('float32')\n",
    "\n",
    "# 4) Last 7 days events and songs\n",
    "X_all['events_last_7d'] = events_last_7d.astype('int32')\n",
    "X_all['songs_last_7d']  = songs_last_7d.astype('int32')\n",
    "\n",
    "# 5) Active days\n",
    "X_all['active_days'] = active_days.astype('int32')\n",
    "\n",
    "# 6) Total listening time\n",
    "X_all['total_listen_time'] = total_listen_time.astype('float32')\n",
    "\n",
    "# 7) Last 1 day / 3 days event counts\n",
    "X_all['events_last_1d'] = events_last_1d.astype('int32')\n",
    "X_all['events_last_3d'] = events_last_3d.astype('int32')\n",
    "\n",
    "# 8) Last 1 day / 3 days song counts\n",
    "X_all['songs_last_1d'] = songs_last_1d.astype('int32')\n",
    "X_all['songs_last_3d'] = songs_last_3d.astype('int32')\n",
    "\n",
    "# Session-related features\n",
    "X_all['session_count']                = session_count\n",
    "X_all['mean_session_duration']        = mean_session_duration\n",
    "X_all['max_session_duration']         = max_session_duration\n",
    "X_all['min_session_duration']         = min_session_duration\n",
    "X_all['std_session_duration']         = std_session_duration\n",
    "\n",
    "X_all['mean_event_count_per_session'] = mean_event_count_per_session\n",
    "X_all['max_event_count_per_session']  = max_event_count_per_session\n",
    "X_all['min_event_count_per_session']  = min_event_count_per_session\n",
    "X_all['std_event_count_per_session']  = std_event_count_per_session\n",
    "\n",
    "# Add page counts and ratio features\n",
    "for p in pages_of_interest:\n",
    "    safe_name = p.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    col_cnt   = f\"cnt_page_{safe_name}\"\n",
    "    col_ratio = f\"ratio_page_{safe_name}\"\n",
    "    \n",
    "    X_all[col_cnt] = page_count_series[p]\n",
    "    # ratio = page count / total events (n_events)\n",
    "    X_all[col_ratio] = (\n",
    "        X_all[col_cnt] / (X_all['n_events'] + eps)\n",
    "    ).astype('float32')\n",
    "\n",
    "print(\"Page count + ratio features added to X_all\")\n",
    "\n",
    "# Paid-related\n",
    "X_all['ever_paid']      = ever_paid\n",
    "X_all['n_level_change'] = n_level_change\n",
    "\n",
    "# Status-related\n",
    "for code in status_codes_of_interest:\n",
    "    col_cnt = f\"n_status_{code}\"\n",
    "    X_all[col_cnt] = status_count_series[code]\n",
    "\n",
    "# 404 fraction\n",
    "X_all['frac_status_404'] = (\n",
    "    X_all['n_status_404'] / (X_all['n_events'] + eps)\n",
    ").astype('float32')\n",
    "\n",
    "# 200 fraction (optional)\n",
    "X_all['frac_status_200'] = (\n",
    "    X_all['n_status_200'] / (X_all['n_events'] + eps)\n",
    ").astype('float32')\n",
    "\n",
    "print(\"Status features added to X_all\")\n",
    "print(X_all[['n_status_404', 'frac_status_404']].head())\n",
    "\n",
    "\n",
    "print(\"Numeric features merged into X_all\")\n",
    "print(\"X_all shape:\", X_all.shape)\n",
    "print(X_all.head())\n",
    "print(\"\\nX_all dtypes:\")\n",
    "print(X_all.dtypes)"
   ],
   "id": "2bf90727e0673af1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.14 Category + One-Hot",
   "id": "c3777f4deb042163"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. One-hot encode categorical features on sliding_labels\n",
    "cat_cols = ['gender', 'state', 'level']\n",
    "sliding_labels[cat_cols] = sliding_labels[cat_cols].fillna(\"missing\")\n",
    "\n",
    "cat_ohe = pd.get_dummies(\n",
    "    sliding_labels[cat_cols],\n",
    "    columns=cat_cols,\n",
    "    prefix=cat_cols\n",
    ")\n",
    "\n",
    "print(\"Categorical one-hot feature example:\")\n",
    "print(cat_ohe.head())\n",
    "\n",
    "# Merge\n",
    "X_train = pd.concat([X_all, cat_ohe], axis=1)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_train example:\")\n",
    "print(X_train.head())\n",
    "\n",
    "# y_train\n",
    "y_train = sliding_labels['target']"
   ],
   "id": "c68fb157dc3cb15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_train.head(100)",
   "id": "5346ddf0945b07b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_train.head(100)",
   "id": "5bc58111f160eed6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Test set features",
   "id": "269fbfd3a1a015e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4.1 Sort by userId + ts, then group\n",
    "test_df_sorted = test_df.sort_values(['userId', 'ts']).copy()\n",
    "test_groups = dict(tuple(test_df_sorted.groupby('userId')))\n",
    "test_users = sorted(test_groups.keys())\n",
    "\n",
    "print(\"Number of test users:\", len(test_users))\n",
    "\n",
    "# 4.2 Global cutoff for test (observation end): use last day in test\n",
    "global_cutoff_test = test_df_sorted['ts'].max().normalize()\n",
    "print(\"Test global cutoff_date:\", global_cutoff_test)\n",
    "\n",
    "# 4.3 Initialize feature table: one sample per user\n",
    "X_test = pd.DataFrame(index=test_users)\n"
   ],
   "id": "30980f3e640d44fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.1 Lifetime",
   "id": "873f3bf24aab87f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Registration time per user (from test)\n",
    "uid_registration_test = (\n",
    "    test_df_sorted\n",
    "    .groupby('userId')['registration']\n",
    "    .first()\n",
    ")\n",
    "\n",
    "# Days from registration to cutoff\n",
    "days_since_registration_test = (\n",
    "    (global_cutoff_test - uid_registration_test) / np.timedelta64(1, 'D')\n",
    ").astype('float32')\n",
    "days_since_registration_test = days_since_registration_test.clip(lower=0)\n",
    "\n",
    "X_test['days_since_registration'] = days_since_registration_test.reindex(test_users)\n",
    "\n",
    "print(\"days_since_registration_test example:\")\n",
    "print(X_test['days_since_registration'].head())\n"
   ],
   "id": "6110d257e7baa4eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2 n_events / recency_hours / active_days",
   "id": "e6a9bc3457d17622"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "n_events_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "recency_hours_test = pd.Series(0.0, index=test_users, dtype='float32')\n",
    "active_days_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    # Only consider events <= cutoff\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    \n",
    "    if df_before.empty:\n",
    "        n_events_test.loc[uid] = 0\n",
    "        recency_hours_test.loc[uid] = 9999.0\n",
    "        active_days_test.loc[uid] = 0\n",
    "        continue\n",
    "    \n",
    "    # Total events\n",
    "    n_events_test.loc[uid] = len(df_before)\n",
    "    \n",
    "    # recency_hours: cutoff - last event\n",
    "    last_ts = df_before['ts'].iloc[-1]\n",
    "    delta_h = (global_cutoff_test - last_ts) / np.timedelta64(1, 'h')\n",
    "    recency_hours_test.loc[uid] = float(delta_h)\n",
    "    \n",
    "    # Active days: number of distinct dates\n",
    "    active_days_test.loc[uid] = df_before['ts'].dt.normalize().nunique()\n",
    "\n",
    "X_test['n_events'] = n_events_test\n",
    "X_test['recency_hours'] = recency_hours_test\n",
    "X_test['active_days'] = active_days_test\n",
    "\n",
    "print(\"n_events / recency_hours / active_days examples:\")\n",
    "print(X_test[['n_events', 'recency_hours', 'active_days']].head())\n"
   ],
   "id": "950f18ce39c56de9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.3 recent 7 / 3 / 1 behavior",
   "id": "c9a6e300c9ae3756"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "events_last_7d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "events_last_3d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "events_last_1d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "\n",
    "seven_days  = np.timedelta64(7, 'D')\n",
    "three_days  = np.timedelta64(3, 'D')\n",
    "one_day     = np.timedelta64(1, 'D')\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    \n",
    "    ts_vals = df_before['ts'].values\n",
    "    \n",
    "    # Last 7 days\n",
    "    mask_7 = ts_vals > (global_cutoff_test - seven_days)\n",
    "    events_last_7d_test.loc[uid] = int(mask_7.sum())\n",
    "    \n",
    "    # Last 3 days\n",
    "    mask_3 = ts_vals > (global_cutoff_test - three_days)\n",
    "    events_last_3d_test.loc[uid] = int(mask_3.sum())\n",
    "    \n",
    "    # Last 1 day\n",
    "    mask_1 = ts_vals > (global_cutoff_test - one_day)\n",
    "    events_last_1d_test.loc[uid] = int(mask_1.sum())\n",
    "\n",
    "X_test['events_last_7d'] = events_last_7d_test\n",
    "X_test['events_last_3d'] = events_last_3d_test\n",
    "X_test['events_last_1d'] = events_last_1d_test\n",
    "\n",
    "print(\"events_last_*_test examples:\")\n",
    "print(X_test[['events_last_7d', 'events_last_3d', 'events_last_1d']].head())\n"
   ],
   "id": "9f9d0765a204c9cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.4 recent 7 / 3 / 1 songs",
   "id": "3149a796cd547cf5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "songs_last_7d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "songs_last_3d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "songs_last_1d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "total_listen_time_test = pd.Series(0.0, index=test_users, dtype='float32')\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    \n",
    "    df_song = df_before[df_before['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        continue\n",
    "    \n",
    "    ts_song = df_song['ts'].values\n",
    "    len_song = df_song['length'].values\n",
    "    \n",
    "    # Total listening time (all songs <= cutoff)\n",
    "    total_listen_time_test.loc[uid] = float(len_song.sum())\n",
    "    \n",
    "    # Last 7 days song count\n",
    "    mask_7 = ts_song > (global_cutoff_test - seven_days)\n",
    "    songs_last_7d_test.loc[uid] = int(mask_7.sum())\n",
    "    \n",
    "    # Last 3 days\n",
    "    mask_3 = ts_song > (global_cutoff_test - three_days)\n",
    "    songs_last_3d_test.loc[uid] = int(mask_3.sum())\n",
    "    \n",
    "    # Last 1 day\n",
    "    mask_1 = ts_song > (global_cutoff_test - one_day)\n",
    "    songs_last_1d_test.loc[uid] = int(mask_1.sum())\n",
    "\n",
    "X_test['songs_last_7d']     = songs_last_7d_test\n",
    "X_test['songs_last_3d']     = songs_last_3d_test\n",
    "X_test['songs_last_1d']     = songs_last_1d_test\n",
    "X_test['total_listen_time'] = total_listen_time_test\n",
    "\n",
    "print(\"songs_last_*_test & total_listen_time_test examples:\")\n",
    "print(X_test[['songs_last_7d', 'songs_last_3d', 'songs_last_1d', 'total_listen_time']].head())\n"
   ],
   "id": "5aabb58fc0f9bf19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.5 Level-at-cutoff",
   "id": "b2d50a7ad69d6f78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "level_at_cutoff_test = pd.Series(\"unknown\", index=test_users, dtype=object)\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    level_at_cutoff_test.loc[uid] = str(df_before['level'].iloc[-1])\n",
    "\n",
    "print(\"level_at_cutoff_test example:\")\n",
    "print(level_at_cutoff_test.head())\n"
   ],
   "id": "9044d9483e3bc9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.6 Page Count + Page Ratio",
   "id": "fa97d48d07e95cbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Universe of pages (union of train and test)\n",
    "all_pages = pd.concat([train_df['page'], test_df['page']]).unique()\n",
    "\n",
    "# Initialize features (two per page: count_XXX, ratio_XXX)\n",
    "for p in all_pages:\n",
    "    cname = f\"cnt_page_{p.replace(' ', '_')}\"\n",
    "    X_test[cname] = 0\n",
    "\n",
    "# Ratio features\n",
    "for p in all_pages:\n",
    "    cname = f\"ratio_page_{p.replace(' ', '_')}\"\n",
    "    X_test[cname] = 0.0\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "\n",
    "    # Page counts\n",
    "    page_counts = df_before['page'].value_counts()\n",
    "\n",
    "    for p, cnt in page_counts.items():\n",
    "        cname = f\"cnt_page_{p.replace(' ', '_')}\"\n",
    "        if cname in X_test.columns:\n",
    "            X_test.loc[uid, cname] = cnt\n",
    "\n",
    "    total_events = len(df_before)\n",
    "    if total_events > 0:\n",
    "        for p, cnt in page_counts.items():\n",
    "            rname = f\"ratio_page_{p.replace(' ', '_')}\"\n",
    "            if rname in X_test.columns:\n",
    "                X_test.loc[uid, rname] = cnt / total_events"
   ],
   "id": "3af8e51dd28231b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.7 Status Count + Status Ratio",
   "id": "4ec4071c29f43e78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Universe of status codes\n",
    "all_status = pd.concat([train_df['status'], test_df['status']]).dropna().unique()\n",
    "\n",
    "# Initialize\n",
    "for s in all_status:\n",
    "    X_test[f\"cnt_status_{s}\"] = 0\n",
    "    X_test[f\"ratio_status_{s}\"] = 0.0\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "\n",
    "    status_counts = df_before['status'].value_counts()\n",
    "    total_events = len(df_before)\n",
    "\n",
    "    for s, cnt in status_counts.items():\n",
    "        cname = f\"cnt_status_{s}\"\n",
    "        rname = f\"ratio_status_{s}\"\n",
    "        X_test.loc[uid, cname] = cnt\n",
    "        X_test.loc[uid, rname] = cnt / total_events"
   ],
   "id": "e073c335eb22a7f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.8 Level Features",
   "id": "e3570124974f1ae8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_test['is_paid_last'] = 0\n",
    "X_test['ever_paid'] = 0\n",
    "X_test['n_level_change'] = 0\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "\n",
    "    levels = df_before['level'].astype(str).values\n",
    "\n",
    "    # Last level\n",
    "    X_test.loc[uid, 'is_paid_last'] = 1 if levels[-1] == 'paid' else 0\n",
    "\n",
    "    # Ever paid in history\n",
    "    X_test.loc[uid, 'ever_paid'] = 1 if 'paid' in levels else 0\n",
    "\n",
    "    # Number of level switches\n",
    "    if len(levels) > 1:\n",
    "        X_test.loc[uid, 'n_level_change'] = np.sum(levels[1:] != levels[:-1])"
   ],
   "id": "b8491a5b67380dd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.9 Session Duration Features",
   "id": "99db7856c85b8e94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_test['session_mean_duration'] = 0.0\n",
    "X_test['session_max_duration']  = 0.0\n",
    "X_test['session_min_duration']  = 0.0\n",
    "X_test['session_std_duration']  = 0.0\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "\n",
    "    session_grp = df_before.groupby('sessionId')\n",
    "\n",
    "    durations = []\n",
    "    for sid, g in session_grp:\n",
    "        start = g['ts'].min()\n",
    "        end   = g['ts'].max()\n",
    "        durations.append((end - start) / np.timedelta64(1, 's'))   # seconds\n",
    "\n",
    "    if len(durations) > 0:\n",
    "        durations = np.array(durations)\n",
    "        X_test.loc[uid, 'session_mean_duration'] = durations.mean()\n",
    "        X_test.loc[uid, 'session_max_duration']  = durations.max()\n",
    "        X_test.loc[uid, 'session_min_duration']  = durations.min()\n",
    "        X_test.loc[uid, 'session_std_duration']  = durations.std()"
   ],
   "id": "250a58890393bdff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.10 Session Event Count Features",
   "id": "9904f68e0a3e5f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "X_test['session_mean_events'] = 0.0\n",
    "X_test['session_max_events']  = 0.0\n",
    "X_test['session_min_events']  = 0.0\n",
    "X_test['session_std_events']  = 0.0\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "\n",
    "    session_grp = df_before.groupby('sessionId')\n",
    "    event_counts = session_grp.size().values\n",
    "\n",
    "    if len(event_counts) > 0:\n",
    "        event_counts = np.array(event_counts)\n",
    "        X_test.loc[uid, 'session_mean_events'] = event_counts.mean()\n",
    "        X_test.loc[uid, 'session_max_events']  = event_counts.max()\n",
    "        X_test.loc[uid, 'session_min_events']  = event_counts.min()\n",
    "        X_test.loc[uid, 'session_std_events']  = event_counts.std()"
   ],
   "id": "eb7261ac10b1e552"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.11 Category + One-Hot",
   "id": "44bc2fd369325a3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Static gender/state for test\n",
    "test_user_static = (\n",
    "    test_df_sorted\n",
    "    .groupby('userId')\n",
    "    .agg({\n",
    "        'gender': 'first',\n",
    "        'state':  'first',\n",
    "    })\n",
    ")\n",
    "\n",
    "cat_test = pd.DataFrame(index=test_users)\n",
    "cat_test['gender'] = test_user_static['gender']\n",
    "cat_test['state']  = test_user_static['state']\n",
    "cat_test['level']  = level_at_cutoff_test\n",
    "\n",
    "cat_test = cat_test.fillna(\"missing\")\n",
    "\n",
    "cat_test_ohe = pd.get_dummies(\n",
    "    cat_test,\n",
    "    columns=['gender', 'state', 'level'],\n",
    "    prefix=['gender', 'state', 'level']\n",
    ")\n",
    "\n",
    "print(\"Test categorical one-hot example:\")\n",
    "print(cat_test_ohe.head())\n",
    "\n",
    "# Align to train categorical columns\n",
    "cat_test_ohe = cat_test_ohe.reindex(columns=cat_ohe.columns, fill_value=0)\n",
    "\n",
    "print(\"cat_test_ohe shape after aligning to train:\", cat_test_ohe.shape)"
   ],
   "id": "c3cf63d034c6bcfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.12 Combining features",
   "id": "31c79fc4c7ab7c85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Final test features (align column order)\n",
    "X_test_full = pd.concat([X_test, cat_test_ohe], axis=1)\n",
    "X_test_full = X_test_full.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "print(\"X_test_full shape:\", X_test_full.shape)\n",
    "print(X_test_full.head())"
   ],
   "id": "871c66b0b0ea5c05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_test_full.head(100)",
   "id": "46715f2f63a9c0fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Models",
   "id": "1887bbfca0c0d6b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Simple oversampling helper\n",
    "def oversample(X, y):\n",
    "    X = X.copy()\n",
    "    X['target'] = y\n",
    "    major = X[X['target'] == 0]\n",
    "    minor = X[X['target'] == 1]\n",
    "\n",
    "    if len(minor) == 0:\n",
    "        raise ValueError(\"No positive samples\")\n",
    "\n",
    "    ratio = max(1, len(major) // len(minor))\n",
    "    minor_ov = pd.concat([minor] * ratio, ignore_index=True)\n",
    "\n",
    "    df_new = pd.concat([major, minor_ov], axis=0).sample(frac=1.0, random_state=42)\n",
    "    y_new = df_new['target'].values\n",
    "    X_new = df_new.drop(columns=['target'])\n",
    "\n",
    "    print(\"Positive rate after oversampling:\", y_new.mean())\n",
    "    return X_new, y_new"
   ],
   "id": "761fc6b7075915c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Grid Search ",
   "id": "108efdf6e1777a9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### A. Grid Search for Logistic Regression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def cv_auc_lr(params, X, y, n_splits=3, random_state=42):\n",
    "    \"\"\"\n",
    "    For a given set of Logistic Regression parameters,\n",
    "    compute the mean AUC using K-fold cross-validation with oversampling.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    aucs = []\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), start=1):\n",
    "        X_tr = X.iloc[tr_idx].copy()\n",
    "        y_tr = y.iloc[tr_idx].copy()\n",
    "        X_val = X.iloc[val_idx].copy()\n",
    "        y_val = y.iloc[val_idx].copy()\n",
    "\n",
    "        # Apply oversampling only on the training fold to avoid data leakage\n",
    "        X_tr_os, y_tr_os = oversample(X_tr, y_tr.values)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_tr_os_scaled = scaler.fit_transform(X_tr_os)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        clf = LogisticRegression(\n",
    "            max_iter=2000,\n",
    "            solver=\"liblinear\",\n",
    "            **params\n",
    "        )\n",
    "        clf.fit(X_tr_os_scaled, y_tr_os)\n",
    "\n",
    "        val_proba = clf.predict_proba(X_val_scaled)[:, 1]\n",
    "        auc = roc_auc_score(y_val, val_proba)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return np.mean(aucs)\n",
    "\n",
    "\n",
    "# A relatively small parameter grid to keep computation time reasonable\n",
    "lr_param_grid = [\n",
    "    {\"C\": 0.1, \"class_weight\": None},\n",
    "    {\"C\": 0.3, \"class_weight\": None},\n",
    "    {\"C\": 1.0, \"class_weight\": None},\n",
    "    {\"C\": 3.0, \"class_weight\": None},\n",
    "    {\"C\": 0.3, \"class_weight\": \"balanced\"},\n",
    "    {\"C\": 1.0, \"class_weight\": \"balanced\"},\n",
    "    {\"C\": 3.0, \"class_weight\": \"balanced\"},\n",
    "]\n",
    "\n",
    "best_lr_auc = -1\n",
    "best_lr_params = None\n",
    "\n",
    "for params in lr_param_grid:\n",
    "    mean_auc = cv_auc_lr(params, X_train, y_train)\n",
    "    print(f\"LR params={params}, mean AUC={mean_auc:.6f}\")\n",
    "    if mean_auc > best_lr_auc:\n",
    "        best_lr_auc = mean_auc\n",
    "        best_lr_params = params\n",
    "\n",
    "print(\"\\n>>> Best LR params:\", best_lr_params)\n",
    "print(\">>> Best LR CV AUC:\", best_lr_auc)\n"
   ],
   "id": "9bcc34d09145e4ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### B. Grid Search for Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def cv_auc_rf(params, X, y, n_splits=3, random_state=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    aucs = []\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), start=1):\n",
    "        X_tr = X.iloc[tr_idx].copy()\n",
    "        y_tr = y.iloc[tr_idx].copy()\n",
    "        X_val = X.iloc[val_idx].copy()\n",
    "        y_val = y.iloc[val_idx].copy()\n",
    "\n",
    "        # Oversampling on the training fold for RF (optional)\n",
    "        X_tr_os, y_tr_os = oversample(X_tr, y_tr.values)\n",
    "\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=300,  # Fixed to a moderate value to control complexity\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            **params\n",
    "        )\n",
    "        clf.fit(X_tr_os, y_tr_os)\n",
    "\n",
    "        val_proba = clf.predict_proba(X_val)[:, 1]\n",
    "        auc = roc_auc_score(y_val, val_proba)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return np.mean(aucs)\n",
    "\n",
    "\n",
    "rf_param_grid = [\n",
    "    {\"max_depth\": 8, \"min_samples_split\": 10, \"min_samples_leaf\": 5, \"max_features\": \"sqrt\"},\n",
    "    {\"max_depth\": 12, \"min_samples_split\": 10, \"min_samples_leaf\": 5, \"max_features\": \"sqrt\"},\n",
    "    {\"max_depth\": 12, \"min_samples_split\": 20, \"min_samples_leaf\": 10, \"max_features\": \"sqrt\"},\n",
    "    {\"max_depth\": None, \"min_samples_split\": 20, \"min_samples_leaf\": 10, \"max_features\": \"sqrt\"},\n",
    "]\n",
    "\n",
    "best_rf_auc = -1\n",
    "best_rf_params = None\n",
    "\n",
    "for params in rf_param_grid:\n",
    "    mean_auc = cv_auc_rf(params, X_train, y_train)\n",
    "    print(f\"RF params={params}, mean AUC={mean_auc:.6f}\")\n",
    "    if mean_auc > best_rf_auc:\n",
    "        best_rf_auc = mean_auc\n",
    "        best_rf_params = params\n",
    "\n",
    "print(\"\\n>>> Best RF params:\", best_rf_params)\n",
    "print(\">>> Best RF CV AUC:\", best_rf_auc)\n"
   ],
   "id": "d856685d0a05d3d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### C. Refit Best LR & RF on Full Training Set + Voting\n",
    "\n",
    "# 1) Oversample the full training set for LR\n",
    "X_lr_os, y_lr_os = oversample(X_train.copy(), y_train.values)\n",
    "\n",
    "scaler_lr = StandardScaler()\n",
    "X_lr_os_scaled = scaler_lr.fit_transform(X_lr_os)\n",
    "X_test_lr_scaled = scaler_lr.transform(X_test_full)\n",
    "\n",
    "best_lr_clf = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    solver=\"liblinear\",\n",
    "    **best_lr_params\n",
    ")\n",
    "best_lr_clf.fit(X_lr_os_scaled, y_lr_os)\n",
    "\n",
    "pred_lr = best_lr_clf.predict_proba(X_test_lr_scaled)[:, 1]\n",
    "\n",
    "# 2) Train RF on the oversampled full training set\n",
    "X_rf_os, y_rf_os = oversample(X_train.copy(), y_train.values)\n",
    "\n",
    "best_rf_clf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    **best_rf_params\n",
    ")\n",
    "best_rf_clf.fit(X_rf_os, y_rf_os)\n",
    "\n",
    "pred_rf = best_rf_clf.predict_proba(X_test_full)[:, 1]\n"
   ],
   "id": "d1364c51eaae32ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### D. LR + RF Soft Voting\n",
    "w_lr, w_rf = 1.0, 2.0\n",
    "\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "user_ids = example_sub[\"id\"].astype(str)\n",
    "\n",
    "proba_lr_aligned = (\n",
    "    pd.Series(pred_lr, index=X_test_full.index)\n",
    "    .loc[user_ids]\n",
    "    .values\n",
    ")\n",
    "\n",
    "proba_rf_aligned = (\n",
    "    pd.Series(pred_rf, index=X_test_full.index)\n",
    "    .loc[user_ids]\n",
    "    .values\n",
    ")\n",
    "\n",
    "voting_proba = (\n",
    "    w_lr * proba_lr_aligned +\n",
    "    w_rf * proba_rf_aligned\n",
    ") / (w_lr + w_rf)\n",
    "\n",
    "threshold = np.quantile(voting_proba, 0.5)\n",
    "print(\"Best LR+RF voting top-50% threshold =\", threshold)\n",
    "\n",
    "pred_label = (voting_proba >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": example_sub[\"id\"],\n",
    "    \"target\": pred_label\n",
    "})\n",
    "print(submission[\"target\"].value_counts(normalize=True))\n",
    "\n",
    "submission.to_csv(\"submission_LR_RF_voting_gridsearch.csv\", index=False)\n",
    "print(\"Saved submission_LR_RF_voting_gridsearch.csv\")"
   ],
   "id": "cd82b26d1bcb8357"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "71ec74b4781df6be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
