{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:40:00.649927Z",
     "start_time": "2025-12-19T00:40:00.282024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ],
   "id": "2cf84cb136f2ca32",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Data preprocessing",
   "id": "da7e0849881571bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:40:38.384569Z",
     "start_time": "2025-12-19T00:40:00.650859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Read raw parquet files\n",
    "train_df = pd.read_parquet(\"train.parquet\")\n",
    "test_df  = pd.read_parquet(\"test.parquet\")\n",
    "\n",
    "# 2) Convert time columns\n",
    "train_df['ts'] = pd.to_datetime(train_df['ts'], unit='ms')\n",
    "test_df['ts']  = pd.to_datetime(test_df['ts'], unit='ms')\n",
    "\n",
    "train_df['registration'] = pd.to_datetime(train_df['registration'])\n",
    "test_df['registration']  = pd.to_datetime(test_df['registration'])\n",
    "\n",
    "train_df['time'] = pd.to_datetime(train_df['time'])\n",
    "test_df['time']  = pd.to_datetime(test_df['time'])\n",
    "\n",
    "# 3) Clean string columns\n",
    "train_df['page'] = train_df['page'].astype(str).str.strip()\n",
    "test_df['page']  = test_df['page'].astype(str).str.strip()\n",
    "\n",
    "train_df['location'] = train_df['location'].astype(str).str.strip()\n",
    "test_df['location']  = test_df['location'].astype(str).str.strip()\n",
    "\n",
    "# 4) Extract state from location (last two characters), e.g. \"New York, NY\" -> \"NY\"\n",
    "train_df['state'] = train_df['location'].str[-2:]\n",
    "test_df['state']  = test_df['location'].str[-2:]\n",
    "\n",
    "# 5) Sort by userId and timestamp\n",
    "train_df = train_df.sort_values(['userId', 'ts'])\n",
    "test_df  = test_df.sort_values(['userId', 'ts'])\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape :\", test_df.shape)\n"
   ],
   "id": "85377673a0b70bc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (17499636, 20)\n",
      "Test shape : (4393179, 20)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Sliding-window labels (cutoff-based)",
   "id": "5b90f797e1f1159"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:41:06.860133Z",
     "start_time": "2025-12-19T00:40:38.387407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "horizon_days = 10\n",
    "cutoff_start = pd.to_datetime(\"2018-10-15\")\n",
    "\n",
    "max_ts_date = train_df['ts'].max().normalize()\n",
    "cutoff_end  = max_ts_date - pd.Timedelta(days=horizon_days + 1)\n",
    "\n",
    "print(\"cutoff_start:\", cutoff_start.date())\n",
    "print(\"max_ts_date :\", max_ts_date.date())\n",
    "print(\"cutoff_end  :\", cutoff_end.date())\n",
    "\n",
    "cutoff_dates = pd.date_range(start=cutoff_start, end=cutoff_end, freq=\"D\")\n",
    "print(\"Number of cutoff dates:\", len(cutoff_dates))\n",
    "\n",
    "# First churn time per user (if ever)\n",
    "first_churn_ts = (\n",
    "    train_df[train_df['page'] == \"Cancellation Confirmation\"]\n",
    "    .groupby('userId')['ts']\n",
    "    .min()\n",
    ")\n",
    "print(\"Users who ever churned:\", len(first_churn_ts))\n",
    "\n",
    "all_samples_list = []\n",
    "for cutoff_date in cutoff_dates:\n",
    "    # Observation period: up to cutoff (inclusive)\n",
    "    obs_mask = (train_df['ts'] <= cutoff_date)\n",
    "\n",
    "    # Prediction window: (cutoff, cutoff + horizon_days]\n",
    "    fut_end  = cutoff_date + pd.Timedelta(days=horizon_days)\n",
    "    fut_mask = (train_df['ts'] > cutoff_date) & (train_df['ts'] <= fut_end)\n",
    "\n",
    "    # Users observed by cutoff\n",
    "    users_obs = train_df.loc[obs_mask, 'userId'].unique()\n",
    "    users_obs = np.sort(users_obs)\n",
    "    if len(users_obs) == 0:\n",
    "        continue\n",
    "\n",
    "    # Keep users who are still \"alive\" at cutoff:\n",
    "    # either never churned or churn time is after cutoff\n",
    "    churn_ts_sub = first_churn_ts.reindex(users_obs)\n",
    "    alive_mask = (churn_ts_sub.isna()) | (churn_ts_sub > cutoff_date)\n",
    "    alive_users = users_obs[alive_mask.values]\n",
    "    if len(alive_users) == 0:\n",
    "        continue\n",
    "\n",
    "    # Users who churn within the future window\n",
    "    cc_future_users = (\n",
    "        train_df.loc[\n",
    "            fut_mask & (train_df['page'] == \"Cancellation Confirmation\"),\n",
    "            'userId'\n",
    "        ].unique()\n",
    "    )\n",
    "    cc_future_users = np.intersect1d(cc_future_users, alive_users)\n",
    "\n",
    "    # Build label array\n",
    "    y_array = np.zeros(len(alive_users), dtype=int)\n",
    "    y_array[np.isin(alive_users, cc_future_users)] = 1\n",
    "\n",
    "    tmp = pd.DataFrame({\n",
    "        \"userId\": alive_users,\n",
    "        \"cutoff_date\": cutoff_date,\n",
    "        \"target\": y_array,\n",
    "    })\n",
    "    all_samples_list.append(tmp)\n",
    "\n",
    "sliding_labels = pd.concat(all_samples_list, ignore_index=True)\n",
    "\n",
    "sliding_labels['sample_id'] = (\n",
    "    sliding_labels['userId'].astype(str)\n",
    "    + \"_\" +\n",
    "    sliding_labels['cutoff_date'].astype(str)\n",
    ")\n",
    "sliding_labels = sliding_labels.set_index('sample_id')\n",
    "sliding_labels['cutoff_date'] = pd.to_datetime(sliding_labels['cutoff_date'])\n",
    "\n",
    "y_train = sliding_labels['target']\n",
    "\n",
    "print(\"Sliding-window samples:\", len(sliding_labels))\n",
    "print(\"Positive rate:\", y_train.mean())\n"
   ],
   "id": "210cb02cc608f9b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutoff_start: 2018-10-15\n",
      "max_ts_date : 2018-11-20\n",
      "cutoff_end  : 2018-11-09\n",
      "Number of cutoff dates: 26\n",
      "Users who ever churned: 4271\n",
      "Sliding-window samples: 400774\n",
      "Positive rate: 0.050260246423171166\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Static categorical features (gender/state)",
   "id": "a6a8dbc4574b04d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:41:17.927138Z",
     "start_time": "2025-12-19T00:41:06.862440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_static = (\n",
    "    train_df\n",
    "    .sort_values('ts')\n",
    "    .groupby('userId')\n",
    "    .agg({\n",
    "        'gender': 'first',\n",
    "        'state':  'first',\n",
    "    })\n",
    ")\n",
    "\n",
    "sliding_labels['gender'] = sliding_labels['userId'].map(user_static['gender'])\n",
    "sliding_labels['state']  = sliding_labels['userId'].map(user_static['state'])\n"
   ],
   "id": "d2d17f021b1ad3e2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Lifetime feature: days_since_registration",
   "id": "62ef2a07e231550e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:41:25.498363Z",
     "start_time": "2025-12-19T00:41:17.929027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "uid_registration = (\n",
    "    train_df\n",
    "    .sort_values('ts')\n",
    "    .groupby('userId')['registration']\n",
    "    .first()\n",
    ")\n",
    "sliding_labels['registration_ts'] = sliding_labels['userId'].map(uid_registration)\n",
    "\n",
    "days_since_registration = (\n",
    "    (sliding_labels['cutoff_date'] - sliding_labels['registration_ts'])\n",
    "    / np.timedelta64(1, 'D')\n",
    ").astype('float32').clip(lower=0)\n"
   ],
   "id": "50471732708d8c15",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Grouping (train / labels)",
   "id": "1fa7b4f583ceb413"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:41:50.193584Z",
     "start_time": "2025-12-19T00:41:25.500041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df_sorted = train_df.sort_values(['userId', 'ts']).copy()\n",
    "train_groups = dict(tuple(train_df_sorted.groupby('userId')))\n",
    "label_groups = dict(tuple(sliding_labels.groupby('userId')))\n",
    "\n",
    "print(\"Train users:\", len(train_groups))\n",
    "print(\"Label users:\", len(label_groups))\n"
   ],
   "id": "7e68e4b3d55707f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train users: 19140\n",
      "Label users: 17351\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Basic behavior features (n_events / recency / recent windows / active_days / total_listen_time)",
   "id": "cf98f22d5fd2bb03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:42:23.950227Z",
     "start_time": "2025-12-19T00:41:50.194356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- n_events: number of events up to cutoff (inclusive)\n",
    "n_events = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "    pos = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "    n_events.loc[sample_ids] = pos.astype('int32')\n",
    "\n",
    "# --- recency_hours: hours since last event before cutoff\n",
    "recency_hours = pd.Series(np.nan, index=sliding_labels.index, dtype='float32')\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "    for j, sid in enumerate(sample_ids):\n",
    "        h = hi[j]\n",
    "        if h == 0:\n",
    "            recency_hours.loc[sid] = 9999.0\n",
    "        else:\n",
    "            last_ts = ts_vals[h - 1]\n",
    "            recency_hours.loc[sid] = (cutoffs[j] - last_ts) / np.timedelta64(1, 'h')\n",
    "\n",
    "# --- recent window counts: 7d / 3d / 1d (all events)\n",
    "events_last_7d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "events_last_3d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "events_last_1d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "seven_days  = np.timedelta64(7, 'D')\n",
    "three_days  = np.timedelta64(3, 'D')\n",
    "one_day     = np.timedelta64(1, 'D')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    ts_vals = train_groups[uid]['ts'].values\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "    lo7 = np.searchsorted(ts_vals, cutoffs - seven_days, side='right')\n",
    "    lo3 = np.searchsorted(ts_vals, cutoffs - three_days, side='right')\n",
    "    lo1 = np.searchsorted(ts_vals, cutoffs - one_day, side='right')\n",
    "\n",
    "    events_last_7d.loc[sample_ids] = (hi - lo7).astype('int32')\n",
    "    events_last_3d.loc[sample_ids] = (hi - lo3).astype('int32')\n",
    "    events_last_1d.loc[sample_ids] = (hi - lo1).astype('int32')\n",
    "\n",
    "# --- recent window counts: 7d / 3d / 1d (songs only)\n",
    "songs_last_7d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "songs_last_3d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "songs_last_1d = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    df_u = train_groups[uid]\n",
    "    df_song = df_u[df_u['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        continue\n",
    "    ts_song = df_song['ts'].values\n",
    "\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    hi = np.searchsorted(ts_song, cutoffs, side='right')\n",
    "    lo7 = np.searchsorted(ts_song, cutoffs - seven_days, side='right')\n",
    "    lo3 = np.searchsorted(ts_song, cutoffs - three_days, side='right')\n",
    "    lo1 = np.searchsorted(ts_song, cutoffs - one_day, side='right')\n",
    "\n",
    "    songs_last_7d.loc[sample_ids] = (hi - lo7).astype('int32')\n",
    "    songs_last_3d.loc[sample_ids] = (hi - lo3).astype('int32')\n",
    "    songs_last_1d.loc[sample_ids] = (hi - lo1).astype('int32')\n",
    "\n",
    "# --- active_days: number of unique active days up to cutoff\n",
    "active_days = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    df_u = train_groups[uid]\n",
    "    unique_days = np.unique(df_u['ts'].dt.normalize().values)  # sorted\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "    hi = np.searchsorted(unique_days, cutoffs, side='right')\n",
    "    active_days.loc[sample_ids] = hi.astype('int32')\n",
    "\n",
    "# --- total_listen_time: cumulative listening seconds up to cutoff\n",
    "total_listen_time = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    df_u = train_groups[uid]\n",
    "    df_song = df_u[df_u['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        continue\n",
    "\n",
    "    ts_song = df_song['ts'].values\n",
    "    len_song = df_song['length'].values\n",
    "    cum_len = np.cumsum(len_song)\n",
    "\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "    hi = np.searchsorted(ts_song, cutoffs, side='right')\n",
    "    for j, sid in enumerate(sample_ids):\n",
    "        h = hi[j]\n",
    "        total_listen_time.loc[sid] = 0.0 if h == 0 else float(cum_len[h - 1])\n"
   ],
   "id": "4cc6332df336a605",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/xtlxv68s5sv1j3rj3ww3zh_h0000gn/T/ipykernel_1812/1603713011.py:30: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '71.65' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  recency_hours.loc[sid] = (cutoffs[j] - last_ts) / np.timedelta64(1, 'h')\n",
      "/var/folders/zq/xtlxv68s5sv1j3rj3ww3zh_h0000gn/T/ipykernel_1812/1603713011.py:114: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '257291.02684000006' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  total_listen_time.loc[sid] = 0.0 if h == 0 else float(cum_len[h - 1])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. level_at_cutoff (last known level)",
   "id": "c1562f17bda793f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:42:25.814327Z",
     "start_time": "2025-12-19T00:42:23.951655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "level_at_cutoff = pd.Series(\"unknown\", index=sliding_labels.index, dtype=object)\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals  = df_u['ts'].values\n",
    "    lvl_vals = df_u['level'].astype(str).values\n",
    "\n",
    "    cutoffs = lbl_u['cutoff_date'].values\n",
    "    sample_ids = lbl_u.index\n",
    "\n",
    "    hi = np.searchsorted(ts_vals, cutoffs, side='right')\n",
    "    out = []\n",
    "    for j, h in enumerate(hi):\n",
    "        out.append(\"unknown\" if h == 0 else lvl_vals[h - 1])\n",
    "    level_at_cutoff.loc[sample_ids] = out\n",
    "\n",
    "sliding_labels['level'] = level_at_cutoff\n"
   ],
   "id": "ac56aab6f4ccb99f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Session features (keep your original logic)",
   "id": "529da57cb1ab3b36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:43:43.083735Z",
     "start_time": "2025-12-19T00:42:25.815030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "session_count = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "mean_session_duration = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "max_session_duration  = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "min_session_duration  = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "std_session_duration  = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "\n",
    "mean_event_count_per_session = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "max_event_count_per_session  = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "min_event_count_per_session  = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "std_event_count_per_session  = pd.Series(0.0, index=sliding_labels.index, dtype='float32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "\n",
    "    df_u = train_groups[uid]\n",
    "    sess_u = (\n",
    "        df_u.groupby('sessionId')\n",
    "        .agg(\n",
    "            session_start=('ts', 'min'),\n",
    "            session_end=('ts', 'max'),\n",
    "            session_event_count=('ts', 'count'),\n",
    "        )\n",
    "        .sort_values('session_start')\n",
    "    )\n",
    "    if sess_u.empty:\n",
    "        continue\n",
    "\n",
    "    sess_start = sess_u['session_start'].values\n",
    "    sess_end   = sess_u['session_end'].values\n",
    "    sess_cnt   = sess_u['session_event_count'].values.astype('int32')\n",
    "    sess_dur   = (sess_end - sess_start) / np.timedelta64(1, 's')\n",
    "\n",
    "    # Ensure cutoffs are increasing (safe)\n",
    "    lbl_u_sorted = lbl_u.sort_values('cutoff_date')\n",
    "    cutoffs = lbl_u_sorted['cutoff_date'].values\n",
    "    sample_ids = lbl_u_sorted.index\n",
    "\n",
    "    hi_sess = np.searchsorted(sess_start, cutoffs, side='right')\n",
    "    for j, sid in enumerate(sample_ids):\n",
    "        h = hi_sess[j]\n",
    "        if h == 0:\n",
    "            session_count.loc[sid] = 0\n",
    "            continue\n",
    "        d = sess_dur[:h]\n",
    "        c = sess_cnt[:h]\n",
    "\n",
    "        session_count.loc[sid] = h\n",
    "        mean_session_duration.loc[sid] = d.mean()\n",
    "        max_session_duration.loc[sid]  = d.max()\n",
    "        min_session_duration.loc[sid]  = d.min()\n",
    "        std_session_duration.loc[sid]  = d.std(ddof=0)\n",
    "\n",
    "        mean_event_count_per_session.loc[sid] = c.mean()\n",
    "        max_event_count_per_session.loc[sid]  = c.max()\n",
    "        min_event_count_per_session.loc[sid]  = c.min()\n",
    "        std_event_count_per_session.loc[sid]  = c.std(ddof=0)\n"
   ],
   "id": "5af159a68a0677df",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/xtlxv68s5sv1j3rj3ww3zh_h0000gn/T/ipykernel_1812/3116568393.py:49: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '23176.090909090908' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  mean_session_duration.loc[sid] = d.mean()\n",
      "/var/folders/zq/xtlxv68s5sv1j3rj3ww3zh_h0000gn/T/ipykernel_1812/3116568393.py:52: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '28070.654432422176' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  std_session_duration.loc[sid]  = d.std(ddof=0)\n",
      "/var/folders/zq/xtlxv68s5sv1j3rj3ww3zh_h0000gn/T/ipykernel_1812/3116568393.py:54: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '114.45454545454545' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  mean_event_count_per_session.loc[sid] = c.mean()\n",
      "/var/folders/zq/xtlxv68s5sv1j3rj3ww3zh_h0000gn/T/ipykernel_1812/3116568393.py:57: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '137.10139820012688' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  std_event_count_per_session.loc[sid]  = c.std(ddof=0)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Page count + page ratio (train/test share the same all_pages universe; train is strictly per-cutoff to avoid leakage)",
   "id": "b38d696c0b18f5d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:44:20.413837Z",
     "start_time": "2025-12-19T00:43:43.088746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define all_pages as the union of train and test pages\n",
    "all_pages = pd.concat([train_df['page'], test_df['page']]).astype(str).unique()\n",
    "all_pages = np.sort(all_pages)  # stable column order\n",
    "page2idx = {p: i for i, p in enumerate(all_pages)}\n",
    "P = len(all_pages)\n",
    "print(\"Total distinct pages (train ∪ test):\", P)\n",
    "\n",
    "# Initialize page count table for training samples\n",
    "page_cnt_df = pd.DataFrame(\n",
    "    0,\n",
    "    index=sliding_labels.index,\n",
    "    columns=[f\"cnt_page_{p.replace(' ', '_').replace('/', '_')}\" for p in all_pages],\n",
    "    dtype='int32'\n",
    ")\n",
    "\n",
    "# For each user, one pass accumulation over events; snapshot at each cutoff\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "\n",
    "    df_u = train_groups[uid]\n",
    "    ts_vals = df_u['ts'].values\n",
    "    page_idx = df_u['page'].map(page2idx).values  # integer codes\n",
    "\n",
    "    # Sort cutoffs so we can scan in O(events + cutoffs)\n",
    "    lbl_u_sorted = lbl_u.sort_values('cutoff_date')\n",
    "    cutoffs = lbl_u_sorted['cutoff_date'].values\n",
    "    sample_ids = lbl_u_sorted.index\n",
    "\n",
    "    counts = np.zeros(P, dtype=np.int32)\n",
    "    k = 0\n",
    "    m = len(ts_vals)\n",
    "\n",
    "    for j, sid in enumerate(sample_ids):\n",
    "        cutoff = cutoffs[j]\n",
    "        while k < m and ts_vals[k] <= cutoff:\n",
    "            counts[page_idx[k]] += 1\n",
    "            k += 1\n",
    "\n",
    "        # Write the full vector of page counts for this sample\n",
    "        page_cnt_df.loc[sid, :] = counts\n",
    "\n",
    "# Page ratios: divide by n_events (more stable than summing page_cnt_df)\n",
    "eps = 1e-6\n",
    "page_ratio_df = (page_cnt_df.div(n_events.astype(float) + eps, axis=0)).astype('float32')\n",
    "page_ratio_df.columns = [c.replace(\"cnt_page_\", \"ratio_page_\") for c in page_ratio_df.columns]\n",
    "\n",
    "print(\"Page cnt/ratio built:\", page_cnt_df.shape, page_ratio_df.shape)\n"
   ],
   "id": "d10f6c0d3f07ef0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct pages (train ∪ test): 22\n",
      "Page cnt/ratio built: (400774, 22) (400774, 22)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10. Level features: n_level_change + ever_paid (no leakage: per-cutoff)",
   "id": "a100c99d0582bfa8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:44:26.071825Z",
     "start_time": "2025-12-19T00:44:20.416978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# n_level_change: number of free/paid switches before cutoff\n",
    "n_level_change = pd.Series(0, index=sliding_labels.index, dtype='int32')\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    df_u = train_groups[uid]\n",
    "    lvl_vals = df_u['level'].astype(str).values\n",
    "    ts_vals  = df_u['ts'].values\n",
    "    if len(lvl_vals) <= 1:\n",
    "        continue\n",
    "\n",
    "    change_ts = ts_vals[1:][lvl_vals[1:] != lvl_vals[:-1]]\n",
    "    if len(change_ts) == 0:\n",
    "        continue\n",
    "\n",
    "    lbl_u_sorted = lbl_u.sort_values('cutoff_date')\n",
    "    cutoffs = lbl_u_sorted['cutoff_date'].values\n",
    "    sample_ids = lbl_u_sorted.index\n",
    "\n",
    "    hi_change = np.searchsorted(change_ts, cutoffs, side='right')\n",
    "    n_level_change.loc[sample_ids] = hi_change.astype('int32')\n",
    "\n",
    "# ever_paid: whether paid has appeared before cutoff\n",
    "ever_paid = pd.Series(0, index=sliding_labels.index, dtype='int8')\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    df_u = train_groups[uid]\n",
    "    paid_ts = df_u.loc[df_u['level'].astype(str).values == \"paid\", 'ts'].values\n",
    "    if len(paid_ts) == 0:\n",
    "        continue\n",
    "\n",
    "    lbl_u_sorted = lbl_u.sort_values('cutoff_date')\n",
    "    cutoffs = lbl_u_sorted['cutoff_date'].values\n",
    "    sample_ids = lbl_u_sorted.index\n",
    "\n",
    "    ever_paid.loc[sample_ids] = (np.searchsorted(paid_ts, cutoffs, side='right') > 0).astype('int8')\n"
   ],
   "id": "5b448f82a40bef85",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 11. Status code features (keep your original logic)",
   "id": "7b10ed3d548dcb5a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:44:40.876175Z",
     "start_time": "2025-12-19T00:44:26.072660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "status_codes_of_interest = [200, 404, 307]\n",
    "status_count_series = {code: pd.Series(0, index=sliding_labels.index, dtype='int32') for code in status_codes_of_interest}\n",
    "\n",
    "for uid, lbl_u in label_groups.items():\n",
    "    if uid not in train_groups:\n",
    "        continue\n",
    "    df_u = train_groups[uid]\n",
    "\n",
    "    lbl_u_sorted = lbl_u.sort_values('cutoff_date')\n",
    "    cutoffs = lbl_u_sorted['cutoff_date'].values\n",
    "    sample_ids = lbl_u_sorted.index\n",
    "\n",
    "    for code in status_codes_of_interest:\n",
    "        df_code = df_u[df_u['status'] == code]\n",
    "        if df_code.empty:\n",
    "            continue\n",
    "        ts_code = df_code['ts'].values\n",
    "        hi_code = np.searchsorted(ts_code, cutoffs, side='right')\n",
    "        status_count_series[code].loc[sample_ids] = hi_code.astype('int32')\n"
   ],
   "id": "b01b373462b26ef7",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 12. Merge all training features X_all (numeric + page cnt/ratio + status + level + session)",
   "id": "70c832e21caa928a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:44:41.095636Z",
     "start_time": "2025-12-19T00:44:40.876922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_all = pd.DataFrame(index=sliding_labels.index)\n",
    "\n",
    "X_all['days_since_registration'] = days_since_registration\n",
    "X_all['n_events'] = n_events.astype('int32')\n",
    "X_all['recency_hours'] = recency_hours.astype('float32')\n",
    "\n",
    "X_all['events_last_7d'] = events_last_7d.astype('int32')\n",
    "X_all['events_last_3d'] = events_last_3d.astype('int32')\n",
    "X_all['events_last_1d'] = events_last_1d.astype('int32')\n",
    "\n",
    "X_all['songs_last_7d'] = songs_last_7d.astype('int32')\n",
    "X_all['songs_last_3d'] = songs_last_3d.astype('int32')\n",
    "X_all['songs_last_1d'] = songs_last_1d.astype('int32')\n",
    "\n",
    "X_all['active_days'] = active_days.astype('int32')\n",
    "X_all['total_listen_time'] = total_listen_time.astype('float32')\n",
    "\n",
    "X_all['session_count'] = session_count\n",
    "X_all['mean_session_duration'] = mean_session_duration\n",
    "X_all['max_session_duration']  = max_session_duration\n",
    "X_all['min_session_duration']  = min_session_duration\n",
    "X_all['std_session_duration']  = std_session_duration\n",
    "\n",
    "X_all['mean_event_count_per_session'] = mean_event_count_per_session\n",
    "X_all['max_event_count_per_session']  = max_event_count_per_session\n",
    "X_all['min_event_count_per_session']  = min_event_count_per_session\n",
    "X_all['std_event_count_per_session']  = std_event_count_per_session\n",
    "\n",
    "# Page cnt + ratio\n",
    "X_all = pd.concat([X_all, page_cnt_df, page_ratio_df], axis=1)\n",
    "\n",
    "# Level\n",
    "X_all['ever_paid'] = ever_paid\n",
    "X_all['n_level_change'] = n_level_change\n",
    "\n",
    "# Status\n",
    "for code in status_codes_of_interest:\n",
    "    X_all[f\"n_status_{code}\"] = status_count_series[code]\n",
    "\n",
    "eps = 1e-6\n",
    "X_all['frac_status_404'] = (X_all['n_status_404'] / (X_all['n_events'] + eps)).astype('float32')\n",
    "X_all['frac_status_200'] = (X_all['n_status_200'] / (X_all['n_events'] + eps)).astype('float32')\n",
    "\n",
    "print(\"X_all shape:\", X_all.shape)\n"
   ],
   "id": "48aebb5c85e2511b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_all shape: (400774, 71)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 13. One-hot encode categorical features (train)",
   "id": "12a066d65143f768"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:44:41.487893Z",
     "start_time": "2025-12-19T00:44:41.096448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cat_cols = ['gender', 'state', 'level']\n",
    "sliding_labels[cat_cols] = sliding_labels[cat_cols].fillna(\"missing\")\n",
    "\n",
    "cat_ohe = pd.get_dummies(sliding_labels[cat_cols], columns=cat_cols, prefix=cat_cols)\n",
    "X_train = pd.concat([X_all, cat_ohe], axis=1)\n",
    "print(\"X_train shape:\", X_train.shape)\n"
   ],
   "id": "3d23bad1b41d1f98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (400774, 124)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 14. Build test features (aligned to train column space: all_pages + one-hot)",
   "id": "690523d400869dda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:45:02.773526Z",
     "start_time": "2025-12-19T00:44:41.488958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_df_sorted = test_df.sort_values(['userId', 'ts']).copy()\n",
    "test_groups = dict(tuple(test_df_sorted.groupby('userId')))\n",
    "test_users = sorted(test_groups.keys())\n",
    "\n",
    "global_cutoff_test = test_df_sorted['ts'].max().normalize()\n",
    "print(\"Test global cutoff_date:\", global_cutoff_test)\n",
    "\n",
    "X_test = pd.DataFrame(index=test_users)\n",
    "\n",
    "# days_since_registration\n",
    "uid_registration_test = test_df_sorted.groupby('userId')['registration'].first()\n",
    "X_test['days_since_registration'] = (\n",
    "    (global_cutoff_test - uid_registration_test) / np.timedelta64(1, 'D')\n",
    ").astype('float32').clip(lower=0).reindex(test_users)\n",
    "\n",
    "# n_events / recency / active_days\n",
    "n_events_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "recency_hours_test = pd.Series(0.0, index=test_users, dtype='float32')\n",
    "active_days_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        n_events_test.loc[uid] = 0\n",
    "        recency_hours_test.loc[uid] = 9999.0\n",
    "        active_days_test.loc[uid] = 0\n",
    "        continue\n",
    "\n",
    "    n_events_test.loc[uid] = len(df_before)\n",
    "    last_ts = df_before['ts'].iloc[-1]\n",
    "    recency_hours_test.loc[uid] = float((global_cutoff_test - last_ts) / np.timedelta64(1, 'h'))\n",
    "    active_days_test.loc[uid] = df_before['ts'].dt.normalize().nunique()\n",
    "\n",
    "X_test['n_events'] = n_events_test\n",
    "X_test['recency_hours'] = recency_hours_test\n",
    "X_test['active_days'] = active_days_test\n",
    "\n",
    "# recent window counts: events\n",
    "events_last_7d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "events_last_3d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "events_last_1d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    ts_vals = df_before['ts'].values\n",
    "    events_last_7d_test.loc[uid] = int((ts_vals > (global_cutoff_test - seven_days)).sum())\n",
    "    events_last_3d_test.loc[uid] = int((ts_vals > (global_cutoff_test - three_days)).sum())\n",
    "    events_last_1d_test.loc[uid] = int((ts_vals > (global_cutoff_test - one_day)).sum())\n",
    "\n",
    "X_test['events_last_7d'] = events_last_7d_test\n",
    "X_test['events_last_3d'] = events_last_3d_test\n",
    "X_test['events_last_1d'] = events_last_1d_test\n",
    "\n",
    "# songs + total_listen_time\n",
    "songs_last_7d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "songs_last_3d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "songs_last_1d_test = pd.Series(0, index=test_users, dtype='int32')\n",
    "total_listen_time_test = pd.Series(0.0, index=test_users, dtype='float32')\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    df_song = df_before[df_before['page'] == \"NextSong\"]\n",
    "    if df_song.empty:\n",
    "        continue\n",
    "    ts_song = df_song['ts'].values\n",
    "    len_song = df_song['length'].values\n",
    "\n",
    "    total_listen_time_test.loc[uid] = float(len_song.sum())\n",
    "    songs_last_7d_test.loc[uid] = int((ts_song > (global_cutoff_test - seven_days)).sum())\n",
    "    songs_last_3d_test.loc[uid] = int((ts_song > (global_cutoff_test - three_days)).sum())\n",
    "    songs_last_1d_test.loc[uid] = int((ts_song > (global_cutoff_test - one_day)).sum())\n",
    "\n",
    "X_test['songs_last_7d'] = songs_last_7d_test\n",
    "X_test['songs_last_3d'] = songs_last_3d_test\n",
    "X_test['songs_last_1d'] = songs_last_1d_test\n",
    "X_test['total_listen_time'] = total_listen_time_test\n",
    "\n",
    "# level_at_cutoff_test\n",
    "level_at_cutoff_test = pd.Series(\"unknown\", index=test_users, dtype=object)\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    level_at_cutoff_test.loc[uid] = str(df_before['level'].iloc[-1])\n",
    "\n",
    "# Page cnt/ratio on test (same all_pages column space)\n",
    "page_cnt_test = pd.DataFrame(\n",
    "    0,\n",
    "    index=test_users,\n",
    "    columns=page_cnt_df.columns,\n",
    "    dtype='int32'\n",
    ")\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    idx = df_before['page'].map(page2idx).values\n",
    "    bc = np.bincount(idx, minlength=P).astype(np.int32)\n",
    "    page_cnt_test.loc[uid, :] = bc\n",
    "\n",
    "page_ratio_test = (page_cnt_test.div(n_events_test.astype(float) + eps, axis=0)).astype('float32')\n",
    "page_ratio_test.columns = [c.replace(\"cnt_page_\", \"ratio_page_\") for c in page_ratio_test.columns]\n",
    "\n",
    "# Session features (test)\n",
    "X_test['session_count'] = 0\n",
    "X_test['mean_session_duration'] = 0.0\n",
    "X_test['max_session_duration']  = 0.0\n",
    "X_test['min_session_duration']  = 0.0\n",
    "X_test['std_session_duration']  = 0.0\n",
    "\n",
    "X_test['mean_event_count_per_session'] = 0.0\n",
    "X_test['max_event_count_per_session']  = 0.0\n",
    "X_test['min_event_count_per_session']  = 0.0\n",
    "X_test['std_event_count_per_session']  = 0.0\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "\n",
    "    sess_u = (\n",
    "        df_before.groupby('sessionId')\n",
    "        .agg(\n",
    "            session_start=('ts', 'min'),\n",
    "            session_end=('ts', 'max'),\n",
    "            session_event_count=('ts', 'count'),\n",
    "        )\n",
    "    )\n",
    "    if sess_u.empty:\n",
    "        continue\n",
    "\n",
    "    dur = ((sess_u['session_end'] - sess_u['session_start']) / np.timedelta64(1, 's')).values\n",
    "    cnt = sess_u['session_event_count'].values.astype(np.int32)\n",
    "\n",
    "    X_test.loc[uid, 'session_count'] = len(sess_u)\n",
    "    X_test.loc[uid, 'mean_session_duration'] = dur.mean()\n",
    "    X_test.loc[uid, 'max_session_duration']  = dur.max()\n",
    "    X_test.loc[uid, 'min_session_duration']  = dur.min()\n",
    "    X_test.loc[uid, 'std_session_duration']  = dur.std(ddof=0)\n",
    "\n",
    "    X_test.loc[uid, 'mean_event_count_per_session'] = cnt.mean()\n",
    "    X_test.loc[uid, 'max_event_count_per_session']  = cnt.max()\n",
    "    X_test.loc[uid, 'min_event_count_per_session']  = cnt.min()\n",
    "    X_test.loc[uid, 'std_event_count_per_session']  = cnt.std(ddof=0)\n",
    "\n",
    "# Level features (test; only one global cutoff)\n",
    "X_test['ever_paid'] = 0\n",
    "X_test['n_level_change'] = 0\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    lv = df_before['level'].astype(str).values\n",
    "    X_test.loc[uid, 'ever_paid'] = 1 if (lv == 'paid').any() else 0\n",
    "    if len(lv) > 1:\n",
    "        X_test.loc[uid, 'n_level_change'] = int(np.sum(lv[1:] != lv[:-1]))\n",
    "\n",
    "# Status features (test; same status_codes_of_interest)\n",
    "for code in status_codes_of_interest:\n",
    "    X_test[f\"n_status_{code}\"] = 0\n",
    "\n",
    "for uid, df_u in test_groups.items():\n",
    "    df_before = df_u[df_u['ts'] <= global_cutoff_test]\n",
    "    if df_before.empty:\n",
    "        continue\n",
    "    for code in status_codes_of_interest:\n",
    "        X_test.loc[uid, f\"n_status_{code}\"] = int((df_before['status'] == code).sum())\n",
    "\n",
    "X_test['frac_status_404'] = (X_test['n_status_404'] / (X_test['n_events'] + eps)).astype('float32')\n",
    "X_test['frac_status_200'] = (X_test['n_status_200'] / (X_test['n_events'] + eps)).astype('float32')\n",
    "\n",
    "# Merge page cnt/ratio into X_test\n",
    "X_test = pd.concat([X_test, page_cnt_test, page_ratio_test], axis=1)\n",
    "\n",
    "# Static categorical features (test): gender/state/level\n",
    "test_user_static = (\n",
    "    test_df_sorted.groupby('userId')\n",
    "    .agg({'gender': 'first', 'state': 'first'})\n",
    ")\n",
    "\n",
    "cat_test = pd.DataFrame(index=test_users)\n",
    "cat_test['gender'] = test_user_static['gender']\n",
    "cat_test['state']  = test_user_static['state']\n",
    "cat_test['level']  = level_at_cutoff_test\n",
    "cat_test = cat_test.fillna(\"missing\")\n",
    "\n",
    "cat_test_ohe = pd.get_dummies(\n",
    "    cat_test,\n",
    "    columns=['gender', 'state', 'level'],\n",
    "    prefix=['gender', 'state', 'level']\n",
    ")\n",
    "cat_test_ohe = cat_test_ohe.reindex(columns=cat_ohe.columns, fill_value=0)\n",
    "\n",
    "X_test_full = pd.concat([X_test, cat_test_ohe], axis=1)\n",
    "\n",
    "# Align test columns to train columns\n",
    "X_test_full = X_test_full.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "print(\"X_test_full shape:\", X_test_full.shape)\n"
   ],
   "id": "2f56ef5cca82575a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test global cutoff_date: 2018-11-20 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/xtlxv68s5sv1j3rj3ww3zh_h0000gn/T/ipykernel_1812/2050866787.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '101.7525' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  recency_hours_test.loc[uid] = float((global_cutoff_test - last_ts) / np.timedelta64(1, 'h'))\n",
      "/var/folders/zq/xtlxv68s5sv1j3rj3ww3zh_h0000gn/T/ipykernel_1812/2050866787.py:72: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '65479.8747' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  total_listen_time_test.loc[uid] = float(len_song.sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_full shape: (2904, 124)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 15. Heavy-tail feature transforms: log1p + ratio normalization (train/test consistent)",
   "id": "b5af16fea91d0fc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:45:03.040533Z",
     "start_time": "2025-12-19T00:45:02.774842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_tail_transforms(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    eps = 1e-6\n",
    "\n",
    "    # 1) log1p for heavy-tailed features (counts/cumulative quantities)\n",
    "    log_cols = [\n",
    "        'n_events',\n",
    "        'events_last_7d', 'events_last_3d', 'events_last_1d',\n",
    "        'songs_last_7d', 'songs_last_3d', 'songs_last_1d',\n",
    "        'total_listen_time',\n",
    "        'session_count',\n",
    "        'mean_session_duration', 'max_session_duration', 'min_session_duration', 'std_session_duration',\n",
    "        'mean_event_count_per_session', 'max_event_count_per_session', 'min_event_count_per_session', 'std_event_count_per_session',\n",
    "        'recency_hours',\n",
    "    ]\n",
    "\n",
    "    # Log-transform page/status count columns; do NOT log-transform ratio columns\n",
    "    log_cols += [c for c in X.columns if c.startswith('cnt_page_')]\n",
    "    log_cols += [c for c in X.columns if c.startswith('n_status_')]\n",
    "    log_cols += [c for c in X.columns if c.startswith('cnt_status_')]\n",
    "\n",
    "    for c in log_cols:\n",
    "        if c in X.columns:\n",
    "            X[c] = np.log1p(np.clip(X[c].astype(float), a_min=0, a_max=None)).astype('float32')\n",
    "\n",
    "    # 2) Ratio-style normalization to control for user scale\n",
    "    if 'active_days' in X.columns and 'n_events' in X.columns:\n",
    "        X['events_per_active_day'] = (X['n_events'] / (X['active_days'] + eps)).astype('float32')\n",
    "    if 'active_days' in X.columns and 'songs_last_7d' in X.columns:\n",
    "        X['songs7_per_active_day'] = (X['songs_last_7d'] / (X['active_days'] + eps)).astype('float32')\n",
    "    if 'active_days' in X.columns and 'total_listen_time' in X.columns:\n",
    "        X['listen_time_per_active_day'] = (X['total_listen_time'] / (X['active_days'] + eps)).astype('float32')\n",
    "\n",
    "    if 'days_since_registration' in X.columns and 'n_events' in X.columns:\n",
    "        X['events_per_day_since_reg'] = (X['n_events'] / (X['days_since_registration'] + 1.0)).astype('float32')\n",
    "\n",
    "    if 'total_listen_time' in X.columns and 'n_events' in X.columns:\n",
    "        X['listen_time_per_event'] = (X['total_listen_time'] / (X['n_events'] + eps)).astype('float32')\n",
    "\n",
    "    # If cnt_page_NextSong exists, add average listen time per song\n",
    "    if 'total_listen_time' in X.columns and 'cnt_page_NextSong' in X.columns:\n",
    "        X['listen_time_per_song'] = (X['total_listen_time'] / (X['cnt_page_NextSong'] + eps)).astype('float32')\n",
    "\n",
    "    if 'session_count' in X.columns and 'active_days' in X.columns:\n",
    "        X['sessions_per_active_day'] = (X['session_count'] / (X['active_days'] + eps)).astype('float32')\n",
    "\n",
    "    return X\n",
    "\n",
    "X_train = add_tail_transforms(X_train)\n",
    "X_test_full = add_tail_transforms(X_test_full)\n",
    "X_test_full = X_test_full.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "print(\"After tail transforms:\")\n",
    "print(\"X_train shape:\", X_train.shape, \"X_test_full shape:\", X_test_full.shape)\n"
   ],
   "id": "5a84502f29fd53a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tail transforms:\n",
      "X_train shape: (400774, 131) X_test_full shape: (2904, 131)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 16. Model training (your original logic)",
   "id": "48bd404250387c13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:45:03.044850Z",
     "start_time": "2025-12-19T00:45:03.041334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def oversample(X, y):\n",
    "    \"\"\"\n",
    "    Simple oversampling: repeat minority class to roughly match majority size.\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    X['target'] = y\n",
    "    major = X[X['target'] == 0]\n",
    "    minor = X[X['target'] == 1]\n",
    "    if len(minor) == 0:\n",
    "        raise ValueError(\"No positive samples.\")\n",
    "    ratio = max(1, len(major) // len(minor))\n",
    "    minor_ov = pd.concat([minor] * ratio, ignore_index=True)\n",
    "    df_new = pd.concat([major, minor_ov], axis=0).sample(frac=1.0, random_state=42)\n",
    "    y_new = df_new['target'].values\n",
    "    X_new = df_new.drop(columns=['target'])\n",
    "    print(\"Positive rate after oversample:\", y_new.mean())\n",
    "    return X_new, y_new\n"
   ],
   "id": "15513eae97fc9cc4",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 16.1 LightGBM",
   "id": "9557566423d92634"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:45:22.217439Z",
     "start_time": "2025-12-19T00:45:03.045501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_clf = lgb.LGBMClassifier(\n",
    "    n_estimators=800,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    max_depth=-1,\n",
    "    random_state=42\n",
    ")\n",
    "lgb_clf.fit(X_train, y_train)\n",
    "pred_lgb = lgb_clf.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "user_ids = example_sub['id'].astype(str)\n",
    "\n",
    "proba_lgb_aligned = pd.Series(pred_lgb, index=X_test_full.index).loc[user_ids].values\n",
    "threshold = np.quantile(proba_lgb_aligned, 0.5)\n",
    "pred_label = (proba_lgb_aligned >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\"id\": example_sub[\"id\"], \"target\": pred_label})\n",
    "submission.to_csv(\"submission_LightGBM.csv\", index=False)\n",
    "print(\"Saved submission_LightGBM.csv\")\n"
   ],
   "id": "6bbb6c54ddcc646c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 20143, number of negative: 380631\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017586 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13516\n",
      "[LightGBM] [Info] Number of data points in the train set: 400774, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.050260 -> initscore=-2.938974\n",
      "[LightGBM] [Info] Start training from score -2.938974\n",
      "Saved submission_LightGBM.csv\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 16.2 Logistic Regression",
   "id": "78378a19565248b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:48:28.278985Z",
     "start_time": "2025-12-19T00:45:22.218553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_lr, y_lr = oversample(X_train, y_train.values)\n",
    "scaler_lr = StandardScaler()\n",
    "X_lr_scaled = scaler_lr.fit_transform(X_lr)\n",
    "X_test_lr_scaled = scaler_lr.transform(X_test_full)\n",
    "\n",
    "lr_clf = LogisticRegression(max_iter=2000, solver=\"liblinear\")\n",
    "lr_clf.fit(X_lr_scaled, y_lr)\n",
    "pred_lr = lr_clf.predict_proba(X_test_lr_scaled)[:, 1]\n",
    "\n",
    "proba_lr_aligned = pd.Series(pred_lr, index=X_test_full.index).loc[user_ids].values\n",
    "threshold = np.quantile(proba_lr_aligned, 0.5)\n",
    "pred_label = (proba_lr_aligned >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\"id\": example_sub[\"id\"], \"target\": pred_label})\n",
    "submission.to_csv(\"submission_LR.csv\", index=False)\n",
    "print(\"Saved submission_LR.csv\")\n"
   ],
   "id": "314d1e29836ff10a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rate after oversample: 0.48785193856338427\n",
      "Saved submission_LR.csv\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 16.4 RandomForest",
   "id": "4e2c69889a19035"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:49:51.503783Z",
     "start_time": "2025-12-19T00:48:28.281786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=12,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "rf_clf.fit(X_train, y_train)\n",
    "pred_rf = rf_clf.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "proba_rf_aligned = pd.Series(pred_rf, index=X_test_full.index).loc[user_ids].values\n",
    "threshold = np.quantile(proba_rf_aligned, 0.5)\n",
    "pred_label = (proba_rf_aligned >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\"id\": example_sub[\"id\"], \"target\": pred_label})\n",
    "submission.to_csv(\"submission_RF.csv\", index=False)\n",
    "print(\"Saved submission_RF.csv\")\n"
   ],
   "id": "79ca3633f1d3b890",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_RF.csv\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 17. Ensemble (examples: soft voting with Top-50% rule)",
   "id": "808248a3639df87f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:49:51.521155Z",
     "start_time": "2025-12-19T00:49:51.504707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensemble 1: LR + LGBM soft voting (Top 50%)\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "user_ids = example_sub[\"id\"].astype(str)\n",
    "\n",
    "proba_lr_aligned = (\n",
    "    pd.Series(pred_lr, index=X_test_full.index.astype(str))\n",
    "    .loc[user_ids]\n",
    "    .values\n",
    ")\n",
    "\n",
    "proba_lgb_aligned = (\n",
    "    pd.Series(pred_lgb, index=X_test_full.index.astype(str))\n",
    "    .loc[user_ids]\n",
    "    .values\n",
    ")\n",
    "\n",
    "w_lr, w_lgb = 1.0, 1.0  # you can try (2,1), (3,1), (4,1), etc.\n",
    "voting_proba = (w_lr * proba_lr_aligned + w_lgb * proba_lgb_aligned) / (w_lr + w_lgb)\n",
    "\n",
    "threshold = np.quantile(voting_proba, 0.5)\n",
    "print(\"LR + LGBM voting threshold (top50) =\", threshold)\n",
    "\n",
    "pred_label = (voting_proba >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": example_sub[\"id\"],\n",
    "    \"target\": pred_label\n",
    "})\n",
    "print(submission[\"target\"].value_counts(normalize=True))\n",
    "submission.to_csv(\"submission_LR_LGBM_voting.csv\", index=False)\n",
    "print(\"Saved submission_LR_LGBM_voting.csv\")\n"
   ],
   "id": "4aa15fee4576e70b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR + LGBM voting threshold (top50) = 0.3189396564927801\n",
      "target\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Saved submission_LR_LGBM_voting.csv\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:49:51.532529Z",
     "start_time": "2025-12-19T00:49:51.522026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensemble 2: RF + LGBM + LR weighted soft voting (Top 50%)\n",
    "w_rf, w_lgb, w_lr = 1.0, 3.0, 1.0\n",
    "\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "user_ids = example_sub[\"id\"].astype(str)\n",
    "\n",
    "proba_rf_aligned = (\n",
    "    pd.Series(pred_rf, index=X_test_full.index.astype(str))\n",
    "    .loc[user_ids]\n",
    "    .values\n",
    ")\n",
    "\n",
    "proba_lgb_aligned = (\n",
    "    pd.Series(pred_lgb, index=X_test_full.index.astype(str))\n",
    "    .loc[user_ids]\n",
    "    .values\n",
    ")\n",
    "\n",
    "proba_lr_aligned = (\n",
    "    pd.Series(pred_lr, index=X_test_full.index.astype(str))\n",
    "    .loc[user_ids]\n",
    "    .values\n",
    ")\n",
    "\n",
    "voting_proba = (\n",
    "    w_rf  * proba_rf_aligned +\n",
    "    w_lgb * proba_lgb_aligned +\n",
    "    w_lr  * proba_lr_aligned\n",
    ") / (w_rf + w_lgb + w_lr)\n",
    "\n",
    "threshold = np.quantile(voting_proba, 0.5)\n",
    "print(\"RF + LGBM + LR voting threshold =\", threshold)\n",
    "\n",
    "pred_label = (voting_proba >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": example_sub[\"id\"],\n",
    "    \"target\": pred_label\n",
    "})\n",
    "\n",
    "print(submission[\"target\"].value_counts(normalize=True))\n",
    "submission.to_csv(\"submission_RF_LGBM_LR_voting_weighted.csv\", index=False)\n",
    "print(\"Saved submission_RF_LGBM_LR_voting_weighted.csv\")\n"
   ],
   "id": "982ad5b0a524c8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF + LGBM + LR voting threshold = 0.16635179788828938\n",
      "target\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Saved submission_RF_LGBM_LR_voting_weighted.csv\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:49:51.559088Z",
     "start_time": "2025-12-19T00:49:51.533396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensemble 3: Sweep LR:LGBM weights (Top 50%)\n",
    "weight_grid = [\n",
    "    (1.0, 6.1),\n",
    "    (1.0, 6.2),\n",
    "    (1.0, 6.3),\n",
    "    (1.0, 6.4),\n",
    "    (1.0, 6.5),\n",
    "    (1.0, 6.6),\n",
    "    (1.0, 6.7),\n",
    "    (1.0, 6.8),\n",
    "    (1.0, 6.9),\n",
    "]\n",
    "\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "user_ids = example_sub[\"id\"].astype(str)\n",
    "\n",
    "proba_lr_aligned = (\n",
    "    pd.Series(pred_lr, index=X_test_full.index.astype(str))\n",
    "    .loc[user_ids]\n",
    "    .values\n",
    ")\n",
    "\n",
    "proba_lgb_aligned = (\n",
    "    pd.Series(pred_lgb, index=X_test_full.index.astype(str))\n",
    "    .loc[user_ids]\n",
    "    .values\n",
    ")\n",
    "\n",
    "for w_lr, w_lgb in weight_grid:\n",
    "    print(f\"\\n=== Voting: LR:LGBM = {w_lr}:{w_lgb} ===\")\n",
    "    voting_proba = (w_lr * proba_lr_aligned + w_lgb * proba_lgb_aligned) / (w_lr + w_lgb)\n",
    "\n",
    "    threshold = np.quantile(voting_proba, 0.5)\n",
    "    print(\"threshold =\", threshold)\n",
    "\n",
    "    pred_label = (voting_proba >= threshold).astype(int)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": example_sub[\"id\"],\n",
    "        \"target\": pred_label\n",
    "    })\n",
    "\n",
    "    print(\"positive ratio:\", submission[\"target\"].mean())\n",
    "\n",
    "    fname = f\"submission_LR_LGBM_voting_w{w_lr}_w{w_lgb}.csv\"\n",
    "    submission.to_csv(fname, index=False)\n",
    "    print(\"Saved:\", fname)\n"
   ],
   "id": "a9fbc54bbd1ad43c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Voting: LR:LGBM = 1.0:6.1 ===\n",
      "threshold = 0.13123549313526348\n",
      "positive ratio: 0.5\n",
      "Saved: submission_LR_LGBM_voting_w1.0_w6.1.csv\n",
      "\n",
      "=== Voting: LR:LGBM = 1.0:6.2 ===\n",
      "threshold = 0.13017550453965482\n",
      "positive ratio: 0.5\n",
      "Saved: submission_LR_LGBM_voting_w1.0_w6.2.csv\n",
      "\n",
      "=== Voting: LR:LGBM = 1.0:6.3 ===\n",
      "threshold = 0.12917160851810322\n",
      "positive ratio: 0.5\n",
      "Saved: submission_LR_LGBM_voting_w1.0_w6.3.csv\n",
      "\n",
      "=== Voting: LR:LGBM = 1.0:6.4 ===\n",
      "threshold = 0.12831287384940493\n",
      "positive ratio: 0.5\n",
      "Saved: submission_LR_LGBM_voting_w1.0_w6.4.csv\n",
      "\n",
      "=== Voting: LR:LGBM = 1.0:6.5 ===\n",
      "threshold = 0.12738394395856076\n",
      "positive ratio: 0.5\n",
      "Saved: submission_LR_LGBM_voting_w1.0_w6.5.csv\n",
      "\n",
      "=== Voting: LR:LGBM = 1.0:6.6 ===\n",
      "threshold = 0.12642738982385338\n",
      "positive ratio: 0.5\n",
      "Saved: submission_LR_LGBM_voting_w1.0_w6.6.csv\n",
      "\n",
      "=== Voting: LR:LGBM = 1.0:6.7 ===\n",
      "threshold = 0.12558149356130538\n",
      "positive ratio: 0.5\n",
      "Saved: submission_LR_LGBM_voting_w1.0_w6.7.csv\n",
      "\n",
      "=== Voting: LR:LGBM = 1.0:6.8 ===\n",
      "threshold = 0.12477035237207047\n",
      "positive ratio: 0.5\n",
      "Saved: submission_LR_LGBM_voting_w1.0_w6.8.csv\n",
      "\n",
      "=== Voting: LR:LGBM = 1.0:6.9 ===\n",
      "threshold = 0.12380178752386548\n",
      "positive ratio: 0.5\n",
      "Saved: submission_LR_LGBM_voting_w1.0_w6.9.csv\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T00:49:51.567149Z",
     "start_time": "2025-12-19T00:49:51.559988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensemble 4: Custom weight grid for (RF, LGBM, LR)\n",
    "example_sub = pd.read_csv(\"example_submission.csv\")\n",
    "\n",
    "# (w_rf, w_lgb, w_lr)\n",
    "weight_grid = [\n",
    "    (1, 3, 1),\n",
    "]\n",
    "\n",
    "for w_rf, w_lgb, w_lr in weight_grid:\n",
    "    voting_proba = (\n",
    "        w_rf  * proba_rf_aligned +\n",
    "        w_lgb * proba_lgb_aligned +\n",
    "        w_lr  * proba_lr_aligned\n",
    "    ) / (w_rf + w_lgb + w_lr)\n",
    "\n",
    "    threshold = np.quantile(voting_proba, 0.5)\n",
    "    pred = (voting_proba >= threshold).astype(int)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": example_sub[\"id\"],\n",
    "        \"target\": pred\n",
    "    })\n",
    "\n",
    "    fname = f\"submission_RF_LGBM_LR_{w_rf}_{w_lgb}_{w_lr}.csv\"\n",
    "    submission.to_csv(fname, index=False)\n",
    "\n",
    "    print(\n",
    "        f\"{fname} | \"\n",
    "        f\"thr={threshold:.6f} | \"\n",
    "        f\"pos_rate={pred.mean():.4f}\"\n",
    "    )"
   ],
   "id": "90aef6fb0cd0833f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_RF_LGBM_LR_1_3_1.csv | thr=0.166352 | pos_rate=0.5000\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
